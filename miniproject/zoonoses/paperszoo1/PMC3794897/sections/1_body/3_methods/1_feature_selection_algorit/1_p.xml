<?xml version="1.0" encoding="UTF-8"?>
<p>Feature selection methods try to find the subset of relevant features for building robust learning models that can accurately inform a classification algorithm 
 <xref rid="pcbi.1003254-Guyon1" ref-type="bibr">[16]</xref>. We focussed on the random forest algorithm (RFA), since it offers excellent performance in classification tasks 
 <xref rid="pcbi.1003254-DiazUriarte1" ref-type="bibr">[17]</xref>, and provides direct measures of variable importance and classification error. Each tree in a random forest is trained on a bootstrap sample of the data, and at each split a random subset of the variables is chosen from all the available variables (in this case, a subset of positions in the sequence for each split). Final classification of each sample results from aggregating the votes of all trees in the forest. The importance measure of each variable is obtained as the loss of accuracy of classification caused by the random permutation of attribute values for that variable. RFA identifies which variables give the most discriminating information regarding the independent categorical variable of interest (host reservoir in this case). We used the 
 <italic>varSelRF</italic> package in 
 <italic>R</italic> to run the random forest algorithm 
 <xref rid="pcbi.1003254-DiazUriarte2" ref-type="bibr">[56]</xref>.
</p>
