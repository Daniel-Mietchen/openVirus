<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6331110</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0210829</article-id><article-id pub-id-type="publisher-id">PONE-D-18-25867</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Infectious Diseases</subject><subj-group><subject>Disease Vectors</subject><subj-group><subject>Insect Vectors</subject><subj-group><subject>Mosquitoes</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Species Interactions</subject><subj-group><subject>Disease Vectors</subject><subj-group><subject>Insect Vectors</subject><subj-group><subject>Mosquitoes</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Insects</subject><subj-group><subject>Mosquitoes</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Infectious Diseases</subject><subj-group><subject>Disease Vectors</subject><subj-group><subject>Insect Vectors</subject><subj-group><subject>Mosquitoes</subject><subj-group><subject>Aedes Aegypti</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Species Interactions</subject><subj-group><subject>Disease Vectors</subject><subj-group><subject>Insect Vectors</subject><subj-group><subject>Mosquitoes</subject><subj-group><subject>Aedes Aegypti</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Insects</subject><subj-group><subject>Mosquitoes</subject><subj-group><subject>Aedes Aegypti</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Infectious Diseases</subject><subj-group><subject>Vector-Borne Diseases</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Insects</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Entomology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Taxonomy</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Data Management</subject><subj-group><subject>Taxonomy</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Application of convolutional neural networks for classification of adult mosquitoes in the field</article-title><alt-title alt-title-type="running-head">Application of convolutional neural networks for classification of adult mosquitoes</alt-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Motta</surname><given-names>Daniel</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Santos</surname><given-names>Alex &#x000c1;lisson Bandeira</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Project administration</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="author-notes" rid="econtrib001"><sup>&#x02021;</sup></xref></contrib><contrib contrib-type="author"><name><surname>Winkler</surname><given-names>Ingrid</given-names></name><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="author-notes" rid="econtrib001"><sup>&#x02021;</sup></xref></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1655-0325</contrib-id><name><surname>Machado</surname><given-names>Bruna Aparecida Souza</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Pereira</surname><given-names>Daniel Andr&#x000e9; Dias Imperial</given-names></name><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Software</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Cavalcanti</surname><given-names>Alexandre Morais</given-names></name><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Software</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Fonseca</surname><given-names>Eduardo Oyama Lins</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="author-notes" rid="econtrib001"><sup>&#x02021;</sup></xref></contrib><contrib contrib-type="author"><name><surname>Kirchner</surname><given-names>Frank</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref><xref ref-type="author-notes" rid="econtrib001"><sup>&#x02021;</sup></xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Badar&#x000f3;</surname><given-names>Roberto</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>University Center SENAI CIMATEC, National Service of Industrial Learning&#x02013;SENAI, Salvador, Bahia, Brazil</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Health Institute of Technologies (CIMATEC ITS), National Service of Industrial Learning&#x02013;SENAI, Salvador, Bahia, Brazil</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Research Centre for Artificial Intelligence, DFKI, Bremen, Germany</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Lanz-Mendoza</surname><given-names>Humberto</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>Instituto Nacional de Salud P&#x000fa;blica, MEXICO</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="other" id="econtrib001"><p>&#x02021; These authors also contributed equally to this work.</p></fn><corresp id="cor001">* E-mail: <email>brunam@fieb.org.br</email></corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>1</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>14</volume><issue>1</issue><elocation-id>e0210829</elocation-id><history><date date-type="received"><day>3</day><month>9</month><year>2018</year></date><date date-type="accepted"><day>2</day><month>1</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2019 Motta et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Motta et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0210829.pdf"/><abstract><p>Dengue, chikungunya and Zika are arboviruses transmitted by mosquitos of the genus <italic>Aedes</italic> and have caused several outbreaks in world over the past ten years. Morphological identification of mosquitos is currently restricted due to the small number of adequately trained professionals. We implemented a computational model based on a convolutional neural network (CNN) to extract features from mosquito images to identify adult mosquitoes from the species <italic>Aedes aegypti</italic>, <italic>Aedes albopictus</italic> and <italic>Culex quinquefasciatus</italic>. To train the CNN to perform automatic morphological classification of mosquitoes, we used a dataset that included 4,056 mosquito images. Three neural networks, including LeNet, AlexNet and GoogleNet, were used. During the validation phase, the accuracy of the mosquito classification was 57.5% using LeNet, 74.7% using AlexNet and 83.9% using GoogleNet. During the testing phase, the best result (76.2%) was obtained using GoogleNet; results of 52.4% and 51.2% were obtained using LeNet and AlexNet, respectively. Significantly, accuracies of 100% and 90% were achieved for the classification of <italic>Aedes</italic> and <italic>Culex</italic>, respectively. A classification accuracy of 82% was achieved for <italic>Aedes</italic> females. Our results provide information that is fundamental for the automatic morphological classification of adult mosquito species in field. The use of CNN's is an important method for autonomous identification and is a valuable and accessible resource for health workers and taxonomists for the identification of some insects that can transmit infectious agents to humans.</p></abstract><funding-group><funding-statement>The study was unfunded. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="6"/><table-count count="5"/><page-count count="18"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are within the manuscript.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are within the manuscript.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Arthropod-borne viruses are responsible for more than 100 of the diseases that comprise the estimated global burden of communicable human diseases[<xref rid="pone.0210829.ref001" ref-type="bibr">1</xref>]. Vector-borne diseases cause more than one billion infections and more than one million deaths in humans every year[<xref rid="pone.0210829.ref002" ref-type="bibr">2</xref>]. Dengue, chikungunya and Zika are the most common arboviruses and have caused several major epidemics in France Polynesia and Latin America during the past 10 years[<xref rid="pone.0210829.ref003" ref-type="bibr">3</xref>&#x02013;<xref rid="pone.0210829.ref006" ref-type="bibr">6</xref>]. These three viral diseases are among the greatest public health challenges in the world[<xref rid="pone.0210829.ref007" ref-type="bibr">7</xref>]. Entomology research is considered to be a priority by the World Health Organization to develop tools that can be applied to reduce incidence and mortality and prevent epidemics caused by vector-borne diseases worldwide[<xref rid="pone.0210829.ref002" ref-type="bibr">2</xref>].</p><p><italic>Aedes aegypti</italic> and <italic>Aedes albopictus</italic> have received worldwide attention since both species are efficient vectors for the transmission of human arboviral diseases, such as Zika, dengue, chikungunya, and yellow fever[<xref rid="pone.0210829.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0210829.ref008" ref-type="bibr">8</xref>,<xref rid="pone.0210829.ref009" ref-type="bibr">9</xref>]. Additionally, controversial studies have also indicated that the genus <italic>Culex</italic> could be a possible vector of the Zika virus[<xref rid="pone.0210829.ref010" ref-type="bibr">10</xref>&#x02013;<xref rid="pone.0210829.ref012" ref-type="bibr">12</xref>]. <italic>Ae</italic>. <italic>aegypti</italic>, <italic>Ae</italic>. <italic>albopictus</italic> and <italic>C</italic>. <italic>quinquefasciatus</italic> are very common domiciliary mosquito vectors that are present in almost all urban areas in Brazil[<xref rid="pone.0210829.ref007" ref-type="bibr">7</xref>]. <italic>Ae</italic>. <italic>aegypti</italic>, <italic>Ae</italic>. <italic>Albopictus</italic> are, among all <italic>Aedes</italic> species, the ones that circulate in endemic areas. Furthermore, <italic>Aedes</italic> mosquitoes have been demonstrated to be capable of transmitting Zika and other viruses in many tropical areas in the Americas, Africa, and Asia[<xref rid="pone.0210829.ref013" ref-type="bibr">13</xref>].</p><p>Entomological characterization is fundamental for acquiring information about mosquito behaviour. In general, the current procedure used to identify the species of an insect requires an individual visual examination of the insect, which is time consuming and requires several years of experience[<xref rid="pone.0210829.ref014" ref-type="bibr">14</xref>,<xref rid="pone.0210829.ref015" ref-type="bibr">15</xref>]. While the general interest in documenting the diversity of insect species has grown exponentially over the years, the number of taxonomists and other professionals trained in species identification has steadily declined[<xref rid="pone.0210829.ref016" ref-type="bibr">16</xref>&#x02013;<xref rid="pone.0210829.ref018" ref-type="bibr">18</xref>]. Important factors that can impede the correct identification of mosquito species include the method of preservation used during the transport of samples and the use of the appropriate equipment to capture mosquitoes without damaging them[<xref rid="pone.0210829.ref019" ref-type="bibr">19</xref>]. Due to that, developing a tool to allow the classification of adult mosquitoes in the field, considering the environmental issues, would be very important[<xref rid="pone.0210829.ref020" ref-type="bibr">20</xref>].</p><p>Another possibility for species identification is the use of molecular techniques that have been validated by various studies, such DNA barcoding, environmental DNA testing and real-time PCR (qPCR)[<xref rid="pone.0210829.ref021" ref-type="bibr">21</xref>&#x02013;<xref rid="pone.0210829.ref023" ref-type="bibr">23</xref>]. However, molecular identification of mosquitoes is a slow and expensive process for most laboratories.</p><p>Recently, new models that facilitate the automatic classification of mosquitoes have been developed. Some studies have attempted to classify mosquito species based on the frequency and harmonics of their wingbeats[<xref rid="pone.0210829.ref024" ref-type="bibr">24</xref>,<xref rid="pone.0210829.ref025" ref-type="bibr">25</xref>]. Techniques based on image feature analysis have also been used as a classification method [<xref rid="pone.0210829.ref026" ref-type="bibr">26</xref>&#x02013;<xref rid="pone.0210829.ref029" ref-type="bibr">29</xref>]. In addition, Machine Learning and Deep Learning techniques have been used for mosquito classification[<xref rid="pone.0210829.ref030" ref-type="bibr">30</xref>&#x02013;<xref rid="pone.0210829.ref032" ref-type="bibr">32</xref>]. Most of these studies, however, aimed to develop methods and tools that requires a laboratory environment. The development of a tool to support taxonomist and health workers in the field would be very helpful in accelerating the knowledge of which mosquito is circulating in the community and supporting health authorities in controlling harmful mosquitoes, once it will reduce the lag between the time the trap is placed and the taxonomic inspection occurs[<xref rid="pone.0210829.ref033" ref-type="bibr">33</xref>].</p><p>Wingbeat vibration techniques have been reported to demonstrate a high accuracy in the classification of mosquitoes[<xref rid="pone.0210829.ref032" ref-type="bibr">32</xref>]. However, the sensors used have a limited amount of memory and are unable to store the entire data stream for later processing. Indeed, the sensors must process the data stream in real time to identify events of interest and filter out background noise[<xref rid="pone.0210829.ref034" ref-type="bibr">34</xref>].</p><p>Within the field of image feature analysis, studies have been conducted to identify mosquitoes according to their life cycle stage (egg, larval phase, pupal stage and adult). Some studies have attempted to automatize the process of egg counting to assess the level of fecundity and thereby estimate the mosquito population size and conduct morphometric analysis[<xref rid="pone.0210829.ref035" ref-type="bibr">35</xref>&#x02013;<xref rid="pone.0210829.ref038" ref-type="bibr">38</xref>]. Studies conducted to classify mosquito-based image in adult phase still focus on parts of the body of the mosquito. The outlines of body parts, such as wings, are stable and diverse but have not been frequently used in conventional taxonomy due to difficulties with lexical descriptions[<xref rid="pone.0210829.ref015" ref-type="bibr">15</xref>]. A classification method capable of analysing the features of the whole body is essential, since in the field there is no equipment that allow the health worker to separate parts of the body and analyse it.</p><p>Deep learning methods are essential for the processes underlying general object recognition[<xref rid="pone.0210829.ref039" ref-type="bibr">39</xref>]. In 2012, during the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which was the largest contest conducted in the object recognition field, a breakthrough in deep learning occurred when a convolutional network won the competition by reducing the state-of-the-art top-5 error rate from 26.1% to 15.3%[<xref rid="pone.0210829.ref040" ref-type="bibr">40</xref>]. Convolutional neural networks (CNN) constitute a class of models that utilize prior knowledge to compensate for data that is not available[<xref rid="pone.0210829.ref039" ref-type="bibr">39</xref>]. A CNN method has been developed to classify <italic>Aedes</italic> mosquito larva using a small dataset; when 200 epochs were used, the network achieved 96.8% accuracy[<xref rid="pone.0210829.ref031" ref-type="bibr">31</xref>].</p><p>It should be emphasized that correct identification of adult <italic>Aedes</italic> mosquito species is essential for both the recognition of the vectors involved in disease transmission and the development of fast and efficient control strategies[<xref rid="pone.0210829.ref015" ref-type="bibr">15</xref>]. In this study, we compared three CNN's, LeNet, AlexNet and GoogLeNet to evaluate the potential application of a Deep Learning technique to classify adult mosquito species in the field. The objective is to develop an epidemiological tool to be used in the real-world environment to facilitate the work of entomologists and health workers in the classification of adult mosquitoes of the species <italic>Ae</italic>. <italic>aegypti</italic>, <italic>Ae</italic>. <italic>albopictus</italic> and <italic>C</italic>. <italic>quinquefasciatus</italic>. As showed, this tool would be very useful to reduce the time, to allow its use by less experienced expert and to capture preserved features of the body characteristics of the mosquito. Another objective of this work is to initiate a preliminary scientific study that would allow the community be part of the control of vector-borne diseases.</p><p>Considering that the selection of algorithm training parameters is essential to improve the accuracy and reliability of the method, this study also applied the statistical analysis prior to the selection to determine the parameters set to be used to classify the mosquitoes.</p></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and methods</title><sec id="sec003"><title>Ethics statement</title><p>No permits were required for sampling for this study. The field sampling did not involve any endangered or protected species.</p></sec><sec id="sec004"><title>Sample collection</title><p>The mosquito samples used for image capture were obtained from the Parasitology Laboratory of the Federal University of Bahia&#x02013;UFBA (Salvador, Brazil) and were also collected in the field. The samples obtained from UFBA were <italic>Ae</italic>. <italic>aegypti</italic> mosquitos (10 females and 6 males). The capture of adult insects in the field resulted in the collection of 73 specimens of <italic>Ae</italic>. <italic>aegypti</italic>, 94 specimens of <italic>Ae</italic>. <italic>albopictus</italic> and 110 specimens of <italic>C</italic>. <italic>quinquefasciatus</italic>.</p><p>The field sampling took place between September and October 2017 in the city of Salvador in two collection areas (Bahia, Brazil). CDC light traps and suction tubes were used for the collection of adult insects.</p><p>The captured specimens were euthanized with ethyl acetate and stored in entomological collection tubes until identification was performed by an entomologist.</p></sec><sec id="sec005"><title>Construction of the dataset: Structure, acquisition and distribution</title><p>A robust, correctly structured dataset was created to allow the differentiation of the <italic>Ae</italic>. <italic>aegypti</italic>, <italic>Ae</italic>. <italic>albopictus</italic> and <italic>C</italic>. <italic>quinquefasciatus</italic> species. In addition, differentiation of males and females was conducted as described in a previous study[<xref rid="pone.0210829.ref041" ref-type="bibr">41</xref>]. A model was developed to classify the mosquitoes into six different classes according to gender and species. At this stage, our work does not intend to classify other species and non-mosquitoes images.</p><p>The images in the dataset were extracted from the ImageNet platform and were also photographed with various cameras, including a Leica DMC2900 (Leica Microsystems, Heerbrugg, Switzerland) coupled to a stereoscopic Leica M205C at the Oswaldo Cruz Institute of Entomology at FIOCRUZ (Rio de Janeiro&#x02013;Brazil), a Canon Power Shot D30 (Canon, Tokyo, Japan) coupled to a Wild M3C stereomicroscope (Leica Microsystems, Heerbrugg, Switzerland) at SENAI CIMATEC (Salvador&#x02013;Brazil) and Samsung J5 (Samsung, Seoul, South Korea) and Apple iPhone 7 (Apple, Cupertino, California, USA) mobile phone cameras. The images were collected at different resolutions and levels of quality to develop a classification method that utilized a wide range of images to prevent possible overfitting of the trained model. An average of 10 photographs of each specimen were taken at different angles and proximities.</p><p>Once the dataset was structured and the images acquired, the number of images to be used for training, validating and testing the model was determined. <xref ref-type="fig" rid="pone.0210829.g001">Fig 1</xref> summarizes how the dataset was used in the computational model and shows the three different phases of the development of the model: training (phase 1), validation (phase 2) and testing (phase 3).</p><fig id="pone.0210829.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.g001</object-id><label>Fig 1</label><caption><title>Representation of the training (phase 1), validating (phase 2) and testing (phase 3) processes of the mosquito classification model.</title></caption><graphic xlink:href="pone.0210829.g001"/></fig></sec><sec id="sec006"><title>Determination of the application images</title><p>The application images were those that we intended use for the classification of mosquitoes after the model was developed. This was done to allow the model to be used by any person, not only a specialist using a microscope, who is able to photograph an insect and then run the model to classify mosquitoes.</p><p>A total of 823 out of the 4,056 images used in this study were manually selected according to the following characteristics: the image should show the entire insect body (to visualize all relevant morphological features) and should have been taken with a mobile phone. <xref ref-type="fig" rid="pone.0210829.g002">Fig 2</xref> shows some captured images of male and female <italic>Ae</italic>. <italic>aegypti</italic> photographed using a digital camera (<xref ref-type="fig" rid="pone.0210829.g002">Fig 2A, 2B, 2C and 2D</xref>) and mobile phone (<xref ref-type="fig" rid="pone.0210829.g002">Fig 2E and 2F</xref>).</p><fig id="pone.0210829.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.g002</object-id><label>Fig 2</label><caption><title>Images used for the validation phase during the development of the model used in this study.</title><p>(A and C) <italic>Ae</italic>. <italic>aegypti</italic> females photographed using a digital camera; (B and D) <italic>Ae</italic>. <italic>aegypti</italic> males photographed using a digital camera; (E) an <italic>Ae</italic>. <italic>aegypti</italic> female photographed using a mobile phone; (F) an <italic>Ae</italic>. <italic>aegypti</italic> male photographed using a mobile phone.</p></caption><graphic xlink:href="pone.0210829.g002"/></fig></sec><sec id="sec007"><title>Determination of the application images to be used for testing</title><p>For the testing phase (phase 3), images were extracted from the dataset and used only after the model was validated. Additionally, the images used for testing were not used to train or validate the model. This was done to avoid research bias and minimize the chance of overfitting.</p><p>To standardize the analysis of the results, the test dataset contained the same number of images from each of the six classes of mosquitoes. It was decided that the number of images corresponding to 20% of the number of images in the smallest class would be used for testing for all classes. As the <italic>C</italic>. <italic>quinquefasciatus</italic> female class was the class with the smallest number of application images (70 images), fourteen images (20%) from each of the other classes were randomly removed from the dataset. Python scripts were employed to randomize the chosen images.</p></sec><sec id="sec008"><title>Distribution of the application images used for the training and validation phases</title><p>Within the references used in this work, no consensus was found concerning the ideal dataset distribution between the training (phase 1), validation (phase 2) and testing (phase 3) phases.</p><p>To determine the best image distribution for training and validation, once the test sample size was already defined, the percentage of application images used in the training phase was defined as a factor, varying in four levels (either 30%, 40%, 50% or 60%), in a full factorial experiment. The objective was to evaluate statistically the best distribution of application images between the training and validation phases that would present the best classification accuracy for the intended application in entomology.</p></sec><sec id="sec009"><title>Selection of neural networks</title><p>We used nVidia DIGITS software (nVidia, Santa Clara, California, USA) with a NVIDIA GeForce GTX TITAN GPU for processing. We used the software on the Linux Ubuntu operating system LTS Distribution 16.04 (Linux, San Francisco, California, USA). The Caffe framework was used for the LeNet[<xref rid="pone.0210829.ref042" ref-type="bibr">42</xref>,<xref rid="pone.0210829.ref043" ref-type="bibr">43</xref>], AlexNet[<xref rid="pone.0210829.ref031" ref-type="bibr">31</xref>] and GoogLeNet convolutional neural networks [<xref rid="pone.0210829.ref042" ref-type="bibr">42</xref>,<xref rid="pone.0210829.ref043" ref-type="bibr">43</xref>].</p><p>For a long period, LeNet was considered the state of the art of Artificial Neural Networks (ANN's). LeNet was one of the first ANN's used to improve the original backpropagation algorithm; the remarkable development of this well-known network was the first step towards the application of the Deep Learning method.</p><p>In 2012, Alex Krizhevsky released AlexNet, which was a deeper and much wider version of the LeNet that won, by a large margin, the difficult ImageNet competition[<xref rid="pone.0210829.ref039" ref-type="bibr">39</xref>]. AlexNet has a very similar architecture to that of LeNet, but it can better describe images. This network relies on eight layers, including convolutional, local responses, max-pooling and fully connected layers.</p><p>In 2014, Christian Szegedy, from Google, began to investigate methods to reduce the computational burden of deep neural networks[<xref rid="pone.0210829.ref043" ref-type="bibr">43</xref>]. In doing so, he focused on the efficiency of the architecture of the deep neural network (codenamed <italic>Inception</italic>, later named <italic>GoogLeNet)</italic>. This neural network was the winner of ILSVRC 2014, surpassing AlexNet, and was proclaimed the new paradigm for convolutional neural networks.</p></sec><sec id="sec010"><title>Classification of the model parameters</title><p>The CNN algorithms requires the definition of several parameters prior to the training phase. Depending on the application, these parameters might have more or less influence on the classification results. Prior random tests were performed to understand which set parameters have more influence and should be statistically analysed as a factor in the full factorial experiment in order to stablish the best value. Other parameters with less influence on the results were set as default in the algorithm.</p><p>Number of epochs and Seed were parameters defined as a constant value. The prior random tests showed that 200 epochs are enough to stabilize the training and validation phases. In case it generates overfitting it would be easily identified. Define the seed as a constant value was important to standardize the neural network weights initiation for all CNN and also, for each time we run the algorithm. The parameters evaluated in the full factorial experiment were: percentage of the application image used in the training phase; solver algorithm; the learning rate (LR); and learning rate decay function. The other parameters, such as the batch size, batch accumulation, learning rate step size, learning rate power, learning rate gamma, crop size and mean subtraction, were set according to pre-defined default for each network.</p></sec><sec id="sec011"><title>Full factorial experiments</title><p>To understand the influence of each parameter and their correlations with one another, full factorial experiments were conducted using the Minitab 17 software (Minitab, State College, Pennsylvania, USA). The parameters used as the inputs for the full factorial experiments are shown in <xref rid="pone.0210829.t001" ref-type="table">Table 1</xref>.</p><table-wrap id="pone.0210829.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.t001</object-id><label>Table 1</label><caption><title>Parameters used as inputs for the full factorial experiments in this study.</title></caption><alternatives><graphic id="pone.0210829.t001g" xlink:href="pone.0210829.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="2" colspan="1">Parameters</th><th align="center" rowspan="2" colspan="1">Levels</th><th align="center" colspan="4" rowspan="1">Level</th></tr><tr><th align="center" rowspan="1" colspan="1">1</th><th align="center" rowspan="1" colspan="1">2</th><th align="center" rowspan="1" colspan="1">3</th><th align="center" rowspan="1" colspan="1">4</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Number of epochs</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" colspan="4" rowspan="1">200</td></tr><tr><td align="left" rowspan="1" colspan="1">Seed</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" colspan="4" rowspan="1">10</td></tr><tr><td align="left" rowspan="1" colspan="1">% application images used at training</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">30</td><td align="center" rowspan="1" colspan="1">40</td><td align="center" rowspan="1" colspan="1">50</td><td align="center" rowspan="1" colspan="1">60</td></tr><tr><td align="left" rowspan="1" colspan="1">Solver algorithm</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">SGD</td><td align="center" rowspan="1" colspan="1">NAG</td><td align="center" rowspan="1" colspan="1">Adam</td><td align="center" rowspan="1" colspan="1">AdaGrad</td></tr><tr><td align="left" rowspan="1" colspan="1">Learning rate</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">0.001</td><td align="center" rowspan="1" colspan="1">0.01</td><td align="center" rowspan="1" colspan="1">0.1</td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">LR decay function</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">Step Down</td><td align="center" rowspan="1" colspan="1">Exponential</td><td align="center" rowspan="1" colspan="1">Sigmoid</td><td align="center" rowspan="1" colspan="1">Polynomial</td></tr></tbody></table></alternatives></table-wrap><p>Using the Minitab software, we created an experiment with four factors: the percentage of application images used at training, solver algorithm, learning rate, and LR decay function. The number of epochs and the seed was fixed. For the experiments, we defined the number of levels as shown in <xref rid="pone.0210829.t001" ref-type="table">Table 1</xref>. During the next step, the formats of the selected parameters were defined as either numerical or text; the percentage of application images used at training and the LR were defined as numerical, and the solver algorithm and LR decay function were defined as text. The values for each level was defined based on the random experiments performed before.</p><p>Based on the result of the factorial experiments, it was calculated that 192 experiments should be conducted on one of the three convolutional neural networks. The output variables to be evaluated were the percent accuracy during the validation phase, loss function and percent accuracy during the testing phase.</p></sec><sec id="sec012"><title>Full factorial experiments analysis</title><p>Based on the full factorial experiments, GoogLeNet was the network selected for conducting the 192 planned experiments.</p><p>The first step in the analysis was to interpret the statistical significance of the parameters that were used (percent accuracy during the validation phase, loss function and percent accuracy during the testing phase) and their relationship with the results. The &#x02018;P value&#x02019; was calculated to evaluate the statistical significance of the parameters and the effects of their interactions on the outcome of the experiments.</p><p>The best performance obtained for each outcome was then defined as the target value for optimization analysis and was used to determine the best combination of parameters. These combinations were used to train the other two convolutional neural networks, LeNet and AlexNet.</p></sec></sec><sec sec-type="results" id="sec013"><title>Results</title><p>The species and gender of each of the dataset classes used to classify the 4,056 images are shown in <xref rid="pone.0210829.t002" ref-type="table">Table 2</xref>.</p><table-wrap id="pone.0210829.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.t002</object-id><label>Table 2</label><caption><title>Dataset structure and sample sizes of the mosquito images used in this study.</title></caption><alternatives><graphic id="pone.0210829.t002g" xlink:href="pone.0210829.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Class name</th><th align="center" rowspan="1" colspan="1">Total number of images</th><th align="center" rowspan="1" colspan="1">Non-application images (used in training)</th><th align="center" rowspan="1" colspan="1">Application images</th><th align="center" rowspan="1" colspan="1">Application images used for training and validation</th><th align="center" rowspan="1" colspan="1">Application images used for testing</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>aegypti</italic> female</td><td align="center" rowspan="1" colspan="1">947</td><td align="center" rowspan="1" colspan="1">723</td><td align="center" rowspan="1" colspan="1">224</td><td align="center" rowspan="1" colspan="1">210</td><td align="center" rowspan="1" colspan="1">14</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>aegypti</italic> male</td><td align="center" rowspan="1" colspan="1">282</td><td align="center" rowspan="1" colspan="1">201</td><td align="center" rowspan="1" colspan="1">81</td><td align="center" rowspan="1" colspan="1">67</td><td align="center" rowspan="1" colspan="1">14</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>albopictus</italic> female</td><td align="center" rowspan="1" colspan="1">1050</td><td align="center" rowspan="1" colspan="1">821</td><td align="center" rowspan="1" colspan="1">229</td><td align="center" rowspan="1" colspan="1">215</td><td align="center" rowspan="1" colspan="1">14</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>albopictus</italic> male</td><td align="center" rowspan="1" colspan="1">436</td><td align="center" rowspan="1" colspan="1">327</td><td align="center" rowspan="1" colspan="1">109</td><td align="center" rowspan="1" colspan="1">95</td><td align="center" rowspan="1" colspan="1">14</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>C</italic>. <italic>quinquefasciatus</italic> female</td><td align="center" rowspan="1" colspan="1">965</td><td align="center" rowspan="1" colspan="1">895</td><td align="center" rowspan="1" colspan="1">70</td><td align="center" rowspan="1" colspan="1">56</td><td align="center" rowspan="1" colspan="1">14</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>C</italic>. <italic>quinquefasciatus</italic> male</td><td align="center" rowspan="1" colspan="1">376</td><td align="center" rowspan="1" colspan="1">266</td><td align="center" rowspan="1" colspan="1">110</td><td align="center" rowspan="1" colspan="1">96</td><td align="center" rowspan="1" colspan="1">14</td></tr></tbody></table></alternatives></table-wrap><p><xref ref-type="fig" rid="pone.0210829.g003">Fig 3</xref> shows the effects of each parameter (1<sup>st</sup> level analysis) on the accuracy of the testing phase (phase 3). In this analysis, the use of the solver algorithm &#x02018;3&#x02019; (Adam) and a learning rate of &#x02018;3&#x02019; (0.1) significantly reduced the accuracy of the mosquito image classification. However, a learning rate of &#x02018;1&#x02019; (0.001) was an important parameter that was used (&#x02018;p value&#x02019; &#x0003c; 0.00001). Almost every parameter had a significant influence on the results of the performance of the trained neural network (&#x02018;p value&#x02019; &#x0003c; 0.05), with the exception of the interaction between &#x02018;% application images (train) * Solver&#x02019; and &#x02018;% application images (train) * LR Decay Function&#x02019; (&#x02018;p value&#x02019; &#x0003e; 0.05).</p><fig id="pone.0210829.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.g003</object-id><label>Fig 3</label><caption><title>Effects of the first level parameters on the accuracy of the testing phase (average accuracy percentage).</title></caption><graphic xlink:href="pone.0210829.g003"/></fig><p><xref ref-type="fig" rid="pone.0210829.g004">Fig 4</xref> shows the effects of the parameter combination (2<sup>nd</sup> level analysis) on the interaction of the parameters with one another. It is clear that a learning rate of &#x02018;3&#x02019; (0.1) reduced the accuracy of the testing phase when combined with any of the other parameters. Consequently, this learning rate should not be used subsequent analysis. It was also confirmed that the solver algorithm &#x02018;3&#x02019; (Adam) reduced the accuracy of the testing phase. However, the combination of a learning rate of &#x02018;2&#x02019; (0.01) with solver algorithm &#x02018;4&#x02019; (AdaGrad) obtained better results than the combination of a learning rate of &#x02018;1&#x02019; (0.001) with the same solver algorithm (&#x02018;p value&#x02019; &#x0003c; 0.00001). Overall, the learning rate of &#x02018;1&#x02019; (0.001) produced the best results.</p><fig id="pone.0210829.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.g004</object-id><label>Fig 4</label><caption><title>Effects of the second level parameters on the accuracy of the testing phase (average accuracy percentage).</title></caption><graphic xlink:href="pone.0210829.g004"/></fig><p>To define the best set of parameters, an optimization function was calculated in Minitab that set the target values for each outcome. <xref ref-type="fig" rid="pone.0210829.g005">Fig 5</xref> shows the variation of the data obtained for each outcome. The best performance in terms of the percentage accuracy during validation was 83.9%, with a loss function of 0.67 and percentage accuracy during testing of 86.9%.</p><fig id="pone.0210829.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.g005</object-id><label>Fig 5</label><caption><p><bold>Histograms representing the outcome data</bold>: (A) percent accuracy&#x02013;validation, (B) percent accuracy&#x02013;testing, (C) loss function.</p></caption><graphic xlink:href="pone.0210829.g005"/></fig><p>The best parameter combination was obtained in the 4-2-1-3 set, which resulted in the use of 60% of the application images during training (the NAG Solver algorithm; 0.001 LR and Sigmoid LR decay function). Therefore, this parameter set was used for the subsequent training of all three convolutional neural networks.</p><p><xref ref-type="fig" rid="pone.0210829.g006">Fig 6</xref> shows the results obtained during the training and validation phases for the convolutional neural networks LeNet, AlexNet and GoogLeNet.</p><fig id="pone.0210829.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.g006</object-id><label>Fig 6</label><caption><p><bold>Representation of the training process of each CNN: (A) LeNet, (B) AlexNet and (C) GoogLeNet (orange line: accuracy validation; green line: loss validation; blue line: loss training).</bold> A&#x02013;LeNet (after 200 epochs): percent accuracy (validation) 57.50%, loss (validation) 2.07, loss (training) 0.03; B&#x02013;AlexNet (after 200 epochs): percent accuracy (validation) 74.69%, loss (validation) 0.83, loss (training) 0.11; C&#x02013;GoogLeNet (after 200 epochs): percent accuracy (validation) 83.88%, loss (validation) 1.03, loss (training) 0.00.</p></caption><graphic xlink:href="pone.0210829.g006"/></fig><p>The LeNet neural network achieved an accuracy of 57.5% for the classification of mosquitoes (<xref ref-type="fig" rid="pone.0210829.g006">Fig 6A</xref>). Interestingly, the loss function (2.07) significantly increased during the training of the network, indicating possible overfitting. <xref rid="pone.0210829.t003" ref-type="table">Table 3</xref> presents the accuracy achieved by this network for each mosquito species. The overall performance of the LeNet neural network during the testing phase (phase 3) was 52.4%.</p><table-wrap id="pone.0210829.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.t003</object-id><label>Table 3</label><caption><title>Confusion matrix showing the results of the testing phase for the LeNet neural network.</title></caption><alternatives><graphic id="pone.0210829.t003g" xlink:href="pone.0210829.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Class</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>aegypti</italic> female</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>aegypti</italic> male</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>albopictus</italic> female</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>albopictus</italic> male</th><th align="center" rowspan="1" colspan="1"><italic>C</italic>. <italic>quinquefasciatus</italic> female</th><th align="center" rowspan="1" colspan="1"><italic>C</italic>. <italic>quinquefasciatus</italic> male</th><th align="center" rowspan="1" colspan="1">Accuracy (%)</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>aegypti</italic> female</bold></td><td align="center" rowspan="1" colspan="1">7</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">50.00</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>aegypti</italic> male</bold></td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">42.86</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>albopictus</italic> female</bold></td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">71.43</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>albopictus</italic> male</bold></td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">7</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">50.00</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>C</italic>. <italic>quinquefasciatus</italic> female</bold></td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">64.29</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>C</italic>. <italic>quinquefasciatus</italic> male</bold></td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">35.71</td></tr></tbody></table></alternatives></table-wrap><p><xref ref-type="fig" rid="pone.0210829.g006">Fig 6B</xref> shows the results obtained during the training and validation phases for the AlexNet neural network. Using the previously selected parameters (200 epochs; seed of 10; 60% of application images used for training; NAG solver algorithm; 0.001 LR; sigmoid LR decay function), the network achieved an accuracy of 74.7%, and the validation and training losses were reduced by approximately 0.83 and 0.11, respectively. <xref rid="pone.0210829.t004" ref-type="table">Table 4</xref> presents the overall accuracy obtained during the testing phase (phase 3) for each of the mosquito species for the AlexNet neural network. The overall result of the testing phase was 51.2%.</p><table-wrap id="pone.0210829.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.t004</object-id><label>Table 4</label><caption><title>Confusion matrix showing the results of the testing phase for the AlexNet neural network.</title></caption><alternatives><graphic id="pone.0210829.t004g" xlink:href="pone.0210829.t004"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Class</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>aegypti</italic> female</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>aegypti</italic> male</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>albopictus</italic> female</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>albopictus</italic> male</th><th align="center" rowspan="1" colspan="1"><italic>C</italic>. <italic>quinquefasciatus</italic> female</th><th align="center" rowspan="1" colspan="1"><italic>C</italic>. <italic>quinquefasciatus</italic> male</th><th align="center" rowspan="1" colspan="1">Accuracy (%)</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>aegypti</italic> female</bold></td><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">35.71</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>aegypti</italic> male</bold></td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">7</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">50.00</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>albopictus</italic> female</bold></td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">11</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">78.57</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>albopictus</italic> male</bold></td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">64.29</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>C</italic>. <italic>quinquefasciatus</italic> female</bold></td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">6</td><td align="center" rowspan="1" colspan="1">14.29</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>C</italic>. <italic>quinquefasciatus</italic> male</bold></td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">64.29</td></tr></tbody></table></alternatives></table-wrap><p><xref ref-type="fig" rid="pone.0210829.g006">Fig 6C</xref> shows the results obtained during training and validation phases for the GoogLeNet convolutional neural network. Using the previously selected parameters, the network achieved an accuracy of 83.9%, and the validation and training losses were reduced by approximately 1.03 and 0, respectively. <xref rid="pone.0210829.t005" ref-type="table">Table 5</xref> presents the accuracy of the results obtained by the GoogLeNet network during the testing phase for each of the mosquito species (phase 3). Overall, the performance of the GoogLeNet neural network during the testing phase was 76.2%.</p><table-wrap id="pone.0210829.t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0210829.t005</object-id><label>Table 5</label><caption><title>Confusion matrix showing the results of the testing phase for the GoogLeNet neural network.</title></caption><alternatives><graphic id="pone.0210829.t005g" xlink:href="pone.0210829.t005"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Class</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>aegypti</italic> female</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>aegypti</italic> male</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>albopictus</italic> female</th><th align="center" rowspan="1" colspan="1"><italic>Ae</italic>. <italic>albopictus</italic> male</th><th align="center" rowspan="1" colspan="1"><italic>C</italic>. <italic>quinquefasciatus</italic> female</th><th align="center" rowspan="1" colspan="1"><italic>C</italic>. <italic>quinquefasciatus</italic> male</th><th align="center" rowspan="1" colspan="1">Accuracy (%)</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>aegypti</italic> female</bold></td><td align="center" rowspan="1" colspan="1">9</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">64.29</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>aegypti</italic> male</bold></td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">12</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">85.71</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>albopictus</italic> female</bold></td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">12</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">85.71</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>Ae</italic>. <italic>albopictus</italic> male</bold></td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">13</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">92.86</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>C</italic>. <italic>quinquefasciatus</italic> female</bold></td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">8</td><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">57.14</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold><italic>C</italic>. <italic>quinquefasciatus</italic> male</bold></td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">3</td><td align="center" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">71.43</td></tr></tbody></table></alternatives></table-wrap><p>With the exception of its performance during the classification of <italic>C</italic>. <italic>quinquefasciatus</italic> in the validation phase in terms of the loss function after 200 epochs, GoogLeNet demonstrated the best performance for the classification of adult mosquitoes.</p><p>It is also relevant to evaluate the classification performance for the different genera of mosquitoes. For the classification of the <italic>Aedes</italic> and <italic>Culex</italic> genera, an accuracy of 96.4% was obtained. Another important outcome was the performance in the classification of <italic>Aedes</italic> females, during which an overall accuracy of 82.1% was achieved.</p></sec><sec sec-type="conclusions" id="sec014"><title>Discussion</title><p>To the best of our knowledge, this is the first study that uses convolutional neural networks (CNN's) to extract features from images of adult mosquitoes to identify <italic>Ae</italic>. <italic>aegypti</italic> and <italic>Ae</italic>. <italic>albopictus</italic> species in the field. Members of the genus <italic>Aedes</italic> are well-known carriers and disseminators of arboviruses[<xref rid="pone.0210829.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0210829.ref013" ref-type="bibr">13</xref>,<xref rid="pone.0210829.ref044" ref-type="bibr">44</xref>,<xref rid="pone.0210829.ref045" ref-type="bibr">45</xref>]. Also, we included the <italic>C</italic>. <italic>quinquefasciatus</italic> mosquito in our study because its medical importance as a vector for some arboviruses associated with the recent explosive epidemic occurred in Brazil[<xref rid="pone.0210829.ref010" ref-type="bibr">10</xref>,<xref rid="pone.0210829.ref012" ref-type="bibr">12</xref>]. However, the competence of <italic>Culex sp</italic>. to transmit Zika virus has not been confirmed by recent studies[<xref rid="pone.0210829.ref011" ref-type="bibr">11</xref>,<xref rid="pone.0210829.ref046" ref-type="bibr">46</xref>].</p><p>Sanchez-Ortiz et al.[<xref rid="pone.0210829.ref031" ref-type="bibr">31</xref>] used a very similar CNN-like method as our study, but focused on the use of digital images of larva as input for a machine learning algorithm for <italic>Aedes</italic> larva identification. The algorithm showed excellent performance, with 100% accuracy, in the identification of <italic>Aedes</italic> larva; however, for other mosquito species, the misclassification rate was 30%. Additionally, the sample size used in this study was tiny. Although their review have a significant scientific contribution, it requires the use of laboratory equipment and do not address issues such as time and logistics required in the traditional classification method for capturing and analysing the samples.</p><p>In our work, we used over 4,000 mosquito images in this preliminary study of the use of CNN's for the identification of mosquitoes; with such a small sample size, we achieved an accuracy of 76.2% in the automatic recognition of species and gender for six classes of mosquito. However, it is very promising that we were able to distinguish <italic>Aedes</italic> mosquitoes from other mosquitoes correctly. Our method was able to achieve 100% accuracy in classifying between <italic>Aedes</italic> and <italic>Culex</italic>, although <italic>Culex</italic> was misclassified as <italic>Aedes</italic> 10% of the time during the test phase. In the recognition of the <italic>Aedes</italic> female, which is the most important vector of arboviruses, we achieved 82% accuracy. It is essential to understand that it is a training process and every time we add a new species further training of the network will be necessary.</p><p>The importance of insects to transmit human diseases have motivated researchers to develop an arsenal of mechanical, chemical, biological and educational tools to help mitigate the harmful effects of insects. Batista et al.[<xref rid="pone.0210829.ref033" ref-type="bibr">33</xref>] reinforce that the efficiency of such systems depends on the knowledge of the spatiotemporal dynamics of insects. In that work, the authors developed a sensor that can measure the wingbeat frequency of flying insects at a distance. However, the study collected data under laboratory conditions. Thus, future works would be required to obtain additional features for real-world deployment.</p><p>Taxonomists have been searching for efficient methods to meet real world insect identification requirements[<xref rid="pone.0210829.ref047" ref-type="bibr">47</xref>]. The correct identification of insects is critically important in entomological research, pest control, and insect-resource utilization for the development of effective control strategies for diseases transmitted by these vectors and can aid in preventing arbovirus infections in humans[<xref rid="pone.0210829.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0210829.ref015" ref-type="bibr">15</xref>]. Also, identifying the vectors associated with pathogens is fundamental to understand the transmission of arthropod-borne viruses[<xref rid="pone.0210829.ref048" ref-type="bibr">48</xref>]. This problem is recognized since the end of the last century, and computer-based recognition systems have been developed to identify insect species automatically, not only for mosquitoes, but also for wasps, moths, spiders, bees and others[<xref rid="pone.0210829.ref015" ref-type="bibr">15</xref>].</p><p>Unfortunately, as mentioned previously, taxonomic identification of mosquitoes is a time-consuming and challenging process that requires trained specialists. The high variability also compromises this method of identification of the morphological and molecular characteristics found in members of the <italic>Culicidae</italic> subfamily, which was the object of our study. Furthermore, molecular identification, although generally accurate, is costly and often impractical. Other disadvantages of molecular techniques are the high cost and time applied in the laboratory. In this way, developing countries, which are often the main endemic areas, do not have sufficient resources for investments in laboratories with high technologies.</p><p>Although there is progress in autonomous identification systems for insects, two inadequacies remain. First, some systems obtain high accuracy of identification for limited insect species but have very narrow functionality, because the source code for those computer programs must be rewritten when these types of system are tested with new morphological insect data. Second, some systems, can be used to identify different taxonomic groups of insects, but the accuracy is not sufficient due to high variations during processes of digitizing morphological features and insufficient powerful classifiers which only can search local optimizing solutions rather than the global optimizing solutions during computation[<xref rid="pone.0210829.ref015" ref-type="bibr">15</xref>].</p><p>Autonomous identification method could be valuable and more accessible to health workers and other non-taxonomists for use in the identification of insects that can transmit infectious agents to humans. Herein, we demonstrated that it is possible to use a CNN to analyse photographic images to identify and classify mosquitoes by species and gender.</p><p>When adequate data are available, many methods, such as the Support Vector Machine (SVM) and Tangent Distance Classifiers (TDCs), can achieve good accuracy in automatic classification[<xref rid="pone.0210829.ref042" ref-type="bibr">42</xref>]. In a report by LeCun et al. on handwritten character recognition, more than 60,000 handwritten characters were used, resulting in a significantly reduced error rate[<xref rid="pone.0210829.ref042" ref-type="bibr">42</xref>]. Additionally, the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) uses approximately 1.2 million images for training, 50,000 for validation, and 150,000 images for testing[<xref rid="pone.0210829.ref039" ref-type="bibr">39</xref>].</p><p>In another application of a convolutional neural network for the identification of <italic>Aedes</italic> wings and larva, Lorenz et al.[<xref rid="pone.0210829.ref026" ref-type="bibr">26</xref>] used 17 species from the <italic>Anopheles</italic>, <italic>Aedes</italic> and <italic>Culex</italic> genera to test the hypothesis that classification based on wing shape characteristics using an Artificial Neural Network (ANN) was more accurate than traditional classification using discriminant analysis. The ANN was able to correctly classify species with greater accuracy than the conventional method using multivariate discriminant analysis, with an accuracy varying from 86% to 100%. However, this study focus on parts of the body of the mosquito and requires the use of laboratory equipment.</p><p>Another challenge in the use of convolutional neural networks for image recognition with a limited number of images is the risk of overfitting. Application of different sets of images during the training, validation and testing phases may reduce the risk of overfitting[<xref rid="pone.0210829.ref039" ref-type="bibr">39</xref>]. Additionally, the use of different, randomly selected images during the testing phase, as demonstrated in our study, is a promising method to avoid overfitting. Limitations due to dataset size and its balance between the classes may explain the low accuracy achieved for some species classified individually in our study. We firmly believe that increasing the dataset for all three phases (training, validation and testing) and the use of different neural network architectures will improve the precision of automatic classification.</p><p>Another possible way to increase the reliability of our neural network for the identification of mosquito images is to re-design the layers of the neural network to include additional morphological characteristics, such as wing shapes, palps and other features.</p><p>This preliminary study is an opportunity to construct a dense dataset of images of mosquitoes that can transmit diseases to humans, including <italic>Anopheles</italic> and other species among the 3,556 species of <italic>Culicidae</italic> that are currently recognized.</p><p>The final goal is to embed the model in a mobile APP that will allow for community participation and thereby facilitate efforts to control vector borne diseases.</p></sec><sec sec-type="conclusions" id="sec015"><title>Conclusion</title><p>This work demonstrates that CNN's can be applied to the autonomous classification of mosquitoes to aid in the screening of possible vectors of arboviruses by health workers and other non-taxonomists. High accuracy of classification of mosquitoes using CNN's can be achieved using different neural networks. LeNet and AlexNet had inferior performances to that of GoogLeNet, which indicates that the use of more complex networks with more layers is necessary to improve accuracy. Taxonomic classification of insects can be performed automatically via the acquisition of image's features that differentiate one insect from another. Only by using both the genus and sex we achieved high accuracy in the identification of <italic>Aedes</italic> females.</p><p>Our results provide information that is fundamental to the automatic morphological classification of a species of interest applying CNN to classify the adult mosquito in the environment it lives. In the critical discussion, the published works develop mainly a laboratory tool. In the taxonomy of adult mosquitoes, an epidemiological classification has a different implication. The application method we propose will be a robust epidemiological instrument for the rapid identification of focus of mosquitoes allowing the community to be part of the control of vector-borne diseases.</p><p>Another essential contribution relies on the construction of a mosquito dataset. Much data is required to increase the reliability of any autonomous method to recognize objects. This study already provides more than 4,000 mosquito images, which is vital in creating a significant and robust dataset for later applications.</p><p>We hope that later, the model can be embedded in a mobile APP to allow for community participation and thereby facilitate efforts to control vector borne diseases. Indeed, this model can be used to improve vector control operations that are linked with fast and reliable identification of targeted species and to provide knowledge of their biology and ecology.</p></sec></body><back><ack><p>We are thankful to the department that facilitated the collection of <italic>Culicidae</italic> at Fiocruz Rio de Janeiro. We give special thanks to Maycon Neves and Monique Motta, who are members of their staff. Additionally, we would like to thank Otavio Ribeiro and Jo&#x000e3;o Marcelo from the High Performance Computing Center of SENAI CIMATEC&#x02014;Bahia. Tiago Trocoli and Nelson Alves from the Brazilian Institute of Robotics, SENAI CIMATEC&#x02014;Bahia, provided valuable support.</p></ack><ref-list><title>References</title><ref id="pone.0210829.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Musso</surname><given-names>D</given-names></name>, <name><surname>Rodriguez-Morales</surname><given-names>AJ</given-names></name>, <name><surname>Levi</surname><given-names>JE</given-names></name>, <name><surname>Cao-Lormeau</surname><given-names>V-M</given-names></name>, <name><surname>Gubler</surname><given-names>DJ</given-names></name>. <article-title>Unexpected outbreaks of arbovirus infections: lessons learned from the Pacific and tropical America</article-title>. <source>Lancet Infect Di1s</source>. Elsevier; <year>2018</year>; <pub-id pub-id-type="doi">10.1016/S1473-3099(18)30269-X</pub-id></mixed-citation></ref><ref id="pone.0210829.ref002"><label>2</label><mixed-citation publication-type="other">WHO. Global Vector Control Response&#x02014;Background document to inform deliberations during the 70th session of the World Health Assembly. WHO. 2017;2030: 47.</mixed-citation></ref><ref id="pone.0210829.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Leta</surname><given-names>S</given-names></name>, <name><surname>Beyene</surname><given-names>TJ</given-names></name>, <name><surname>De Clercq</surname><given-names>EM</given-names></name>, <name><surname>Amenu</surname><given-names>K</given-names></name>, <name><surname>Kraemer</surname><given-names>MUG</given-names></name>, <name><surname>Revie</surname><given-names>CW</given-names></name>. <article-title>Global risk mapping for major diseases transmitted by Aedes aegypti and Aedes albopictus</article-title>. <source>Int J Infect Dis</source>. Elsevier; <year>2018</year>;<volume>67</volume>: <fpage>25</fpage>&#x02013;<lpage>35</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijid.2017.11.026</pub-id>
<?supplied-pmid 29196275?><pub-id pub-id-type="pmid">29196275</pub-id></mixed-citation></ref><ref id="pone.0210829.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Cauchemez</surname><given-names>S</given-names></name>, <name><surname>Besnard</surname><given-names>M</given-names></name>, <name><surname>Bompard</surname><given-names>P</given-names></name>, <name><surname>Dub</surname><given-names>T</given-names></name>, <name><surname>Guillemette-Artur</surname><given-names>P</given-names></name>, <name><surname>Eyrolle-Guignot</surname><given-names>D</given-names></name>, <etal>et al</etal>
<article-title>Association between Zika virus and microcephaly in French Polynesia, 2013&#x02013;2015: a retrospective study</article-title>. <source>Lancet (London, England)</source>. <year>2016</year>;<volume>387</volume>: <fpage>2125</fpage>&#x02013;<lpage>2132</lpage>. <pub-id pub-id-type="doi">10.1016/S0140-6736(16)00651-6</pub-id>
<?supplied-pmid 26993883?><pub-id pub-id-type="pmid">26993883</pub-id></mixed-citation></ref><ref id="pone.0210829.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>del Carpio-Orantes</surname><given-names>L</given-names></name>, <name><surname>Gonz&#x000e1;lez-Clemente</surname><given-names>M del C</given-names></name>, <name><surname>Lamothe-Aguilar</surname><given-names>T</given-names></name>. <article-title>Zika and its vector mosquitoes in Mexico</article-title>. <source>J Asia-Pacific Biodivers</source>. <year>2018</year>;<volume>11</volume>: <fpage>317</fpage>&#x02013;<lpage>319</lpage>. <pub-id pub-id-type="doi">10.1016/j.japb.2018.01.002</pub-id></mixed-citation></ref><ref id="pone.0210829.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Ding</surname><given-names>F</given-names></name>, <name><surname>Fu</surname><given-names>J</given-names></name>, <name><surname>Jiang</surname><given-names>D</given-names></name>, <name><surname>Hao</surname><given-names>M</given-names></name>, <name><surname>Lin</surname><given-names>G</given-names></name>. <article-title>Mapping the spatial distribution of Aedes aegypti and Aedes albopictus</article-title>. <source>Acta Trop</source>. <year>2018</year>;<volume>178</volume>: <fpage>155</fpage>&#x02013;<lpage>162</lpage>. <pub-id pub-id-type="doi">10.1016/j.actatropica.2017.11.020</pub-id>
<?supplied-pmid 29191515?><pub-id pub-id-type="pmid">29191515</pub-id></mixed-citation></ref><ref id="pone.0210829.ref007"><label>7</label><mixed-citation publication-type="other">PAHO. Tool for the diagnosis and care of patients with suspected arboviral diseases [Internet]. Us1.1. 2017. Available: <ext-link ext-link-type="uri" xlink:href="http://iris.paho.org/xmlui/handle/123456789/33895">http://iris.paho.org/xmlui/handle/123456789/33895</ext-link></mixed-citation></ref><ref id="pone.0210829.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Thangamani</surname><given-names>S</given-names></name>, <name><surname>Huang</surname><given-names>J</given-names></name>, <name><surname>Hart</surname><given-names>CE</given-names></name>, <name><surname>Guzman</surname><given-names>H</given-names></name>, <name><surname>Tesh</surname><given-names>RB</given-names></name>. <article-title>Vertical Transmission of Zika Virus in Aedes aegypti Mosquitoes</article-title>. <source>Am J Trop Med Hyg</source>. The American Society of Tropical Medicine and Hygiene; <year>2016</year>;<volume>95</volume>: <fpage>1169</fpage>&#x02013;<lpage>1173</lpage>. <pub-id pub-id-type="doi">10.4269/ajtmh.16-0448</pub-id>
<?supplied-pmid 27573623?><pub-id pub-id-type="pmid">27573623</pub-id></mixed-citation></ref><ref id="pone.0210829.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Cevallos</surname><given-names>V</given-names></name>, <name><surname>Ponce</surname><given-names>P</given-names></name>, <name><surname>Waggoner</surname><given-names>JJ</given-names></name>, <name><surname>Pinsky</surname><given-names>BA</given-names></name>, <name><surname>Coloma</surname><given-names>J</given-names></name>, <name><surname>Quiroga</surname><given-names>C</given-names></name>, <etal>et al</etal>
<article-title>Zika and Chikungunya virus detection in naturally infected Aedes aegypti in Ecuador</article-title>. <source>Acta Trop</source>. <year>2018</year>;<volume>177</volume>: <fpage>74</fpage>&#x02013;<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1016/j.actatropica.2017.09.029</pub-id>
<?supplied-pmid 28982578?><pub-id pub-id-type="pmid">28982578</pub-id></mixed-citation></ref><ref id="pone.0210829.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Guedes</surname><given-names>DR</given-names></name>, <name><surname>Paiva</surname><given-names>MH</given-names></name>, <name><surname>Donato</surname><given-names>MM</given-names></name>, <name><surname>Barbosa</surname><given-names>PP</given-names></name>, <name><surname>Krokovsky</surname><given-names>L</given-names></name>, <name><surname>Rocha</surname><given-names>SW dos S</given-names></name>, <etal>et al</etal>
<article-title>Zika virus replication in the mosquito Culex quinquefasciatus in Brazil</article-title>. <source>Emerg Microbes Infect</source>. <year>2017</year>;<volume>6</volume>
<pub-id pub-id-type="doi">10.1038/emi.2017.59</pub-id>
<?supplied-pmid 28790458?><pub-id pub-id-type="pmid">28790458</pub-id></mixed-citation></ref><ref id="pone.0210829.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Main</surname><given-names>BJ</given-names></name>, <name><surname>Nicholson</surname><given-names>J</given-names></name>, <name><surname>Winokur</surname><given-names>OC</given-names></name>, <name><surname>Steiner</surname><given-names>C</given-names></name>, <name><surname>Riemersma</surname><given-names>KK</given-names></name>, <name><surname>Stuart</surname><given-names>J</given-names></name>, <etal>et al</etal>
<article-title>Vector competence of Aedes aegypti, Culex tarsalis, and Culex quinquefasciatus from California for Zika virus</article-title>. <source>PLoS Negl Trop Dis</source>. Public Library of Science; <year>2018</year>;<volume>12</volume>: <fpage>e0006524</fpage> Available: <pub-id pub-id-type="doi">10.1371/journal.pntd.0006524</pub-id>
<?supplied-pmid 29927940?><pub-id pub-id-type="pmid">29927940</pub-id></mixed-citation></ref><ref id="pone.0210829.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Guo</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>C</given-names></name>, <name><surname>Deng</surname><given-names>Y</given-names></name>, <name><surname>Xing</surname><given-names>D</given-names></name>, <name><surname>Liu</surname><given-names>Q</given-names></name>, <name><surname>Wu</surname><given-names>Q</given-names></name>, <etal>et al</etal>
<article-title>Culex pipiens quinquefasciatus: a potential vector to transmit Zika virus</article-title>. <source>Emerg Microbes Infect</source>. Nature Publishing Group; <year>2016</year>;<volume>5</volume>: <fpage>e102</fpage>
<pub-id pub-id-type="doi">10.1038/emi.2016.102</pub-id>
<?supplied-pmid 27599470?><pub-id pub-id-type="pmid">27599470</pub-id></mixed-citation></ref><ref id="pone.0210829.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Boyer</surname><given-names>S</given-names></name>, <name><surname>Calvez</surname><given-names>E</given-names></name>, <name><surname>Chouin-Carneiro</surname><given-names>T</given-names></name>, <name><surname>Diallo</surname><given-names>D</given-names></name>, <name><surname>Failloux</surname><given-names>A-B</given-names></name>. <article-title>An overview of mosquito vectors of Zika virus</article-title>. <source>Microbes Infect</source>. <year>2018</year>; <pub-id pub-id-type="doi">10.1016/j.micinf.2018.01.006</pub-id></mixed-citation></ref><ref id="pone.0210829.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>SI</given-names></name>, <name><surname>Bisgin</surname><given-names>H</given-names></name>, <name><surname>Ding</surname><given-names>H</given-names></name>, <name><surname>Semey</surname><given-names>HG</given-names></name>, <name><surname>Langley</surname><given-names>DA</given-names></name>, <name><surname>Tong</surname><given-names>W</given-names></name>, <etal>et al</etal>
<article-title>Species identification of food contaminating beetles by recognizing patterns in microscopic images of elytra fragments</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>: <fpage>1</fpage>&#x02013;<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0157940</pub-id>
<?supplied-pmid 27341524?><pub-id pub-id-type="pmid">27341524</pub-id></mixed-citation></ref><ref id="pone.0210829.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>HP</given-names></name>, <name><surname>Ma</surname><given-names>C Sen</given-names></name>, <name><surname>Wen</surname><given-names>H</given-names></name>, <name><surname>Zhan</surname><given-names>Q Bin</given-names></name>, <name><surname>Wang</surname><given-names>XL</given-names></name>. <article-title>A tool for developing an automatic insect identification system based on wing outlines</article-title>. <source>Sci Rep</source>. Nature Publishing Group; <year>2015</year>;<volume>5</volume>: <fpage>1</fpage>&#x02013;<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1038/srep12786</pub-id>
<?supplied-pmid 26251292?><pub-id pub-id-type="pmid">26251292</pub-id></mixed-citation></ref><ref id="pone.0210829.ref016"><label>16</label><mixed-citation publication-type="other">Sifferlin A. Fewer Scientists Are Studying Insects. Here&#x02019;s Why That&#x02019;s So Dangerous. Time. 2018.</mixed-citation></ref><ref id="pone.0210829.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Utsugi</surname><given-names>J</given-names></name>, <name><surname>Toshihide</surname><given-names>K</given-names></name>, <name><surname>Motomi</surname><given-names>ITO</given-names></name>. <article-title>Current progress in DNA barcoding and future implications for entomology</article-title>. <source>Entomol Sci</source>. Wiley/Blackwell (10.1111); <year>2011</year>;<volume>14</volume>: <fpage>107</fpage>&#x02013;<lpage>124</lpage>. <pub-id pub-id-type="doi">10.1111/j.1479-8298.2011.00449.x</pub-id></mixed-citation></ref><ref id="pone.0210829.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Karthika</surname><given-names>P</given-names></name>, <name><surname>Vadivalagan</surname><given-names>C</given-names></name>, <name><surname>Thirumurugan</surname><given-names>D</given-names></name>, <name><surname>Kumar</surname><given-names>RR</given-names></name>, <name><surname>Murugan</surname><given-names>K</given-names></name>, <name><surname>Canale</surname><given-names>A</given-names></name>, <etal>et al</etal>
<article-title>DNA barcoding of five Japanese encephalitis mosquito vectors (Culex fuscocephala, Culex gelidus, Culex tritaeniorhynchus, Culex pseudovishnui and Culex vishnui)</article-title>. <source>Acta Trop</source>. <year>2018</year>;<volume>183</volume>: <fpage>84</fpage>&#x02013;<lpage>91</lpage>. <pub-id pub-id-type="doi">10.1016/j.actatropica.2018.04.006</pub-id>
<?supplied-pmid 29625090?><pub-id pub-id-type="pmid">29625090</pub-id></mixed-citation></ref><ref id="pone.0210829.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Mewara</surname><given-names>A</given-names></name>, <name><surname>Sharma</surname><given-names>M</given-names></name>, <name><surname>Kaura</surname><given-names>T</given-names></name>, <name><surname>Zaman</surname><given-names>K</given-names></name>, <name><surname>Yadav</surname><given-names>R</given-names></name>, <name><surname>Sehgal</surname><given-names>R</given-names></name>. <article-title>Rapid identification of medically important mosquitoes by matrix-assisted laser desorption/ionization time-of-flight mass spectrometry</article-title>. <source>Parasit Vectors</source>. BioMed Central; <year>2018</year>;<volume>11</volume>: <fpage>281</fpage>
<pub-id pub-id-type="doi">10.1186/s13071-018-2854-0</pub-id>
<?supplied-pmid 29720246?><pub-id pub-id-type="pmid">29720246</pub-id></mixed-citation></ref><ref id="pone.0210829.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Zhong</surname><given-names>Y</given-names></name>, <name><surname>Gao</surname><given-names>J</given-names></name>, <name><surname>Lei</surname><given-names>Q</given-names></name>, <name><surname>Zhou</surname><given-names>Y</given-names></name>. <article-title>A vision-based counting and recognition system for flying insects in intelligent agriculture</article-title>. <source>Sensors (Switzerland)</source>. <year>2018</year>;<volume>18</volume>
<pub-id pub-id-type="doi">10.3390/s18051489</pub-id>
<?supplied-pmid 29747429?><pub-id pub-id-type="pmid">29747429</pub-id></mixed-citation></ref><ref id="pone.0210829.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Vossenberg</surname><given-names>BTLH Van De</given-names></name>, <name><surname>Ib&#x000e1;&#x000f1;ez-Justicia</surname><given-names>A</given-names></name>, <name><surname>Metz-Verschure</surname><given-names>E</given-names></name>, <name><surname>Veen</surname><given-names>EJ Van</given-names></name>, <name><surname>Bruil-Dieters</surname><given-names>ML</given-names></name>, <name><surname>Scholte</surname><given-names>EJ</given-names></name>. <article-title>Real-Time PCR Tests in Dutch Exotic Mosquito Surveys; Implementation of Aedes aegypti and Aedes albopictus Identification Tests, and the Development of Tests for the Identification of Aedes atropalpus and Aedes japonicus japonicus (Diptera: Culicidae)</article-title>. <source>J Med Entomol</source>. Entomological Society of America; <year>2015</year>;<volume>52</volume>: <fpage>336</fpage>&#x02013;<lpage>350</lpage>. <pub-id pub-id-type="doi">10.1093/jme/tjv020</pub-id>
<?supplied-pmid 26334807?><pub-id pub-id-type="pmid">26334807</pub-id></mixed-citation></ref><ref id="pone.0210829.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Schneider</surname><given-names>J</given-names></name>, <name><surname>Valentini</surname><given-names>A</given-names></name>, <name><surname>Dejean</surname><given-names>T</given-names></name>, <name><surname>Montarsi</surname><given-names>F</given-names></name>, <name><surname>Taberlet</surname><given-names>P</given-names></name>, <name><surname>Glaizot</surname><given-names>O</given-names></name>, <etal>et al</etal>
<article-title>Detection of Invasive Mosquito Vectors Using Environmental DNA (eDNA) from Water Samples</article-title>. <source>PLoS One</source>. Public Library of Science; <year>2016</year>;<volume>11</volume>: <fpage>e0162493</fpage> Available: <pub-id pub-id-type="doi">10.1371/journal.pone.0162493</pub-id>
<?supplied-pmid 27626642?><pub-id pub-id-type="pmid">27626642</pub-id></mixed-citation></ref><ref id="pone.0210829.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Kumar</surname><given-names>NP</given-names></name>, <name><surname>Rajavel</surname><given-names>AR</given-names></name>, <name><surname>Natarajan</surname><given-names>R</given-names></name>, <name><surname>Jambulingam</surname><given-names>P</given-names></name>. <article-title>DNA Barcodes Can Distinguish Species of Indian Mosquitoes (Diptera: Culicidae)</article-title>. <source>J Med Entomol</source>. Entomological Society of America; <year>2007</year>;<volume>44</volume>: <fpage>1</fpage>&#x02013;<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1603/0022-2585(2007)44[1:DBCDSO]2.0.CO;2</pub-id>
<?supplied-pmid 17294914?><pub-id pub-id-type="pmid">17294914</pub-id></mixed-citation></ref><ref id="pone.0210829.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Ouyang</surname><given-names>TH</given-names></name>, <name><surname>Yang</surname><given-names>EC</given-names></name>, <name><surname>Jiang</surname><given-names>JA</given-names></name>, <name><surname>Lin</surname><given-names>T Te</given-names></name>. <article-title>Mosquito vector monitoring system based on optical wingbeat classification</article-title>. <source>Comput Electron Agric</source>. Elsevier B.V.; <year>2015</year>;<volume>118</volume>: <fpage>47</fpage>&#x02013;<lpage>55</lpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2015.08.021</pub-id></mixed-citation></ref><ref id="pone.0210829.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Arthur</surname><given-names>BJ</given-names></name>, <name><surname>Emr</surname><given-names>KS</given-names></name>, <name><surname>Wyttenbach</surname><given-names>RA</given-names></name>, <name><surname>Hoy</surname><given-names>RR</given-names></name>. <article-title>Mosquito (Aedes aegypti) flight tones: Frequency, harmonicity, spherical spreading, and phase relationships</article-title>. <source>J Acoust Soc Am</source>. <year>2014</year>;<volume>135</volume>: <fpage>933</fpage>&#x02013;<lpage>941</lpage>. <pub-id pub-id-type="doi">10.1121/1.4861233</pub-id>
<?supplied-pmid 25234901?><pub-id pub-id-type="pmid">25234901</pub-id></mixed-citation></ref><ref id="pone.0210829.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Lorenz</surname><given-names>C</given-names></name>, <name><surname>Sergio</surname><given-names>A</given-names></name>, <name><surname>Suesdek</surname><given-names>L</given-names></name>. <article-title>Artificial Neural Network applied as a methodology of mosquito species identification</article-title>. <source>Acta Trop</source>. <year>2015</year>;<volume>152</volume>: <fpage>165</fpage>&#x02013;<lpage>169</lpage>. <pub-id pub-id-type="doi">10.1016/j.actatropica.2015.09.011</pub-id>
<?supplied-pmid 26394186?><pub-id pub-id-type="pmid">26394186</pub-id></mixed-citation></ref><ref id="pone.0210829.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Kesavaraju</surname><given-names>B</given-names></name>, <name><surname>Dickson</surname><given-names>S</given-names></name>. <article-title>New Technique to Count Mosquito Adults: Using ImageJ Software to Estimate Number of Mosquito Adults in a Trap</article-title>. <source>J Am Mosq Control Assoc</source>. <year>2012</year>;<volume>28</volume>: <fpage>330</fpage>&#x02013;<lpage>333</lpage>. <pub-id pub-id-type="doi">10.2987/12-6254R.1</pub-id>
<?supplied-pmid 23393760?><pub-id pub-id-type="pmid">23393760</pub-id></mixed-citation></ref><ref id="pone.0210829.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Yao</surname><given-names>Q</given-names></name>, <name><surname>Lv</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>Q jie</given-names></name>, <name><surname>Diao</surname><given-names>G qiang</given-names></name>, <name><surname>Yang</surname><given-names>B jun</given-names></name>, <name><surname>Chen</surname><given-names>H ming</given-names></name>, <etal>et al</etal>
<article-title>An Insect Imaging System to Automate Rice Light-Trap Pest Identification</article-title>. <source>J Integr Agric</source>. Chinese Academy of Agricultural Sciences; <year>2012</year>;<volume>11</volume>: <fpage>978</fpage>&#x02013;<lpage>985</lpage>. <pub-id pub-id-type="doi">10.1016/S2095-3119(12)60089-6</pub-id></mixed-citation></ref><ref id="pone.0210829.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Mundada</surname><given-names>RG</given-names></name>, <name><surname>Gohokar</surname><given-names>V V</given-names></name>. <source>Detection and Classification of Pests in Greenhouse Using Image Processing</source>. <year>2013</year>;<volume>5</volume>: <fpage>57</fpage>&#x02013;<lpage>63</lpage>.</mixed-citation></ref><ref id="pone.0210829.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Schmidhuber</surname><given-names>J</given-names></name>. <article-title>Deep Learning in neural networks: An overview</article-title>. <source>Neural Networks</source>. Elsevier Ltd; <year>2015</year>;<volume>61</volume>: <fpage>85</fpage>&#x02013;<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2014.09.003</pub-id>
<?supplied-pmid 25462637?><pub-id pub-id-type="pmid">25462637</pub-id></mixed-citation></ref><ref id="pone.0210829.ref031"><label>31</label><mixed-citation publication-type="other">Sanchez-Ortiz A, Fierro-Radilla A, Arista-Jalife A, Cedillo-Hernandez M, Nakano-Miyatake M, Robles-Camarillo D, et al. Mosquito larva classification method based on convolutional neural networks. 2017 International Conference on Electronics, Communications and Computers (CONIELECOMP). 2017. pp. 1&#x02013;6. <pub-id pub-id-type="doi">10.1109/CONIELECOMP.2017.7891835</pub-id></mixed-citation></ref><ref id="pone.0210829.ref032"><label>32</label><mixed-citation publication-type="other">Silva DF, De Souza VMA, Batista GEAPA, Keogh E, Ellis DPW. Applying machine learning and audio analysis techniques to insect recognition in intelligent traps. Proceedings&#x02014;2013 12th International Conference on Machine Learning and Applications, ICMLA 2013. 2013. pp. 99&#x02013;104. <pub-id pub-id-type="doi">10.1109/ICMLA.2013.24</pub-id></mixed-citation></ref><ref id="pone.0210829.ref033"><label>33</label><mixed-citation publication-type="other">Batista GEAPA, Hao Y, Keogh E, Mafra-Neto A. Towards automatic classification on flying insects using inexpensive sensors. Proc - 10th Int Conf Mach Learn Appl ICMLA 2011. 2011;1: 364&#x02013;369. <pub-id pub-id-type="doi">10.1109/ICMLA.2011.145</pub-id></mixed-citation></ref><ref id="pone.0210829.ref034"><label>34</label><mixed-citation publication-type="other">De Souza VMA, Silva DF, Batista GEAPA. Classification of data streams applied to insect recognition: Initial results. Proc&#x02014;2013 Brazilian Conf Intell Syst BRACIS 2013. 2013; 76&#x02013;81. <pub-id pub-id-type="doi">10.1109/BRACIS.2013.21</pub-id></mixed-citation></ref><ref id="pone.0210829.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Gaburro</surname><given-names>J</given-names></name>, <name><surname>Duchemin</surname><given-names>J-B</given-names></name>, <name><surname>Paradkar</surname><given-names>PN</given-names></name>, <name><surname>Nahavandi</surname><given-names>S</given-names></name>, <name><surname>Bhatti</surname><given-names>A</given-names></name>. <article-title>Assessment of ICount software, a precise and fast egg counting tool for the mosquito vector Aedes aegypti</article-title>. <source>Parasit Vectors</source>. Parasites &#x00026; Vectors; <year>2016</year>;<volume>9</volume>: <fpage>590</fpage>
<pub-id pub-id-type="doi">10.1186/s13071-016-1870-1</pub-id>
<?supplied-pmid 27863526?><pub-id pub-id-type="pmid">27863526</pub-id></mixed-citation></ref><ref id="pone.0210829.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Henry</surname><given-names>A</given-names></name>, <name><surname>Thongsripong</surname><given-names>P</given-names></name>, <name><surname>Fonseca-Gonzalez</surname><given-names>I</given-names></name>, <name><surname>Jaramillo-Ocampo</surname><given-names>N</given-names></name>, <name><surname>Dujardin</surname><given-names>JP</given-names></name>. <article-title>Wing shape of dengue vectors from around the world</article-title>. <source>Infect Genet Evol</source>. <year>2010</year>;<volume>10</volume>: <fpage>207</fpage>&#x02013;<lpage>214</lpage>. <pub-id pub-id-type="doi">10.1016/j.meegid.2009.12.001</pub-id>
<?supplied-pmid 20026429?><pub-id pub-id-type="pmid">20026429</pub-id></mixed-citation></ref><ref id="pone.0210829.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Bova</surname><given-names>JE</given-names></name>, <name><surname>Paulson</surname><given-names>SL</given-names></name>, <name><surname>Paulson</surname><given-names>G</given-names></name>. <article-title>Morphological differentiation of the eggs of North American container-inhabiting Aedes mosquitoes</article-title>. <source>J Am Mosq Control Assoc</source>. <year>2016</year>;<volume>32</volume>: <fpage>244</fpage>&#x02013;<lpage>246</lpage>. <pub-id pub-id-type="doi">10.2987/15-6535.1</pub-id>
<?supplied-pmid 27802396?><pub-id pub-id-type="pmid">27802396</pub-id></mixed-citation></ref><ref id="pone.0210829.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Schaper</surname><given-names>S</given-names></name>, <name><surname>Hern&#x000e1;ndez-Chavarr&#x000ed;a</surname><given-names>F</given-names></name>. <article-title>Scanning electron microscopy of the four larval instars of the Dengue fever vector Aedes aegypti (Diptera: Culicidae)</article-title>. <source>Rev Biol Trop (Int J Trop Biol</source>. <year>2006</year>;<volume>54</volume>: <fpage>847</fpage>&#x02013;<lpage>852</lpage>.</mixed-citation></ref><ref id="pone.0210829.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>. <article-title>ImageNet Classification with Deep Convolutional Neural Networks</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2012</year>; <fpage>1</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.protcy.2014.09.007</pub-id></mixed-citation></ref><ref id="pone.0210829.ref040"><label>40</label><mixed-citation publication-type="book"><name><surname>Goodfellow</surname><given-names>I</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Courville</surname><given-names>A</given-names></name>. <source>Deep Learning</source> [Internet]. <publisher-name>MIT Press</publisher-name>; <year>2016</year> Available: <ext-link ext-link-type="uri" xlink:href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</ext-link></mixed-citation></ref><ref id="pone.0210829.ref041"><label>41</label><mixed-citation publication-type="book"><collab>CDC C for DC and P</collab>. <source>Surveillance and Control of Aedes aegypti and Aedes albopictus in the United States</source>. <publisher-name>Centers Dis Control Prev&#x02014;CDC</publisher-name>
<year>2016</year>; <fpage>1</fpage>&#x02013;<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pntd.0004043</pub-id></mixed-citation></ref><ref id="pone.0210829.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>LeCun</surname><given-names>Y</given-names></name>, <name><surname>Bottou</surname><given-names>L</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Haffner</surname><given-names>P</given-names></name>. <article-title>Gradient-Based Learning Applied to Document Recognition</article-title>. <source>Proc IEEE</source>. <year>1998</year>; <volume>46</volume>
<pub-id pub-id-type="doi">10.1109/5.726791</pub-id></mixed-citation></ref><ref id="pone.0210829.ref043"><label>43</label><mixed-citation publication-type="other">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, et al. Going Deeper with Convolutions. arXiv:14094842. 2014; <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></mixed-citation></ref><ref id="pone.0210829.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>Rezza</surname><given-names>G</given-names></name>, <name><surname>Nicoletti</surname><given-names>L</given-names></name>, <name><surname>Angelini</surname><given-names>R</given-names></name>, <name><surname>Romi</surname><given-names>R</given-names></name>, <name><surname>Finarelli</surname><given-names>AC</given-names></name>, <name><surname>Panning</surname><given-names>M</given-names></name>, <etal>et al</etal>
<article-title>Infection with chikungunya virus in Italy: an outbreak in a temperate region</article-title>. <source>Lancet</source>. <year>2007</year>;<volume>370</volume>: <fpage>1840</fpage>&#x02013;<lpage>1846</lpage>. <pub-id pub-id-type="doi">10.1016/S0140-6736(07)61779-6</pub-id>
<?supplied-pmid 18061059?><pub-id pub-id-type="pmid">18061059</pub-id></mixed-citation></ref><ref id="pone.0210829.ref045"><label>45</label><mixed-citation publication-type="journal"><name><surname>Kraemer</surname><given-names>MUG</given-names></name>, <name><surname>Sinka</surname><given-names>ME</given-names></name>, <name><surname>Duda</surname><given-names>KA</given-names></name>, <name><surname>Mylne</surname><given-names>AQN</given-names></name>, <name><surname>Shearer</surname><given-names>FM</given-names></name>, <name><surname>Barker</surname><given-names>CM</given-names></name>, <etal>et al</etal>
<article-title>The global distribution of the arbovirus vectors Aedes aegypti and Ae. albopictus</article-title>. <name><surname>Jit</surname><given-names>M</given-names></name>, editor. <source>Elife</source>. eLife Sciences Publications, Ltd; <year>2015</year>;<volume>4</volume>: <fpage>e08347</fpage>
<pub-id pub-id-type="doi">10.7554/eLife.08347</pub-id>
<?supplied-pmid 26126267?><pub-id pub-id-type="pmid">26126267</pub-id></mixed-citation></ref><ref id="pone.0210829.ref046"><label>46</label><mixed-citation publication-type="journal"><name><surname>Epelboin</surname><given-names>Y</given-names></name>, <name><surname>Talaga</surname><given-names>S</given-names></name>, <name><surname>Epelboin</surname><given-names>L</given-names></name>, <name><surname>Dusfour</surname><given-names>I</given-names></name>. <article-title>Zika virus: An updated review of competent or naturally infected mosquitoes</article-title>. <source>PLoS Negl Trop Dis</source>. Public Library of Science; <year>2017</year>;<volume>11</volume>: <fpage>e0005933</fpage> Available: <pub-id pub-id-type="doi">10.1371/journal.pntd.0005933</pub-id>
<?supplied-pmid 29145400?><pub-id pub-id-type="pmid">29145400</pub-id></mixed-citation></ref><ref id="pone.0210829.ref047"><label>47</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Lin</surname><given-names>C</given-names></name>, <name><surname>Ji</surname><given-names>L</given-names></name>, <name><surname>Liang</surname><given-names>A</given-names></name>. <article-title>A new automatic identification system of insect images at the order level</article-title>. <source>Knowledge-Based Syst</source>. Elsevier B.V.; <year>2012</year>;<volume>33</volume>: <fpage>102</fpage>&#x02013;<lpage>110</lpage>. <pub-id pub-id-type="doi">10.1016/j.knosys.2012.03.014</pub-id></mixed-citation></ref><ref id="pone.0210829.ref048"><label>48</label><mixed-citation publication-type="journal"><name><surname>Santos</surname><given-names>J</given-names></name>, <name><surname>Meneses</surname><given-names>BM</given-names></name>. <article-title>An integrated approach for the assessment of the Aedes aegypti and Aedes albopictus global spatial distribution, and determination of the zones susceptible to the development of Zika virus</article-title>. <source>Acta Trop</source>. <year>2017</year>;<volume>168</volume>: <fpage>80</fpage>&#x02013;<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1016/j.actatropica.2017.01.015</pub-id>
<?supplied-pmid 28111132?><pub-id pub-id-type="pmid">28111132</pub-id></mixed-citation></ref></ref-list></back></article>