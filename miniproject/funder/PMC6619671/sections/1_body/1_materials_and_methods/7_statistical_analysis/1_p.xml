<?xml version="1.0" encoding="UTF-8"?>
<p>To compare the performance of two methods, inter-rater agreement was assessed with a 2 x 2 contingency table and determined by Cohen’s kappa statistic using commercial software SPSS 14.0 (SPSS Inc., Chicago, IL). The values ≤ 0.20, 0.21–0.39, 0.40–0.59, 0.60–0.79, 0.80–0.90, and &gt; 0.90 were interpreted as no, minimal, weak, moderate, strong, and almost perfect agreement, respectively [
 <xref rid="pone.0218139.ref037" ref-type="bibr">37</xref>].
</p>
