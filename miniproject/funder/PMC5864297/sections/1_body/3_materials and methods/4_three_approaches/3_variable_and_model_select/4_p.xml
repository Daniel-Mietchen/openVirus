<?xml version="1.0" encoding="UTF-8"?>
<p>We added and removed random effects to the model structure, taking out and adding fixed effects, executing the regression algorithm, and plotting the DIC achieved against model complexity. Here, model complexity is simply proxied by the descriptional complexity in terms of how large the model was (the number of factors in the regression equation). We stopped when the drop in DIC stabilized (See 
 <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref> ). Notably, the DIC has been shown to select overfits (
 <xref rid="bib2" ref-type="bibr">Ando, 2011</xref>; 
 <xref rid="bib71" ref-type="bibr">Plummer, 2008</xref>; 
 <xref rid="bib84" ref-type="bibr">van der Linde, 2012</xref>), and does not properly account for model complexity in practice. Our driving idea was the identification of the Pareto front that trades off accuracy (in terms of DIC) with model complexity in a transparent manner. This is, of course, a ‘greedy approach,’ in the sense that we do not guarantee that we have indeed found the ‘best’ possible model. However, because our cross-validation (
 <xref ref-type="fig" rid="fig5">Figure 5</xref>) yielded good results, we deemed the stopping rule to be satisfactory.
</p>
