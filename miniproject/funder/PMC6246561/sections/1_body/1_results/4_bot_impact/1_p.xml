<?xml version="1.0" encoding="UTF-8"?>
<p id="Par14">Having found that automated accounts are employed in ways that appear to drive the viral spread of low-credibility articles, let us explore how humans interact with the content shared by bots, which may provide insight into whether and how bots are able to affect public opinion. Figure 
 <xref rid="Fig4" ref-type="fig">4a</xref> shows who retweets whom: humans do most of the retweeting (Fig. 
 <xref rid="Fig4" ref-type="fig">4b</xref>), and they retweet articles posted by likely bots almost as much as those by other humans (Fig. 
 <xref rid="Fig4" ref-type="fig">4c</xref>). This result, which is robust to the choice of threshold used to identify likely humans, suggests that collectively, people do not discriminate between low-credibility content shared by humans versus social bots. It also means that when we observe many accounts exposed to low-credibility information, these are not just bots (re)tweeting it. In fact, we find that the volume of tweets by likely humans scales super-linearly with the volume by likely bots, suggesting that the reach of these articles among humans is amplified by social bots. In other words, each amount of sharing activity by likely bots tends to trigger a disproportionate amount of human engagement. The same amplification effect is not observed for articles from fact-checking sources. Details are presented in Supplementary Discussion (Supplementary Figs. 
 <xref rid="MOESM1" ref-type="media">8</xref>, 
 <xref rid="MOESM1" ref-type="media">9</xref>).
</p>
