<?xml version="1.0" encoding="UTF-8"?>
<p>We reaffirmed findings from previous simulations that showed vaccination levels substantially lower than 70% are sufficient to confer herd immunity to rabies in spatially structured populations [
 <xref rid="pone.0232705.ref062" ref-type="bibr">62</xref>â€“
 <xref rid="pone.0232705.ref065" ref-type="bibr">65</xref>]. The minimum vaccination level needed to protect a target proportion of hosts depends on both the distribution of vaccinated animals and overall host density. Random vaccination rates continued to improve rabies epidemic outcomes as the level of vaccination increased, even up to 50% vaccination. In contrast, when a vaccine firewall was placed very close to the site of pathogen introduction, vaccination rates as low as 10% were sufficient to minimize the number of infected foxes and cause epidemic fadeout in &gt;60% of model iterations. However, rabies had a greater impact on the fox population when it was introduced to high-density regions or when vaccinations were distributed in a firewall far away from the site of pathogen introduction. Consequently, even though vaccination protects some fraction of the unvaccinated population through herd immunity during first year of an epidemic, vaccination rates &lt;50% should not be relied upon to prevent rabies from persisting in the population over longer time frames.
</p>
