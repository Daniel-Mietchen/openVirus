<?xml version="1.0" encoding="UTF-8"?>
<p id="Par7">To obtain an optimum NAR model, the hidden units and feedback delays, ranging from 10 to 35 and from 2 to 8, respectively, were iteratively examined within in-sample data points. Ultimately, we identified the best-fitting model with 18 hidden neurons and 5 feedback delays dependent on the comprehensive optimum performance indices aside from the fact that a fat-tailed distribution, compared to the normal distribution, should be utilized (Supplementary Fig. 
 <xref rid="MOESM1" ref-type="media">S13</xref>). As presented in Supplementary Fig. 
 <xref rid="MOESM1" ref-type="media">S14</xref>, in the preferred model, the minimum MSE values for the training, validation and testing datasets and for the entire dataset were 0.0011, 0.0074, 0.0144 and 0.0038, respectively; the maximum R values of the training, validation, testing subsets and entire dataset were 0.987, 0.931, 0.920 and 0.963, respectively. Moreover, the input-to-error correlations and autocorrelations of the produced residuals were never beyond the estimated 95% uncertainty limits around zero across varying lag times, apart from the one in the ACF plot at lag zero that should occur (Fig. 
 <xref rid="Fig3" ref-type="fig">3</xref> and Table 
 <xref rid="Tab1" ref-type="table">1</xref>). The LM test suggested that the ARCH effect that existed in the original data largely minimized the errors of the NAR model (Table 
 <xref rid="Tab2" ref-type="table">2</xref>). In addition, the response plot of the estimated values from the randomly selected training, validation and testing datasets against their corresponding original observations at different time points showed that the optimal approach could simulate the data points included in the three grouped subsets well because of the small residuals that were mostly located between −0.2 and 0.2 (Fig. 
 <xref rid="Fig4" ref-type="fig">4</xref>). Similarly, according to the modeling steps described above, in these two robustness-test datasets, the best-presenting technique fit to the dataset between June 2008 and December 2016 was such an NAR model with 17 hidden neurons and 5 feedback delays. The identified layer architecture and statistical measures for this preferred network are displayed in Supplementary Figs 
 <xref rid="MOESM1" ref-type="media">S15</xref>–
 <xref rid="MOESM1" ref-type="media">S19</xref> and Tables 
 <xref rid="MOESM1" ref-type="media">S4</xref>, 
 <xref rid="MOESM1" ref-type="media">S5</xref> and 
 <xref rid="MOESM1" ref-type="media">S10</xref>. The best-fitting model developed utilizing the data from June 2008 to December 2017 was an NAR model with 19 hidden neurons and 6 feedback delays, and with regard to the optimal network, all further diagnostic results can be seen in Supplementary Figs 
 <xref rid="MOESM1" ref-type="media">S20</xref>–
 <xref rid="MOESM1" ref-type="media">S24</xref> and Tables 
 <xref rid="MOESM1" ref-type="media">S8</xref>–
 <xref rid="MOESM1" ref-type="media">S10</xref>. Afterwards, these best-performing networks were employed to conduct out-of-sample forecasting, and the simulated and forecasted values obtained were back-transformed to the original scale because they were computed on the transformed scale.
</p>
