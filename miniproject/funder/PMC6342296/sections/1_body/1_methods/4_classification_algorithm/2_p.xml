<?xml version="1.0" encoding="UTF-8"?>
<p>To build the classification tree, each variable was tested for the cutoff value that best split the cases into their true categories. The variable that performed best became the top “rule” in the tree, splitting cases into 2 groups. The same procedure was repeated for each group separately, with the variable that best separated the groups into true classes chosen as the subsequent rule. This process of splitting continued until each case was correctly classified, producing a tree that was over-fit to the training dataset. To identify a tree for categorization of the datasets, we pruned the tree to the smallest tree for which misclassification was minimized in cross-validation. This subtree had the lowest complexity parameter, meaning the lowest misclassification of cases in 
 <italic>n</italic>-fold cross-validation experiments, where 1 case is excluded when creating the tree and the resulting tree is used to estimate the class of the excluded case. We then selected the simplest tree (fewest “rules”) that had a complexity value within 1 standard error of the lowest complexity parameter tree. Optimal splits in the tree were identified using the Gini index, an impurity function. We included cases with missing data when creating the algorithm and allowed surrogate variables, i.e., variables that approximate the categorization achieved by the best splitting variable, to inform splits to enable classification of cases with missing data. The final algorithm was applied to classify all 1,111 cases that occurred during the study period. For cases that were rRT-PCR-positive for ZIKV or DENV, rRT-PCR was used to define the case rather than the algorithm.
</p>
