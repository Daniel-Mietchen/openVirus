<?xml version="1.0" encoding="UTF-8"?>
<p>The Regularization Bayesian method [
 <xref rid="pntd.0006592.ref030" ref-type="bibr">30</xref>] was used with the intention of reducing the arbitrariness of the specification of the neural architecture. In this method, an objective function is optimized forcing the pruning of less relevant weights. The function 
 <italic>trainbr</italic> of MATLAB is a network training function that updates the weight and bias values according to Levenberg-Marquardt optimization. It minimizes a combination of squared errors and weights, and then determines the correct combination so as to produce a network that generalizes well. Among all the simulations performed, the number of effective neurons used by this training algorithm was more than sufficient [
 <xref rid="pntd.0006592.ref030" ref-type="bibr">30</xref>, 
 <xref rid="pntd.0006592.ref031" ref-type="bibr">31</xref>]. A hidden layer with 10 neurons and an output layer with 1 neuron were used. In all the neurons the function of logistic activation was used. In the output layer the network produced zero or one output, representing the two classes of pupae in the containers, absence or presence of pupae. The data set was divided into training data set (70%) and generalization dataset (30%). The data used in the training were balanced to avoid any tendency favorable to the greater number of cases, that is, absences or presence of pupae. All results presented in this article are related to the generalization dataset. That is, data that were not presented in the training.
</p>
