<?xml version="1.0" encoding="UTF-8"?>
<p>The algorithm used in this study is the MIFS-U—Mutual Information Feature Selector under Uniform Distribution—presented in 2002 by Kwak &amp; Choi [
 <xref rid="pntd.0006592.ref025" ref-type="bibr">25</xref>]. It is aimed at overcoming the limitation of the variable selector proposed in 1994 by Battiti [
 <xref rid="pntd.0006592.ref027" ref-type="bibr">27</xref>], generating improved performance in the process of selection. Because of its simplicity, such an algorithm can be used in any classification system, no matter what the learning algorithm is. Input variables can be classified as relevant, irrelevant, or redundant, and you just want to select those that are relevant. The algorithm is initialized with a set F that contains all the variables to be selected and a set S, initially empty, that will be filled for each variable selected in order of importance with the outcome (presence or absence of pupae). The first variable to be selected will be the one that presents the most mutual information with the outcome. Selecting the next variable occurs by choosing a variable 
 <italic>ϕ</italic>
 <sub>
  <italic>i</italic>
 </sub> ∈ 
 <italic>F</italic> that maximizes 
 <inline-formula id="pntd.0006592.e003">
  <alternatives>
   <graphic xlink:href="pntd.0006592.e003.jpg" id="pntd.0006592.e003g" mimetype="image" position="anchor" orientation="portrait" xmlns:xlink="http://www.w3.org/1999/xlink"/>
   <math id="M3">
    <mrow>
     <mi>I</mi>
     <mo stretchy="false">(</mo>
     <mi>C</mi>
     <mo>,</mo>
     <msub>
      <mi>ϕ</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
     <mo>−</mo>
     <mi>β</mi>
     <mstyle displaystyle="true">
      <munder>
       <mo>∑</mo>
       <mrow>
        <msub>
         <mi>ϕ</mi>
         <mi>s</mi>
        </msub>
        <mo>∈</mo>
        <mi>S</mi>
       </mrow>
      </munder>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>I</mi>
       <mo stretchy="false">(</mo>
       <mi>C</mi>
       <mo>;</mo>
       <msub>
        <mi>ϕ</mi>
        <mi>s</mi>
       </msub>
       <mo stretchy="false">)</mo>
       <mo>/</mo>
       <mi>H</mi>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>ϕ</mi>
        <mi>s</mi>
       </msub>
       <mo stretchy="false">)</mo>
       <mo stretchy="false">)</mo>
       <mi>I</mi>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>ϕ</mi>
        <mi>i</mi>
       </msub>
       <mo>;</mo>
       <msub>
        <mi>ϕ</mi>
        <mi>s</mi>
       </msub>
       <mo stretchy="false">)</mo>
      </mrow>
     </mstyle>
    </mrow>
   </math>
  </alternatives>
 </inline-formula> and making F←F-{
 <italic>ϕ</italic>
 <sub>i</sub>}, S ← 
 <italic>ϕ</italic>
 <sub>i</sub>. This process repeats itself until F is an empty set. If β = 0, the algorithm selects variables in the order of mutual information between input and output variables. Redundancy among input variables is never reflected. When β&gt;0, the algorithm deletes redundant variables more effectively. In general, we can fix β = 1 [
 <xref rid="pntd.0006592.ref028" ref-type="bibr">28</xref>]. For all the experiments of this study was set β = 1.
</p>
