<?xml version="1.0" encoding="UTF-8"?>
<list list-type="simple">
 <list-item>
  <p>Definition 1 –Shannon’s entropy 
   <italic>H(X)</italic> of a discrete random variable 
   <italic>X</italic>, with the mass probability function 
   <italic>f</italic>
   <sub>
    <italic>x</italic>
   </sub>
   <italic>(x)</italic>, is defined by: 
   <disp-formula id="pntd.0006592.e001">
    <alternatives>
     <graphic xlink:href="pntd.0006592.e001.jpg" id="pntd.0006592.e001g" mimetype="image" position="anchor" orientation="portrait" xmlns:xlink="http://www.w3.org/1999/xlink"/>
     <math id="M1">
      <mrow>
       <mi>H</mi>
       <mo stretchy="false">(</mo>
       <mi>X</mi>
       <mo stretchy="false">)</mo>
       <mo>=</mo>
       <mo>−</mo>
       <mstyle displaystyle="true">
        <munder>
         <mo>∑</mo>
         <mrow>
          <mi>x</mi>
          <mo>∈</mo>
          <mi>X</mi>
         </mrow>
        </munder>
        <mrow>
         <msub>
          <mi>f</mi>
          <mi>x</mi>
         </msub>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
         <mtext>log</mtext>
         <msub>
          <mi>f</mi>
          <mi>x</mi>
         </msub>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mstyle>
      </mrow>
     </math>
    </alternatives>
    <label>(1)</label>
   </disp-formula> where log is to the base 2 and entropy is expressed in bits.
  </p>
  <p specific-use="continuation">Thus, by definition, 
   <italic>H(X)</italic> &gt; 0.
  </p>
 </list-item>
 <list-item>
  <p>Definition 2 –The mutual information (Shannon’s) 
   <italic>I(X;Y)</italic> between two discrete random variables 
   <italic>X</italic> and 
   <italic>Y</italic>, with joint mass probability function 
   <italic>f</italic>
   <sub>
    <italic>xy</italic>
   </sub>
   <italic>(x</italic>,
   <italic>y)</italic> and marginal mass probability functions 
   <italic>f</italic>
   <sub>
    <italic>x</italic>
   </sub>
   <italic>(x)</italic> and 
   <italic>f</italic>
   <sub>
    <italic>y</italic>
   </sub>
   <italic>(y)</italic> being given by the relative entropy between the joint distribution and the distribution of the marginal product is defined by: 
   <disp-formula id="pntd.0006592.e002">
    <alternatives>
     <graphic xlink:href="pntd.0006592.e002.jpg" id="pntd.0006592.e002g" mimetype="image" position="anchor" orientation="portrait" xmlns:xlink="http://www.w3.org/1999/xlink"/>
     <math id="M2">
      <mrow>
       <mi>I</mi>
       <mo stretchy="false">(</mo>
       <mi>X</mi>
       <mo>,</mo>
       <mi>Y</mi>
       <mo stretchy="false">)</mo>
       <mo>=</mo>
       <mstyle displaystyle="true">
        <munder>
         <mo>∑</mo>
         <mrow>
          <mi>x</mi>
          <mo>∈</mo>
          <mi>X</mi>
         </mrow>
        </munder>
        <mrow>
         <mstyle displaystyle="true">
          <munder>
           <mo>∑</mo>
           <mrow>
            <mi>y</mi>
            <mo>∈</mo>
            <mi>Y</mi>
           </mrow>
          </munder>
          <mrow>
           <msub>
            <mi>f</mi>
            <mrow>
             <mi>x</mi>
             <mi>y</mi>
            </mrow>
           </msub>
           <mo stretchy="false">(</mo>
           <mi>x</mi>
           <mo>,</mo>
           <mi>y</mi>
           <mo stretchy="false">)</mo>
           <mtext>log</mtext>
           <mfrac>
            <mrow>
             <msub>
              <mi>f</mi>
              <mrow>
               <mi>x</mi>
               <mi>y</mi>
              </mrow>
             </msub>
             <mo stretchy="false">(</mo>
             <mi>x</mi>
             <mo>,</mo>
             <mi>y</mi>
             <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
             <msub>
              <mi>f</mi>
              <mi>x</mi>
             </msub>
             <mo stretchy="false">(</mo>
             <mi>x</mi>
             <mo stretchy="false">)</mo>
             <msub>
              <mi>f</mi>
              <mi>y</mi>
             </msub>
             <mo stretchy="false">(</mo>
             <mi>y</mi>
             <mo stretchy="false">)</mo>
            </mrow>
           </mfrac>
          </mrow>
         </mstyle>
        </mrow>
       </mstyle>
      </mrow>
     </math>
    </alternatives>
    <label>(2)</label>
   </disp-formula> where log is to the base 2 and entropy is expressed in bits.
  </p>
  <p specific-use="continuation">with equality if and only if, 
   <italic>f</italic>
   <sub>
    <italic>xy</italic>
   </sub>
   <italic>(x</italic>,
   <italic>y)</italic> = 
   <italic>f</italic>
   <sub>
    <italic>x</italic>
   </sub>
   <italic>(x) f</italic>
   <sub>
    <italic>y</italic>
   </sub>
   <italic>(y)</italic> (that is, if 
   <italic>X</italic> and 
   <italic>Y</italic> are independent).
  </p>
 </list-item>
</list>
