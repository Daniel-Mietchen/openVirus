<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Commun</journal-id><journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id><journal-title-group><journal-title>Nature Communications</journal-title></journal-title-group><issn pub-type="epub">2041-1723</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6246561</article-id><article-id pub-id-type="publisher-id">6930</article-id><article-id pub-id-type="doi">10.1038/s41467-018-06930-7</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>The spread of low-credibility content by social bots</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5479-2256</contrib-id><name><surname>Shao</surname><given-names>Chengcheng</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5354-9257</contrib-id><name><surname>Ciampaglia</surname><given-names>Giovanni Luca</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3994-6106</contrib-id><name><surname>Varol</surname><given-names>Onur</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Kai-Cheng</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Flammini</surname><given-names>Alessandro</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4384-2876</contrib-id><name><surname>Menczer</surname><given-names>Filippo</given-names></name><address><email>fil@iu.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0790 959X</institution-id><institution-id institution-id-type="GRID">grid.411377.7</institution-id><institution>School of Informatics, Computing, and Engineering, </institution><institution>Indiana University Bloomington, </institution></institution-wrap>Bloomington, 47408 IN USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9548 2110</institution-id><institution-id institution-id-type="GRID">grid.412110.7</institution-id><institution>College of Computer, </institution><institution>National University of Defense Technology, </institution></institution-wrap>Changsha, 410073 Hunan China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0790 959X</institution-id><institution-id institution-id-type="GRID">grid.411377.7</institution-id><institution>Indiana University Network Science Institute, </institution></institution-wrap>Bloomington, 47408 IN USA </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>9</volume><elocation-id>4787</elocation-id><history><date date-type="received"><day>30</day><month>5</month><year>2018</year></date><date date-type="accepted"><day>5</day><month>10</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The massive spread of digital misinformation has been identified as a major threat to democracies. Communication, cognitive, social, and computer scientists are studying the complex causes for the viral diffusion of misinformation, while online platforms are beginning to deploy countermeasures. Little systematic, data-based evidence has been published to guide these efforts. Here we analyze 14 million messages spreading 400 thousand articles on Twitter during ten months in 2016 and 2017. We find evidence that social bots played a disproportionate role in spreading articles from low-credibility sources. Bots amplify such content in the early spreading moments, before an article goes viral. They also target users with many followers through replies and mentions. Humans are vulnerable to this manipulation, resharing content posted by bots. Successful low-credibility sources are heavily supported by social bots. These results suggest that curbing social bots may be an effective strategy for mitigating the spread of online misinformation.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">Online misinformation is a threat to a well-informed electorate and undermines democracy. Here, the authors analyse the spread of articles on Twitter, find that bots play a major role in the spread of low-credibility content and suggest control measures for limiting the spread of misinformation.</p></abstract><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par3">As we access news from social media<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, we are exposed to a daily dose of false or misleading news reports, hoaxes, conspiracy theories, click-bait headlines, junk science, and even satire<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. We refer to such content collectively as &#x0201c;misinformation.&#x0201d; The financial incentives through advertising are well understood<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, but political motives can be equally powerful<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. The massive spread of digital misinformation has been identified as a major global risk<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> and alleged to influence elections and threaten democracies<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. While such claims are hard to prove<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, real harm of disinformation has been demonstrated in health and finance<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>.</p><p id="Par4">Social and computer scientists are engaged in efforts to study the complex mix of cognitive, social, and algorithmic biases that make us vulnerable to manipulation by online misinformation<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. These include information overload and finite attention<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, novelty of false news<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, the selective exposure<sup><xref ref-type="bibr" rid="CR13">13</xref>&#x02013;<xref ref-type="bibr" rid="CR15">15</xref></sup> caused by polarized and segregated online social networks<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>, algorithmic popularity bias<sup><xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref></sup>, and other cognitive vulnerabilities such as confirmation bias and motivated reasoning<sup><xref ref-type="bibr" rid="CR21">21</xref>&#x02013;<xref ref-type="bibr" rid="CR23">23</xref></sup>.</p><p id="Par5">Abuse of online information ecosystems can both exploit and reinforce these vulnerabilities. While fabricated news are not a new phenomenon<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, the ease with which social media can be manipulated<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> creates novel challenges and particularly fertile grounds for sowing disinformation<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Public opinion can be influenced thanks to the low cost of producing fraudulent websites and high volumes of software-controlled profiles, known as social bots<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>. These fake accounts can post content and interact with each other and with legitimate users via social connections, just like real people<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Bots can tailor misinformation and target those who are most likely to believe it, taking advantage of our tendencies to attend to what appears popular, to trust information in a social setting<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, and to trust social contacts<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Since earliest manifestations uncovered in 2010<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>, we have seen influential bots affect online debates about vaccination policies<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> and participate actively in political campaigns, both in the United States<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> and other countries<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>.</p><p id="Par6">The fight against online misinformation requires a grounded assessment of the relative impact of different mechanisms by which it spreads. If the problem is mainly driven by cognitive limitations, we need to invest in news literacy education; if social media platforms are fostering the creation of echo chambers, algorithms can be tweaked to broaden exposure to diverse views; and if malicious bots are responsible for many of the falsehoods, we can focus attention on detecting this kind of abuse. Here we focus on gauging the latter effect. With few exception<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>, the literature about the role played by social bots in the spread of misinformation is largely based on anecdotal or limited evidence; a quantitative understanding of the effectiveness of misinformation-spreading attacks based on social bots is still missing. A large-scale, systematic analysis of the spread of misinformation by social bots is now feasible thanks to two tools developed in our lab: the Hoaxy platform to track the online spread of claims<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> and the Botometer machine learning algorithm to detect social bots<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Here we examine social bots and how they promote the spread of misinformation through millions of Twitter posts during and following the 2016 US presidential campaign. We find that social bots amplify the spread of misinformation by exposing humans to this content and inducing them to share it.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Low-credibility content</title><p id="Par7">Our analysis is based on a large corpus of news stories posted on Twitter. Operationally, rather than focusing on individual stories that have been debunked by fact-checkers, we consider low-credibility content, i.e., content from low-credibility sources. Such sources are websites that have been identified by reputable third-party news and fact-checking organizations as routinely publishing various types of low-credibility information (see Methods). There are two reasons for this approach<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. First, these sources have processes for the publication of disinformation: they mimic news media outlets without adhering to the professional standards of journalistic integrity. Second, fact-checking millions of individual articles is unfeasible. As a result, this approach is widely adopted in the literature (see&#x000a0;Supplementary Discussion).</p><p id="Par8">We track the complete production of 120 low-credibility sources by crawling their websites and extracting all public tweets with links to their stories. Our own analysis of a sample of these articles confirms that the vast majority of low-credibility content is some type of misinformation (see Methods). We also crawled and tracked the articles published by seven independent fact-checking organizations. The present analysis focuses on the period from mid-May 2016 to the end of March 2017. During this time, we collected 389,569 articles from low-credibility sources and 15,053 articles from fact-checking sources. We further collected from Twitter all of the public posts linking to these articles: 13,617,425 tweets linked to low-credibility sources and 1,133,674 linked to fact-checking sources. See Methods and Supplementary Methods for details.</p></sec><sec id="Sec4"><title>Spreading patterns and actors</title><p id="Par9">On average, a low-credibility source published approximately 100 articles per week. By the end of the study period, the mean popularity of those articles was approximately 30 tweets per article per week (see Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). However, as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, success is extremely heterogeneous across articles. Whether we measure success by number of posts containing a link (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>) or by number of accounts sharing an article (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>), we find a very broad distribution of popularity spanning several orders of magnitude: while the majority of articles goes unnoticed, a significant fraction goes &#x0201c;viral.&#x0201d; We observe that the popularity distribution of low-credibility articles is almost indistinguishable from that of fact-checking articles, meaning that low-credibility content is equally or more likely to spread virally. This result is similar to that of an analysis based on only fact-checked claims, which found false news to be even more viral than real news<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. The qualitative conclusion is the same: links to low-credibility content reach massive exposure.<fig id="Fig1"><label>Fig. 1</label><caption><p>Online virality of content. <bold>a</bold> Probability distribution (density function) of the number of tweets for articles from both low-credibility (blue circles) and fact-checking (orange squares) sources. The distributions of the number of accounts sharing an article are very similar (see Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>). As illustrations, the diffusion networks of two stories are shown: <bold>b</bold> a medium-virality misleading article titled &#x0201c;FBI just released the Anthony Weiner warrant, and it proves they stole election&#x0201d;, published a month after the 2016 US election and shared in over 400 tweets; and <bold>c</bold> a highly viral fabricated news report titled &#x0201c;Spirit cooking&#x0201d;: Clinton campaign chairman practices bizarre occult ritual, published 4 days before the 2016 US election and shared in over 30,000 tweets. In both cases, only the largest connected component of the network is shown. Nodes and links represent Twitter accounts and retweets of the article, respectively. Node size indicates account influence, measured by the number of times an account was retweeted. Node color represents bot score, from blue (likely human) to red (likely bot); yellow nodes cannot be evaluated because they have either been suspended or deleted all their tweets. An interactive version of the larger network is available online (<ext-link ext-link-type="uri" xlink:href="http://iunetsci.github.io/HoaxyBots/">iunetsci.github.io/HoaxyBots/</ext-link>). Note that Twitter does not provide data to reconstruct a retweet tree; all retweets point to the original tweet. The retweet networks shown here combine multiple cascades (each a &#x0201c;star network&#x0201d; originating from a different tweet) that all share the same article link</p></caption><graphic xlink:href="41467_2018_6930_Fig1_HTML" id="d29e478"/></fig></p><p id="Par10">Even though low-credibility and fact-checking sources show similar popularity distributions, we observe some distinctive patterns in the spread of low-credibility content. First, most articles by low-credibility sources spread through original tweets and retweets, while few are shared in replies (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref>); this is different from articles by fact-checking sources, which are shared mainly via retweets but also replies (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>). In other words, the spreading patterns of low-credibility content are less &#x0201c;conversational.&#x0201d; Second, the more a story was tweeted, the more the tweets were concentrated in the hands of few accounts, who act as &#x0201c;super-spreaders&#x0201d; (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2c</xref>). This goes against the intuition that, as a story reaches a broader audience organically, the contribution of any individual account or group of accounts should matter less. In fact, a single account can post the same low-credibility article hundreds or even thousands of times (see Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">6</xref>). This could suggest that the spread is amplified through automated means.<fig id="Fig2"><label>Fig. 2</label><caption><p>Anomalies. The distribution of types of tweet spreading articles from <bold>a</bold> low-credibility and <bold>b</bold> fact-checking sources are quite different. Each article is mapped along three axes representing the percentages of different types of messages that share it: original tweets, retweets, and replies. When user Alice retweets a tweet by user Bob, the tweet is rebroadcast to all of Alice&#x02019;s followers, whereas when she replies to Bob&#x02019;s tweet, the reply is only seen by Bob and users who follow them both. Color represents the number of articles in each bin, on a log scale. <bold>c</bold> Correlation between popularity of articles from low-credibility sources and concentration of posting activity. We consider a collection of articles shared by a minimum number of tweets as a popularity group. For articles in each popularity group, a violin plot shows the distribution of Gini coefficients which measure concentration of posts by few accounts (see&#x000a0;Supplementary Methods). In violin plots, the width of a contour represents the probability of the corresponding value, and the median is marked by a colored line. <bold>d</bold> Bot score distributions for a random sample of 915 accounts who posted at least one link to a low-credibility source (orange), and for the 961 &#x0201c;super-spreaders&#x0201d; that most actively shared content from low-credibility sources (blue). The two groups have significantly different scores (<italic>p</italic>&#x02009;&#x0003c;&#x02009;10<sup>&#x02212;4</sup> according to the Mann&#x02013;Whitney <italic>U</italic> test): super-spreaders are more likely bots</p></caption><graphic xlink:href="41467_2018_6930_Fig2_HTML" id="d29e523"/></fig></p><p id="Par11">We hypothesize that the &#x0201c;super-spreaders&#x0201d; of low-credibility content are social bots which are automatically posting links to articles, retweeting other accounts, or performing more sophisticated autonomous tasks, like following and replying to other users. To test this hypothesis, we used Botometer to evaluate the Twitter accounts that posted links to articles from low-credibility sources. For each account we computed a bot score (a number in the unit interval) which can be interpreted as the level of automation of that account. We used a threshold of 0.5 to classify an account as bot or human. Details about the Botometer system and the threshold can be found in Methods. We first considered a random sample of the general population of accounts that shared at least one link to a low-credibility article. Only 6% of accounts in the sample are labeled as bots using this method, but they are responsible for spreading 31% of all tweets linking to low-credibility content, and 34% of all articles from low-credibility sources (Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>). We then compared this group with a sample of the top most active accounts (&#x0201c;super-spreaders&#x0201d;), 33% of which have been labeled as bot&#x02014;over five times as many (details in&#x000a0;Supplementary Methods). Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2d</xref> confirms that the super-spreaders are significantly more likely to be bots compared to the general population of accounts who share low-credibility content. Because these results are based on a classification model, it is important to make sure that what we see in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2d</xref> is not due to bias in the way Botometer was trained&#x02014;that the model did not simply learn to assign higher scores to more active accounts. We rule out this competing explanation by showing that higher bot scores cannot be attributed to this kind of bias in the learning model (see Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">16</xref>).</p></sec><sec id="Sec5"><title>Bot strategies</title><p id="Par12">Given this evidence, we submit that bots may play a critical role in driving the viral spread of content from low-credibility sources. To test this question, we examined whether bots tend to get involved at particular times in the spread of popular articles. As shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>, likely bots are more prevalent in the first few seconds after an article is first published on Twitter than at later times. We conjecture that this early intervention exposes many users to low-credibility articles, increasing the chances than an article goes &#x0201c;viral.&#x0201d;<fig id="Fig3"><label>Fig. 3</label><caption><p>Bot strategies. <bold>a</bold> Early bot support after a viral low-credibility article is first shared. We consider a sample of 60,000 accounts that participate in the spread of the 1000 most viral stories from low-credibility sources. We align the times when each article first appears. We focus on the 1&#x02009;h early spreading phase following each of these events, and divide it into logarithmic lag intervals. The plot shows the bot score distribution for accounts sharing the articles during each of these lag intervals. <bold>b</bold> Targeting of influentials. We plot the average number of followers of Twitter users who are mentioned (or replied to) by accounts that link to the most viral 1000 stories. The mentioning accounts are aggregated into three groups by bot score percentile. Error bars indicate standard errors. Inset: Distributions of follower counts for users mentioned by accounts in each percentile group</p></caption><graphic xlink:href="41467_2018_6930_Fig3_HTML" id="d29e560"/></fig></p><p id="Par13">We find that another strategy often used by bots is to mention influential users in tweets that link to low-credibility content. Bots seem to employ this targeting strategy repetitively; for example, a single account mentioned @realDonaldTrump in 19 tweets, each linking to the same false claim about millions of votes by illegal immigrants (see details in&#x000a0;Supplementary Discussion and Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">7</xref>). For a systematic investigation, let us consider all tweets that mention or reply to a user and include a link to a viral article from a low-credibility source in our corpus. The number of followers is often used as a proxy for the influence of a Twitter user. As shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3b</xref>, in general tweets tend to mention popular people. However, accounts with the largest bot scores tend to mention users with a larger number of followers (median and average). A possible explanation for this strategy is that bots (or rather, their operators) target influential users with content from low-credibility sources, creating the appearance that it is widely shared. The hope is that these targets will then reshare the content to their followers, thus boosting its credibility.</p></sec><sec id="Sec6"><title>Bot impact</title><p id="Par14">Having found that automated accounts are employed in ways that appear to drive the viral spread of low-credibility articles, let us explore how humans interact with the content shared by bots, which may provide insight into whether and how bots are able to affect public opinion. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4a</xref> shows who retweets whom: humans do most of the retweeting (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4b</xref>), and they retweet articles posted by likely bots almost as much as those by other humans (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4c</xref>). This result, which is robust to the choice of threshold used to identify likely humans, suggests that collectively, people do not discriminate between low-credibility content shared by humans versus social bots. It also means that when we observe many accounts exposed to low-credibility information, these are not just bots (re)tweeting it. In fact, we find that the volume of tweets by likely humans scales super-linearly with the volume by likely bots, suggesting that the reach of these articles among humans is amplified by social bots. In other words, each amount of sharing activity by likely bots tends to trigger a disproportionate amount of human engagement. The same amplification effect is not observed for articles from fact-checking sources. Details are presented in&#x000a0;Supplementary Discussion (Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">8</xref>, <xref rid="MOESM1" ref-type="media">9</xref>).<fig id="Fig4"><label>Fig. 4</label><caption><p>Impact of bots on humans. <bold>a</bold> Joint distribution of bot scores of accounts that retweeted links to low-credibility articles and accounts that had originally posted the links. Color represents the number of retweeted messages in each bin, on a log scale. <bold>b</bold> The top projection shows the distribution of bot scores for retweeters, who are mostly human. <bold>c</bold> The left projection shows the distribution of bot scores for accounts retweeted by likely humans who are identified by scores below a threshold of 0.4 (black crosses), 0.5 (purple stars), or 0.6 (orange circles). Irrespective of the threshold, we observe a significant portion of likely bots retweeted by likely humans</p></caption><graphic xlink:href="41467_2018_6930_Fig4_HTML" id="d29e612"/></fig></p><p id="Par15">Another way to assess the impact of bots in the spread of low-credibility content is to examine their critical role within the diffusion network. Let us focus on the retweet network<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, where nodes are accounts and connections represents retweets of messages with links to stories&#x02014;just like the networks in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b, c</xref>, but aggregating across all articles from low-credibility sources. We apply a network dismantling procedure<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>: we disconnect one node at a time and analyze the resulting decrease in the total volume of retweets and in the total number of unique articles. The more these quantities are reduced by disconnecting a small number of nodes, the more critical those nodes are in the network. We prioritize accounts to disconnect based on bot score and, for comparison, also based on retweeting activity and influence. Further details can be found in the Methods. Unsurprisingly, Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows that influential nodes are most critical. The most influential nodes are unlikely to be bots, however. Disconnecting nodes with high bot score is the second-best strategy for reducing low-credibility articles (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5a</xref>). For reducing overall post volume, this strategy performs well when about 10% of nodes are disconnected (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref>). Disconnecting active nodes is not as efficient a strategy for reducing low-credibility articles. These results show that bots are critical in the diffusion network, and that targeting them would significantly improve the quality of information in the network. The spread of links to low-credibility content can be virtually eliminated by disconnecting a small percentage of accounts that are most likely to be bots.<fig id="Fig5"><label>Fig. 5</label><caption><p>Dismantling the low-credibility content diffusion network. This analysis is based on a network of retweets linking to low-credibility articles, collected during the 2016 US presidential campaign. The network has 227,363 nodes (accounts); see Methods for further details. The priority of disconnected nodes is determined by ranking accounts on the basis of the different characteristics shown in the legend. The remaining fraction of <bold>a</bold> unique articles from low-credibility sources and <bold>b</bold> retweets linking to those articles is plotted versus the number of disconnected nodes</p></caption><graphic xlink:href="41467_2018_6930_Fig5_HTML" id="d29e650"/></fig></p><p id="Par16">Finally, we compared the extent to which social bots disseminate content from different low-credibility sources. We considered the most popular sources in terms of median and aggregate article posts, and measured the bot scores of the accounts that most actively spread their content. As shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, one site (<ext-link ext-link-type="uri" xlink:href="http://beforeitsnews.com">beforeitsnews.com</ext-link>) stands out for the high degree of automation, but other popular low-credibility sources also have many likely bots among their promoters. The dissemination of content from satire sites like The Onion and fact-checking websites does not display the same level of automation; it appears to be more organic.<fig id="Fig6"><label>Fig. 6</label><caption><p>Popularity and bot support for the top sources. Satire websites are shown in orange, fact-checking sites in blue, and low-credibility sources in red. Popularity is measured by total tweet volume (horizontal axis) and median number of tweets per article (circle area). Bot support is gauged by the median bot score of the 100 most active accounts posting links to articles from each source (vertical axis). Low-credibility sources have greater support by bots, as well as greater median and/or total volume in many cases</p></caption><graphic xlink:href="41467_2018_6930_Fig6_HTML" id="d29e669"/></fig></p></sec></sec><sec id="Sec7" sec-type="discussion"><title>Discussion</title><p id="Par17">Our analysis provides quantitative empirical evidence of the key role played by social bots in the spread of low-credibility content. Relatively few accounts are responsible for a large share of the traffic that carries misinformation. These accounts are likely bots, and we uncovered two manipulation strategies they use. First, bots are particularly active in amplifying content in the very early spreading moments, before an article goes &#x0201c;viral.&#x0201d; Second, bots target influential users through replies and mentions. People are vulnerable to these kinds of manipulation, in the sense that they retweet bots who post low-credibility content almost as much as they retweet other humans. As a result, bots amplify the reach of low-credibility content, to the point that it is statistically indistinguishable from that of fact-checking articles. Successful low-credibility sources in the United States, including those on both ends of the political spectrum, are heavily supported by social bots. Social media platforms are beginning to acknowledge these problems and deploy countermeasures, although their effectiveness is hard to evaluate<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR35">35</xref></sup>.</p><p id="Par18">The present findings complement the recent work by Vosoughi et al.<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> who argue that bots alone do not entirely explain the success of false news. Their analysis is based on a small subset of articles that are fact-checked, whereas the present work considers a much broader set of articles from low-credibility sources, most of which are not fact-checked. In addition, the analysis of Vosoughi et al.<sup><xref ref-type="bibr" rid="CR2">2</xref></sup> does not consider an important mechanism by which bots can amplify the spread of an article, namely, by resharing links originally posted by human accounts. Because of these two methodological differences, the present analysis provides new evidence about the role played by bots.</p><p id="Par19">Our results are robust with respect to various choices. First, using a more restrictive criterion for selecting low-credibility sources, based on a consensus among several news and fact-checking organizations (see Methods), yields qualitatively similar results, leading to the same conclusions. In addition, the findings are not driven by any single source associated with a large portion of the tweet volume. Second, our analysis about active spreaders of low-credibility content being likely bots is robust with respect to the activity threshold used to identify the most active spreaders. Furthermore, bot scores are uncorrelated with account activity volume. Third, the conclusions are not affected by the use of different bot score thresholds to separate social bots and human accounts. Details about all of these robustness analyses can be found in&#x000a0;the Supplementary Discussion (Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">10</xref>&#x02013;<xref rid="MOESM1" ref-type="media">15</xref>).</p><p id="Par20">Our findings demonstrate that social bots are an effective tool to manipulate social media. While the present study focuses on the spread of low-credibility content, such as false news, conspiracy theories, and junk science, similar bot strategies may be used to spread other types of malicious content, such as malware. Although our spreading data are collected from Twitter, there is no reason to believe that the same kind of abuse is not taking place on other digital platforms as well. In fact, viral conspiracy theories spread on Facebook<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> among the followers of pages that, like social bots, can easily be managed automatically and anonymously. While the difficulty to access spreading data on platforms like Facebook is a concern, the growing popularity of ephemeral social media like Snapchat may make future studies of this type of abuse all but impossible.</p><p id="Par21">The results presented here suggest that curbing social bots may be an effective strategy for mitigating the spread of low-credibility content, and that the bot score might provide a useful signal to prioritize accounts for further review. Progress in this direction may be accelerated through partnerships between social media platforms and academic research<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. For example, our lab and others are developing machine learning algorithms to detect social bots<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. The deployment of such tools is fraught with peril, however. While platforms have the right to enforce their terms of service, which forbid impersonation and deception, algorithms do make mistakes. Even a single false-positive error leading to the suspension of a legitimate account may foster valid concerns about censorship. This justifies current human-in-the-loop solutions which unfortunately do not scale with the volume of abuse that is enabled by software. It is therefore imperative to support research both on improved abuse detection algorithms and on countermeasures that take into account the complex interplay between the cognitive and technological factors that favor the spread of misinformation<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>.</p><p id="Par22">An alternative strategy would be to employ CAPTCHAs<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, challenge&#x02013;response tests to determine whether a user is human. CAPTCHAs have been deployed widely and successfully to combat email spam and other types of online abuse. Their use to limit automatic posting or resharing of news links could help stem bot abuse by increasing its cost, but also add undesirable friction to benign applications of automation by legitimate entities, such as news media and emergency response coordinators. These are hard trade-offs that must be studied carefully as we contemplate ways to address the fake news epidemics.</p><p id="Par23">The present study focuses on the role of social bots in the spread of low-credibility content. These kinds of bots are often deceptive. For example, none of the ten Twitter accounts most active at retweeting articles in the core of the misinformation network during the study period identified themselves as bots<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. One question that has not been addressed is whether similar patterns of amplification might be observed in the spread of content from legitimate, high-quality news sources. Mainstream media do use automated accounts to post news feeds, although these bots do not deceptively impersonate humans. While preliminary analysis suggests that mainstream media do not display the same systematic bot support observed for low-credibility sources (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">8</xref>), the use of bots to promote legitimate news content deserves further investigation.</p><p id="Par24">Finally, the present study focuses on US sources during the period preceding and following the 2016 US presidential election. It will be important to explore whether bot manipulation of social media platforms is concentrated around major electoral events in the United States and other countries.</p></sec><sec id="Sec8"><title>Methods</title><sec id="Sec9"><title>Hoaxy data</title><p id="Par25">Data about articles shared on Twitter were collected through Hoaxy, an open platform developed at Indiana University to track the spread of claims and fact checking<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. A search engine, interactive visualizations, and open-source software are freely available (<ext-link ext-link-type="uri" xlink:href="http://hoaxy.iuni.iu.edu">hoaxy.iuni.iu.edu</ext-link>). The data are accessible through a public application program interface (API). Further details are presented in&#x000a0;Supplementary Methods.</p><p id="Par26">The collection period for the present analysis extends from mid-May 2016 until the end of March 2017. During this time, we collected 389,569 articles from 120 low-credibility sites. We also tracked 15,053 stories published by independent fact-checking organizations, such as <ext-link ext-link-type="uri" xlink:href="http://snopes.com">snopes.com</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://politifact.com">politifact.com</ext-link>, and <ext-link ext-link-type="uri" xlink:href="http://factcheck.org">factcheck.org</ext-link>.</p><p id="Par27">The list of low-credibility sources was obtained by merging several lists compiled by third-party news and fact-checking organizations or experts. The collection started with 71 sites and 49 more were added in mid-December 2016. The full list of sources and their provenance is reported in Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>. Many low-credibility sources label their content as satirical, and viral satire is sometimes mistaken for real news. For these reasons, satire sites are not excluded from the list of low-credibility sources. However, our findings are robust with respect to this choice. The Onion is the satirical source with the highest total volume of shares. We repeated our analyses of most viral articles (e.g., Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>) with articles from <ext-link ext-link-type="uri" xlink:href="http://theonion.com">theonion.com</ext-link> excluded and the results were not affected.</p><p id="Par28">We also repeated the analysis using a more restrictive criterion for selecting low-credibility sources, based on a consensus among three or more news and fact-checking organizations. This yields 327,840 articles (86% of the total) from 65 low-credibility sources, also listed in&#x000a0;Supplementary Methods, where we show that the results are robust with respect to these different source selection criteria.</p><p id="Par29">Our analysis does not require a complete list of low-credibility sources, but does rely on the assumption that many articles published by these sources can be classified as some kind of misinformation or unsubstantiated information. To validate this assumption, we checked the content of a random sample of articles. For the purpose of this verification, we adopted a definition of &#x0201c;misinformation&#x0201d; that follows industry convention and includes the following classes: fabricated content, manipulated content, imposter content, false context, misleading content, false connection, and satire<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. To these seven categories we also added articles whose claims could not be verified. We found that fewer that 15% of articles could be verified. More details are available in&#x000a0;Supplementary Methods (Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">3</xref>, <xref rid="MOESM1" ref-type="media">4</xref>).</p><p id="Par30">Using the filtering endpoint of Twitter&#x02019;s public streaming API, we collected 13,617,425 public posts that included links to articles from low-credibility sources and 1,133,674 public posts linking to fact checks. This is the complete set of tweets linking to these articles in the study period, and not a sample (see&#x000a0;Supplementary Methods for details). We extracted metadata about the source of each link, the account that shared it, the original poster in case of retweet or quoted tweet, and any users mentioned or replied to in the tweet.</p><p id="Par31">We transformed links to canonical URLs to merge different links referring to the same article. This happens mainly due to shortening services (44% links are redirected) and extra parameters (34% of URLs contain analytics tracking parameters), but we also found websites that use duplicate domains and snapshot services. Canonical URLs were obtained by resolving redirection and removing analytics parameters.</p><p id="Par32">In the targeting analysis (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3b</xref>), we exclude mentions of sources using the pattern &#x0201c;via @screen_name.&#x0201d;</p></sec><sec id="Sec10"><title>Botometer scores</title><p id="Par33">The bot score of Twitter accounts is computed using the Botometer classifier which evaluates the extent to which an account exhibits similarity to the characteristics of social bots<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. The system is based on a supervised machine learning algorithm leveraging more than a thousand features extracted from public data and metadata about Twitter accounts. These features include various descriptors of information diffusion networks, user metadata, friend statistics, temporal patterns of activity, part-of-speech, and sentiment analysis. The classifier is trained using publicly available datasets of tens of thousands of Twitter users that include both humans and bots of varying sophistication. The Botometer system is available through a public API (<ext-link ext-link-type="uri" xlink:href="http://botometer.iuni.iu.edu">botometer.iuni.iu.edu</ext-link>). It has also been employed in other studies<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup> and is widely adopted, serving hundreds of thousand requests daily.</p><p id="Par34">For the present analysis, we use the Twitter Search API to collect up to 200 of an account&#x02019;s most recent tweets and up to 100 of the most recent tweets mentioning the account. From these data we extract the features used by the Botometer classifier. We use logistic calibration to make the bot scores calculated by the classifier easier to interpret as confidence levels (see&#x000a0;Supplementary Methods and Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">5</xref>).</p><p id="Par35">There are many types of bots and humans using different levels of automation. Accordingly, Botometer provides a score rather than a binary classification. Nevertheless, the model can effectively discriminate between human and bot accounts of different nature; fivefold cross-validation yields an area under the receiver operating characteristic curve (AUC) of 94%<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. (An AUC value of 50% indicates random accuracy and 100% means perfect accuracy.) When a binary classification is needed, we use a bot score threshold of 0.5 which maximizes accuracy<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. See&#x000a0;Supplementary Methods and Discussion for further details about bot classification and the robustness of results based on a bot score threshold.</p></sec><sec id="Sec11"><title>Retweet network</title><p id="Par36">The network studied in the dismantling analysis (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>) is based on retweets with links to articles from low-credibility sources, posted before the 2016 US presidential election (16 May&#x02013;7 November 2016). The network has 227,363 nodes (accounts) and 816,453 directed edges. Each edge is weighted by the number of retweets between the same pair of accounts. When an account is disconnected, all of its incoming and outgoing edges are removed. When we disconnect a retweeting node <italic>i</italic> that was in turn retweeted by some node <italic>j</italic>, only <italic>i</italic> is removed because in the Twitter metadata, each retweet connects directly to the account that originated the tweet. Given the directionality of edges, retweeting activity is measured by node in-strength centrality (weighted in-degree) and influence by out-strength centrality (weighted out-degree).</p></sec><sec id="Sec12"><title>Code availability</title><p id="Par37">Code used to carry out the analyses in this manuscript is available on Github (<ext-link ext-link-type="uri" xlink:href="http://github.com/IUNetSci/HoaxyBots">github.com/IUNetSci/HoaxyBots</ext-link>). Hoaxy is an open-source project and all system software is public (<ext-link ext-link-type="uri" xlink:href="http://github.com/IUNetSci">github.com/IUNetSci</ext-link>). Reasonable additional requests and questions about code can be directed to the corresponding author.</p></sec></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec13"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41467_2018_6930_MOESM1_ESM.pdf"><caption><p>Supplementary Information</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Electronic supplementary material</title><p><bold>Supplementary Information</bold> accompanies this paper at 10.1038/s41467-018-06930-7.</p></sec><ack><title>Acknowledgements</title><p>We are grateful to Ben Serrette and Valentin Pentchev of the Indiana University Network Science Institute (<ext-link ext-link-type="uri" xlink:href="http://iuni.iu.edu">iuni.iu.edu</ext-link>), as well as Lei Wang for supporting the development of the Hoaxy platform. Pik-Mai Hui assisted with the dismantling analysis. Clayton A. Davis developed the Botometer API. Nic Dias provided assistance with claim verification. We are also indebted to Twitter for providing data through their API. C.S. thanks the Center for Complex Networks and Systems Research (<ext-link ext-link-type="uri" xlink:href="http://cnets.indiana.edu">cnets.indiana.edu</ext-link>) for the hospitality during his visit at the Indiana University School of Informatics, Computing, and Engineering. He was supported by the China Scholarship Council. G.L.C. was supported by IUNI. K.-C.Y. was supported by the National Science Foundation (award 1735095). The development of the Botometer platform was supported in part by DARPA (grant W911NF-12-1-0037). The development of the Hoaxy platform was supported in part by the Democracy Fund (grant R-RR-201705-01961). A.F. and F.M. were supported in part by the James S. McDonnell Foundation (grant 220020274) and the National Science Foundation (award 1101743). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>C.S., G.L.C., A.F. and F.M. conceived and designed the research. C.S., O.V., K.-C.Y. and G.L.C. collected data. C.S., O.V., K.-C.Y. and G.L.C. prepared figures. C.S., G.L.C., O.V., A.F. and F.M. analyzed data. G.L.C., A.F. and F.M. wrote the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>There are two data sources analyzed during the current study: Hoaxy for data about article diffusion, and Botometer for data about Twitter bot scores. Further details are available in&#x000a0;Supplementary Methods. Datasets used to carry out the analyses in this manuscript are available on Zenodo (10.5281/zenodo.1402267). Additionally, data about article diffusion and bot scores are available through the public Hoaxy API (<ext-link ext-link-type="uri" xlink:href="http://hoaxy.iuni.iu.edu">hoaxy.iuni.iu.edu</ext-link>) and the public Botometer API (<ext-link ext-link-type="uri" xlink:href="http://botometer.iuni.iu.edu">botometer.iuni.iu.edu</ext-link>), respectively. Reasonable additional requests and questions about data or APIs can be directed to the corresponding author.</p></notes><notes notes-type="COI-statement"><sec id="FPar1"><title>Competing interests</title><p>The authors declare no competing interests.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Gottfried, J. &#x00026; Shearer, E. News use across social media platforms 2016. White Paper, Pew Research Center (2016). <ext-link ext-link-type="uri" xlink:href="http://www.journalism.org/2016/05/26/news-use-across-social-media-platforms-2016/">http://www.journalism.org/2016/05/26/news-use-across-social-media-platforms-2016/</ext-link></mixed-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vosoughi</surname><given-names>S</given-names></name><name><surname>Roy</surname><given-names>D</given-names></name><name><surname>Aral</surname><given-names>S</given-names></name></person-group><article-title>The spread of true and false news online</article-title><source>Science</source><year>2018</year><volume>359</volume><fpage>1146</fpage><lpage>1151</lpage><pub-id pub-id-type="doi">10.1126/science.aap9559</pub-id><pub-id pub-id-type="pmid">29590045</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Markines, B., Cattuto, C. &#x00026; Menczer, F. Social spam detection. In <italic>Proc. 5th International Workshop on Adversarial Information Retrieval on the Web (AIRWeb)</italic> (ACM, New York, 2009).</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Mustafaraj, E. &#x00026; Metaxas, P. T. From obscurity to prominence in minutes: Political speech and real-time search. In <italic>Proc. Web Science Conference: Extending the Frontiers of Society On-Line</italic> (Raleigh, 2010).</mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Ratkiewicz, J. et al. Detecting and tracking political abuse in social media. In <italic>Proc. 5th International AAAI Conference on Weblogs and Social Media (ICWSM)</italic> (AAAI, Palo Alto, 2011).</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Howell, L. et al. Digital wildfires in a hyperconnected world. In <italic>Global Risks</italic> (World Economic Forum, 2013).</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Gu, L., Kropotov, V. &#x00026; Yarochkin, F. The fake news machine: how propagandists abuse the internet and manipulate the public. TrendLabs Research Paper, Trend Micro (2017). <ext-link ext-link-type="uri" xlink:href="https://documents.trendmicro.com/assets/white_papers/wp-fake-news-machine-how-propagandists-abuse-the-internet.pdf">https://documents.trendmicro.com/assets/white_papers/wp-fake-news-machine-how-propagandists-abuse-the-internet.pdf</ext-link>.</mixed-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allcott</surname><given-names>H</given-names></name><name><surname>Gentzkow</surname><given-names>M</given-names></name></person-group><article-title>Social media and fake news in the 2016 election</article-title><source>J. Econ. Perspect.</source><year>2017</year><volume>31</volume><fpage>211</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1257/jep.31.2.211</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hotez</surname><given-names>PJ</given-names></name></person-group><article-title>Texas and its measles epidemics</article-title><source>PLoS Med.</source><year>2016</year><volume>13</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1371/journal.pmed.1002153</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrara</surname><given-names>E</given-names></name><name><surname>Varol</surname><given-names>O</given-names></name><name><surname>Davis</surname><given-names>C</given-names></name><name><surname>Menczer</surname><given-names>F</given-names></name><name><surname>Flammini</surname><given-names>A</given-names></name></person-group><article-title>The rise of social bots</article-title><source>Comm. ACM</source><year>2016</year><volume>59</volume><fpage>96</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1145/2818717</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lazer</surname><given-names>D</given-names></name><etal/></person-group><article-title>The science of fake news</article-title><source>Science</source><year>2018</year><volume>359</volume><fpage>1094</fpage><lpage>1096</lpage><pub-id pub-id-type="doi">10.1126/science.aao2998</pub-id><pub-id pub-id-type="pmid">29590025</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>X</given-names></name><name><surname>Oliveira</surname><given-names>DFM</given-names></name><name><surname>Shirazi</surname><given-names>AS</given-names></name><name><surname>Flammini</surname><given-names>A</given-names></name><name><surname>Menczer</surname><given-names>F</given-names></name></person-group><article-title>Limited individual attention and online virality of low-quality information</article-title><source>Nat. Human. Behav.</source><year>2017</year><volume>1</volume><fpage>0132</fpage><pub-id pub-id-type="doi">10.1038/s41562-017-0132</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Sunstein, C. R. <italic>Going to Extremes: How Like Minds Unite and Divide</italic> (Oxford University Press, Oxford, 2009).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Pariser, E. <italic>The Filter Bubble: How the New Personalized Web Is Changing What We Read and How We Think</italic> (Penguin, New York, 2011).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Nikolov, D., Lalmas, M., Flammini, A. &#x00026; Menczer, F. Quantifying biases in online information exposure. <italic>J. Am. Soc. Inform. Sci. Technol.</italic>&#x000a0;Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.06958">https://arxiv.org/abs/1807.06958</ext-link> (2018).</mixed-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conover</surname><given-names>MD</given-names></name><name><surname>Gon&#x000e7;alves</surname><given-names>B</given-names></name><name><surname>Flammini</surname><given-names>A</given-names></name><name><surname>Menczer</surname><given-names>F</given-names></name></person-group><article-title>Partisan asymmetries in online political activity</article-title><source>EPJ Data Sci.</source><year>2012</year><volume>1</volume><fpage>6</fpage><pub-id pub-id-type="doi">10.1140/epjds6</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Conover, M. et al. Political polarization on Twitter. In <italic>Proc. 5th International AAAI Conference on Weblogs and Social Media (ICWSM)</italic> (AAAI, Barcelona, 2011).</mixed-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salganik</surname><given-names>MJ</given-names></name><name><surname>Dodds</surname><given-names>PS</given-names></name><name><surname>Watts</surname><given-names>DJ</given-names></name></person-group><article-title>Experimental study of inequality and unpredictability in an artificial cultural market</article-title><source>Science</source><year>2006</year><volume>311</volume><fpage>854</fpage><lpage>856</lpage><pub-id pub-id-type="doi">10.1126/science.1121066</pub-id><pub-id pub-id-type="pmid">16469928</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Hodas, N. O. &#x00026; Lerman, K. How limited visibility and divided attention constrain social contagion. In <italic>Proc. ASE/IEEE International Conference on Social Computing</italic> (IEEE Computer Society, Washington, 2012).</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Ciampaglia, G. L., Nematzadeh, A., Menczer, F. &#x00026; Flammini, A. How algorithmic popularity bias hinders or promotes quality. <italic>Sci. Rep.&#x000a0;</italic><bold>8</bold>, 15951 (2018).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Stroud, N. <italic>Niche News: The Politics of News Choice</italic> (Oxford University Press, Oxford, 2011).</mixed-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahan</surname><given-names>DM</given-names></name></person-group><article-title>Ideology, motivated reasoning, and cognitive reflection</article-title><source>Judgm. Decis. Mak.</source><year>2013</year><volume>8</volume><fpage>407</fpage><lpage>424</lpage></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levendusky</surname><given-names>MS</given-names></name></person-group><article-title>Why do partisan media polarize viewers?</article-title><source>Am. J. Pol. Sci.</source><year>2013</year><volume>57</volume><fpage>611</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1111/ajps.12008</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Lippmann, W. <italic>Public Opinion</italic> (Harcourt, Brace and Company, New York, 1922).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Weedon, J., Nuland, W. &#x00026; Stamos, A. Information Operations and Facebook. White paper, Facebook (2017). <ext-link ext-link-type="uri" xlink:href="https://fbnewsroomus.files.wordpress.com/2017/04/facebook-and-information-operations-v1.pdf">https://fbnewsroomus.files.wordpress.com/2017/04/facebook-and-information-operations-v1.pdf</ext-link>.</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Varol, O., Ferrara, E., Davis, C. A., Menczer, F. &#x00026; Flammini, A. Online human-bot interactions: detection, estimation, and characterization. In <italic>Proc. Intl. AAAI Conf. on Web and Social Media (ICWSM)</italic> (AAAI, Palo Alto, 2017).</mixed-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Subrahmanian</surname><given-names>V</given-names></name><etal/></person-group><article-title>The DARPA Twitter Bot Challenge</article-title><source>IEEE Comput.</source><year>2016</year><volume>49</volume><fpage>38</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1109/MC.2016.183</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>Y</given-names></name><name><surname>Meng</surname><given-names>R</given-names></name><name><surname>Johar</surname><given-names>GV</given-names></name></person-group><article-title>Perceived social presence reduces fact-checking</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>2017</year><volume>114</volume><fpage>5976</fpage><lpage>5981</lpage><pub-id pub-id-type="doi">10.1073/pnas.1700175114</pub-id><pub-id pub-id-type="pmid">28533396</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jagatic</surname><given-names>T</given-names></name><name><surname>Johnson</surname><given-names>N</given-names></name><name><surname>Jakobsson</surname><given-names>M</given-names></name><name><surname>Menczer</surname><given-names>F</given-names></name></person-group><article-title>Social phishing</article-title><source>Commun. ACM</source><year>2007</year><volume>50</volume><fpage>94</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1145/1290958.1290968</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Bessi, A. &#x00026; Ferrara, E. Social bots distort the 2016 US presidential election online discussion. <italic>First Monday</italic><bold>21</bold>, 11 (2016).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Woolley, S. C. &#x00026; Howard, P. N. Computational propaganda worldwide: Executive summary. Working Paper 2017.11 (Oxford Internet Institute Oxford, 2017).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Ferrara, E. Disinformation and Social Bot Operations in the Run Up to the 2017 French Presidential Election. <italic>First Monday</italic><bold>22</bold>, 8 (2017).</mixed-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shao</surname><given-names>C</given-names></name><etal/></person-group><article-title>Anatomy of an online misinformation network</article-title><source>PLoS One</source><year>2018</year><volume>13</volume><fpage>e0196087</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0196087</pub-id><pub-id pub-id-type="pmid">29702657</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albert</surname><given-names>R</given-names></name><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Barab&#x000e1;si</surname><given-names>AL</given-names></name></person-group><article-title>Error and attack tolerance of complex networks</article-title><source>Nature</source><year>2000</year><volume>406</volume><fpage>378</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1038/35019019</pub-id><pub-id pub-id-type="pmid">10935628</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Mosseri, A. News feed fyi: showing more informative links in news feed. Press release, Facebook (2017). <ext-link ext-link-type="uri" xlink:href="https://newsroom.fb.com/news/2017/06/news-feed-fyi-showing-more-informative-links-in-news-feed/">https://newsroom.fb.com/news/2017/06/news-feed-fyi-showing-more-informative-links-in-news-feed/</ext-link></mixed-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Del Vicario</surname><given-names>M</given-names></name><etal/></person-group><article-title>The spreading of misinformation online</article-title><source>Proc. Natl. Acad. Sci. USA</source><year>2016</year><volume>113</volume><fpage>554</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1073/pnas.1517441113</pub-id><pub-id pub-id-type="pmid">26729863</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewandowsky</surname><given-names>S</given-names></name><name><surname>Ecker</surname><given-names>UK</given-names></name><name><surname>Cook</surname><given-names>J</given-names></name></person-group><article-title>Beyond misinformation: understanding and coping with the &#x0201c;post-truth&#x0201d; era</article-title><source>J. Appl. Res. Mem. Cogn.</source><year>2017</year><volume>6</volume><fpage>353</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1016/j.jarmac.2017.07.008</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">von Ahn, L., Blum, M., Hopper, N. J. &#x00026; Langford, J. Captcha: Using hard AI problems for security. In <italic>Advances in Cryptology</italic> &#x02014; <italic>Proceedings of EUROCRYPT 2003: International Conference on the Theory and Applications of Cryptographic Techniques</italic> (ed. Biham, E.) 294&#x02013;311 (Springer, Heidelberg, 2003).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Wardle, C. Fake news. It&#x02019;s complicated. White Paper, First Draft News (2017). <ext-link ext-link-type="uri" xlink:href="https://firstdraftnews.com/fake-news-complicated/">https://firstdraftnews.com/fake-news-complicated/</ext-link></mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Wojcik, S., Messing, S., Smith, A., Rainie, L. &#x00026; Hitlin, P. Bots in the twittersphere. White Paper, Pew Research Center (2018). <ext-link ext-link-type="uri" xlink:href="http://www.pewinternet.org/2018/04/09/bots-in-the-twittersphere/">http://www.pewinternet.org/2018/04/09/bots-in-the-twittersphere/</ext-link></mixed-citation></ref></ref-list></back></article>