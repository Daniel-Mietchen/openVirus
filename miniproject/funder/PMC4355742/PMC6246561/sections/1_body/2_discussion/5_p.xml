<?xml version="1.0" encoding="UTF-8"?>
<p id="Par21">The results presented here suggest that curbing social bots may be an effective strategy for mitigating the spread of low-credibility content, and that the bot score might provide a useful signal to prioritize accounts for further review. Progress in this direction may be accelerated through partnerships between social media platforms and academic research
 <sup>
  <xref ref-type="bibr" rid="CR11">11</xref>
 </sup>. For example, our lab and others are developing machine learning algorithms to detect social bots
 <sup>
  <xref ref-type="bibr" rid="CR10">10</xref>,
  <xref ref-type="bibr" rid="CR26">26</xref>,
  <xref ref-type="bibr" rid="CR27">27</xref>
 </sup>. The deployment of such tools is fraught with peril, however. While platforms have the right to enforce their terms of service, which forbid impersonation and deception, algorithms do make mistakes. Even a single false-positive error leading to the suspension of a legitimate account may foster valid concerns about censorship. This justifies current human-in-the-loop solutions which unfortunately do not scale with the volume of abuse that is enabled by software. It is therefore imperative to support research both on improved abuse detection algorithms and on countermeasures that take into account the complex interplay between the cognitive and technological factors that favor the spread of misinformation
 <sup>
  <xref ref-type="bibr" rid="CR37">37</xref>
 </sup>.
</p>
