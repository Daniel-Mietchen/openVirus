<?xml version="1.0" encoding="UTF-8"?>
<p>Although runtimes are inherent to model implementation and computer hardware, presenting the order of magnitude of runtimes and memory requirements could be useful for other researchers. Details on model performance and computational burden were usually lacking in our selection of full-text papers. In our total set of IBM papers, we found 2 examples on the computational burden of their IBM in C++ [
 <xref ref-type="bibr" rid="CR36">36</xref>, 
 <xref ref-type="bibr" rid="CR76">76</xref>]. An influenza simulation with FLUTE [
 <xref ref-type="bibr" rid="CR36">36</xref>] uses approximately 80 megabytes of memory per million simulated individuals. Simulating an epidemic in a population of 10 million people can take up to two hours (on a single processor on an Intel
 <sup>®;</sup> Core Duo T9400), but it may take only seconds if the virus is not highly transmissible or if there are effective interventions [
 <xref ref-type="bibr" rid="CR36">36</xref>]. With 750 - 1000 megabytes of memory required per million simulated individuals, FRED’s computational burden [
 <xref ref-type="bibr" rid="CR76">76</xref>] is about ten times larger. Simulations for the H1N1 pandemic in a population of 1 million people takes less than two minutes on a typical dual-core laptop computer (in 2013) but the runtime will vary depending on the number of individuals infected during the epidemic and depending on which optional features are activated. Unfortunately, computational performance is a significant aspect of a simulator’s usefulness. Investment in performance optimization is required to achieve the full potential of current high-performance workstations [
 <xref ref-type="bibr" rid="CR108">108</xref>]. This seems most feasible using open-source software, as it allows more researchers to contribute to optimization and to leverage on the existing - and ever expanding - IBM knowledge base, thus enabling a cyclic process of innovation and optimization.
</p>
