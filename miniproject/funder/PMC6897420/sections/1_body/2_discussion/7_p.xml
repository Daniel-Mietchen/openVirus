<?xml version="1.0" encoding="UTF-8"?>
<p>Formally measuring the quality of forecasts is challenging and the choice of metric can impact how models are constructed. Following the FluSight Challenge guidelines, we used a probabilistic measure of forecast accuracy, the modified log score, as our primary tool for evaluating forecast accuracy. We also assessed point estimate accuracy as a secondary measure (Fig A in 
 <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). It has been shown that the modified log score (i.e. multiple bins considered accurate) used by the CDC is not strictly proper and could incentivize forecasting teams to modify forecast outputs if their goal was only to achieve the highest score possible [
 <xref rid="pcbi.1007486.ref035" ref-type="bibr">35</xref>, 
 <xref rid="pcbi.1007486.ref036" ref-type="bibr">36</xref>]. Forecasts in the FluSight Network were not modified in such a way [
 <xref rid="pcbi.1007486.ref037" ref-type="bibr">37</xref>]. Most component forecasts were optimized for the proper log-score (i.e. single bins considered accurate) while the FluSight Network ensembles were optimized to the modified log score. By using single bin scoring rules to evaluate forecasts, practitioners could ensure that all forecasts were optimized with the same goal in mind. In the case of the FluSight Network forecasts, the CDC has prioritized accuracy in a probabilistic sense over point-estimate accuracy.
</p>
