<?xml version="1.0" encoding="UTF-8"?>
<p>Twenty-one individual component models were fit to historical data and used to make prospective forecasts during seven training seasons (2010/2011–2016/2017, Table A in 
 <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). Their forecast accuracy varied widely across region, season, and target. A detailed comparative analysis of component model forecast performance can be found elsewhere [
 <xref rid="pcbi.1007486.ref027" ref-type="bibr">27</xref>]; however, here we summarize a few key insights. A seasonal baseline model, whose forecasts for a particular target are based on data from previous seasons and do not update based on data from the current season, was used as a reference point for all component models. Over 50% of the individual component models out-performed the seasonal baseline model in forecasting 1-, 2-, and 3-week ahead incidence as well as season peak percentage and season peak week. However, season-to-season variability in relative forecast performance was large. For example, 10 component models had, in at least one season, better overall accuracy than the model with the best average performance across all seasons. To evaluate model accuracy, we followed CDC convention and used a metric that takes the geometric average of the probabilities assigned to a small range around the eventually observed values. This measure, which we refer to as “forecast score”, can be interpreted as the average probability a given forecast model assigned to the values deemed by the CDC to be accurate (see 
 <xref ref-type="sec" rid="sec008">Methods</xref>). As such, higher values, on a scale of 0 to 1, indicate more accurate models.
</p>
