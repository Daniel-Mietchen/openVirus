<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?properties open_access?><?subarticle pcbi.1007486.r001?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="iso-abbrev">PLoS Comput. Biol</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6897420</article-id><article-id pub-id-type="pmid">31756193</article-id><article-id pub-id-type="publisher-id">PCOMPBIOL-D-19-01044</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1007486</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Infectious Diseases</subject><subj-group><subject>Viral Diseases</subject><subj-group><subject>Influenza</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Statistical Methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical Methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Public and Occupational Health</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Earth Sciences</subject><subj-group><subject>Seasons</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Epidemiology</subject><subj-group><subject>Disease Surveillance</subject><subj-group><subject>Infectious Disease Surveillance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Infectious Diseases</subject><subj-group><subject>Infectious Disease Control</subject><subj-group><subject>Infectious Disease Surveillance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Infectious Diseases</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Network Analysis</subject></subj-group></subj-group></article-categories><title-group><article-title>Accuracy of real-time multi-model ensemble forecasts for seasonal influenza in the U.S.</article-title><alt-title alt-title-type="running-head">Accuracy of real-time multi-model ensemble forecasts for seasonal influenza in the U.S.</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3503-9899</contrib-id><name><surname>Reich</surname><given-names>Nicholas G.</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Project administration</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>McGowan</surname><given-names>Craig J.</given-names></name><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8349-3151</contrib-id><name><surname>Yamana</surname><given-names>Teresa K.</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7137-0728</contrib-id><name><surname>Tushar</surname><given-names>Abhinav</given-names></name><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff004"><sup>4</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4035-0243</contrib-id><name><surname>Ray</surname><given-names>Evan L.</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff005"><sup>5</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4681-091X</contrib-id><name><surname>Osthus</surname><given-names>Dave</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff006"><sup>6</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6248-9097</contrib-id><name><surname>Kandula</surname><given-names>Sasikiran</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Brooks</surname><given-names>Logan C.</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff007"><sup>7</sup></xref></contrib><contrib contrib-type="author"><name><surname>Crawford-Crudell</surname><given-names>Willow</given-names></name><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff008"><sup>8</sup></xref></contrib><contrib contrib-type="author"><name><surname>Gibson</surname><given-names>Graham Casey</given-names></name><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Moore</surname><given-names>Evan</given-names></name><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Silva</surname><given-names>Rebecca</given-names></name><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff009"><sup>9</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5108-8311</contrib-id><name><surname>Biggerstaff</surname><given-names>Matthew</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Johansson</surname><given-names>Michael A.</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff010"><sup>10</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3274-5862</contrib-id><name><surname>Rosenfeld</surname><given-names>Roni</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff011"><sup>11</sup></xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7216-7809</contrib-id><name><surname>Shaman</surname><given-names>Jeffrey</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib></contrib-group><aff id="aff001">
<label>1</label>
<addr-line>Department of Biostatistics and Epidemiology, University of Massachusetts-Amherst, Amherst, Massachusetts, United States of America</addr-line>
</aff><aff id="aff002">
<label>2</label>
<addr-line>Influenza Division, Centers for Disease Control and Prevention, Atlanta, Georgia, United States of America</addr-line>
</aff><aff id="aff003">
<label>3</label>
<addr-line>Department of Environmental Health Sciences, Columbia University, New York, New York, United States of America</addr-line>
</aff><aff id="aff004">
<label>4</label>
<addr-line>School of Computer Science, University of Massachusetts-Amherst, Amherst, Massachusetts, United States of America</addr-line>
</aff><aff id="aff005">
<label>5</label>
<addr-line>Department of Mathematics and Statistics, Mount Holyoke College, South Hadley, Massachusetts, United States of America</addr-line>
</aff><aff id="aff006">
<label>6</label>
<addr-line>Statistical Sciences Group, Los Alamos National Laboratory, Los Alamos, New Mexico, United States of America</addr-line>
</aff><aff id="aff007">
<label>7</label>
<addr-line>Computer Science Department, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America</addr-line>
</aff><aff id="aff008">
<label>8</label>
<addr-line>Department of Mathematics and Statistics, Smith College, Northampton, Massachusetts, United States of America</addr-line>
</aff><aff id="aff009">
<label>9</label>
<addr-line>Department of Mathematics and Statistics, Amherst College, Amherst, Massachusetts, United States of America</addr-line>
</aff><aff id="aff010">
<label>10</label>
<addr-line>Division of Vector-Borne Diseases, Centers for Disease Control and Prevention, San Juan, Puerto Rico, United States of America</addr-line>
</aff><aff id="aff011">
<label>11</label>
<addr-line>Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Pitzer</surname><given-names>Virginia E.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>Yale School of Public Health, UNITED STATES</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p>I have read the journal&#x02019;s policy and the authors of this manuscript have the following competing interests: JS and Columbia University disclose partial ownership of SK Analytics.</p></fn><corresp id="cor001">* E-mail: <email>nick@umass.edu</email></corresp></author-notes><pub-date pub-type="collection"><month>11</month><year>2019</year></pub-date><pub-date pub-type="epub"><day>22</day><month>11</month><year>2019</year></pub-date><volume>15</volume><issue>11</issue><elocation-id>e1007486</elocation-id><history><date date-type="received"><day>25</day><month>6</month><year>2019</year></date><date date-type="accepted"><day>14</day><month>10</month><year>2019</year></date></history><permissions><license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/"><license-p>This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0</ext-link> public domain dedication.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pcbi.1007486.pdf"/><abstract><p>Seasonal influenza results in substantial annual morbidity and mortality in the United States and worldwide. Accurate forecasts of key features of influenza epidemics, such as the timing and severity of the peak incidence in a given season, can inform public health response to outbreaks. As part of ongoing efforts to incorporate data and advanced analytical methods into public health decision-making, the United States Centers for Disease Control and Prevention (CDC) has organized seasonal influenza forecasting challenges since the 2013/2014 season. In the 2017/2018 season, 22 teams participated. A subset of four teams created a research consortium called the FluSight Network in early 2017. During the 2017/2018 season they worked together to produce a collaborative multi-model ensemble that combined 21 separate component models into a single model using a machine learning technique called stacking. This approach creates a weighted average of predictive densities where the weight for each component is determined by maximizing overall ensemble accuracy over past seasons. In the 2017/2018 influenza season, one of the largest seasonal outbreaks in the last 15 years, this multi-model ensemble performed better on average than all individual component models and placed second overall in the CDC challenge. It also outperformed the baseline multi-model ensemble created by the CDC that took a simple average of all models submitted to the forecasting challenge. This project shows that collaborative efforts between research teams to develop ensemble forecasting approaches can bring measurable improvements in forecast accuracy and important reductions in the variability of performance from year to year. Efforts such as this, that emphasize real-time testing and evaluation of forecasting models and facilitate the close collaboration between public health officials and modeling researchers, are essential to improving our understanding of how best to use forecasts to improve public health response to seasonal and emerging epidemic threats.</p></abstract><abstract abstract-type="summary"><title>Author summary</title><p>Seasonal influenza outbreaks cause millions of infections and tens of thousands of deaths in the United States each year. Forecasting the track of an influenza season can help public health officials, business leaders, and the general public decide how to respond to an ongoing or emerging outbreak. Our team assembled over 20 unique forecasting models for seasonal influenza and combined them together into a single &#x0201c;ensemble&#x0201d; model. We made predictions of the 2017/2018 influenza season, each week sending real-time forecasts to the US Centers for Disease Control and Prevention (CDC). In the 2017/2018 influenza season, one of the largest seasonal outbreaks in the last 15 years, our ensemble model performed better on average than all individual forecast models in the ensemble. Based on results from this study, the CDC used forecasts from our ensemble model in public communication and internal reports in the subsequent 2018/2019 influenza season.</p></abstract><funding-group><award-group id="award001"><funding-source><institution>National Institute of General Medical Sciences (US)</institution></funding-source><award-id>R35GM119582</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3503-9899</contrib-id><name><surname>Reich</surname><given-names>Nicholas G.</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution>Defense Advanced Research Projects Agency (US)</institution></funding-source><award-id>YFA16 D16AP00144</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3503-9899</contrib-id><name><surname>Reich</surname><given-names>Nicholas G.</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000774</institution-id><institution>Defense Threat Reduction Agency</institution></institution-wrap></funding-source><award-id>HDTRA1-18-C-0008</award-id></award-group><award-group id="award004"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000057</institution-id><institution>National Institute of General Medical Sciences</institution></institution-wrap></funding-source><award-id>5U54GM088491</award-id></award-group><award-group id="award005"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>0946825</award-id></award-group><award-group id="award006"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000057</institution-id><institution>National Institute of General Medical Sciences</institution></institution-wrap></funding-source><award-id>GM110748</award-id></award-group><funding-statement>NGR, AT, GCG, and EM were funded by National Institutes of Health (grant number R35GM119582). NGR, AT, and ELR were funded by the Defense Advanced Research Projects Agency (YFA16 D16AP00144). LB and RR were funded by grants from the Defense Threat Reduction Agency (Contract No. HDTRA1-18-C-0008) and the National Institutes of Health (grant number 5U54GM088491). LB was funded by the National Science Foundation (grant numbers 0946825, DGE-1252522, and DGE- 1745016), and a gift from Uptake Technologies. TKY, SK, and JS were funded by National Institutes of Health (grant number GM110748). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="6"/><table-count count="0"/><page-count count="19"/></counts><custom-meta-group><custom-meta><meta-name>PLOS Publication Stage</meta-name><meta-value>vor-update-to-uncorrected-proof</meta-value></custom-meta><custom-meta><meta-name>Publication Update</meta-name><meta-value>2019-12-06</meta-value></custom-meta><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All data from this project are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/FluSightNetwork/cdc-flusight-ensemble">https://github.com/FluSightNetwork/cdc-flusight-ensemble</ext-link>), with a permanent repository stored on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1255023">https://doi.org/10.5281/zenodo.1255023</ext-link>).</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All data from this project are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/FluSightNetwork/cdc-flusight-ensemble">https://github.com/FluSightNetwork/cdc-flusight-ensemble</ext-link>), with a permanent repository stored on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1255023">https://doi.org/10.5281/zenodo.1255023</ext-link>).</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Seasonal influenza results in a substantial annual public health burden in the United States and worldwide. The United States Centers for Disease Control and Prevention (CDC) estimates there were 48.8 million cases of influenza, 959,000 influenza-related hospitalizations, and nearly 80,000 influenza-related deaths in the U.S. from October 2017 through May 2018, making the 2017/2018 season one of the largest on record [<xref rid="pcbi.1007486.ref001" ref-type="bibr">1</xref>]. The CDC utilizes a variety of surveillance methods to assess the severity of an influenza season, including monitoring outpatient visits for influenza-like illness (ILI), influenza-related hospitalizations, and virologic testing [<xref rid="pcbi.1007486.ref002" ref-type="bibr">2</xref>]. However, like all surveillance systems, these records describe only a sample of events that have already taken place, and provide limited indication of the future timing or severity of the epidemic, which can vary substantially from season to season [<xref rid="pcbi.1007486.ref003" ref-type="bibr">3</xref>]. Forecasts of an influenza season offer the possibility of providing actionable information on future influenza activity that can be used to improve public health response. Recent years have seen a substantial increase of peer-reviewed research on predicting seasonal influenza [<xref rid="pcbi.1007486.ref004" ref-type="bibr">4</xref>&#x02013;<xref rid="pcbi.1007486.ref011" ref-type="bibr">11</xref>].</p><p>Multi-model ensembles, i.e. models that combine predictions from multiple different component models, have long been seen as having both theoretical and practical advantages over any single model [<xref rid="pcbi.1007486.ref012" ref-type="bibr">12</xref>&#x02013;<xref rid="pcbi.1007486.ref015" ref-type="bibr">15</xref>]. First, it allows for a single forecast to incorporate signals from different data sources and models that may highlight different features of a system. Second, combining signals from models with different biases may allow those biases to offset and result in an ensemble that is more accurate and has lower variance than the individual ensemble components. Weather and climate models have utilized multi-model ensemble systems for these very purposes [<xref rid="pcbi.1007486.ref016" ref-type="bibr">16</xref>&#x02013;<xref rid="pcbi.1007486.ref019" ref-type="bibr">19</xref>], and recent work has extended ensemble forecasting to infectious diseases, including influenza, dengue fever, lymphatic filariasis, and Ebola hemorrhagic fever [<xref rid="pcbi.1007486.ref020" ref-type="bibr">20</xref>&#x02013;<xref rid="pcbi.1007486.ref023" ref-type="bibr">23</xref>]. Throughout the manuscript, we will use the term ensemble to refer generally to these multi-model ensemble approaches.</p><p>Since the 2013/2014 influenza season, the CDC has run an annual prospective influenza forecasting competition, known as the FluSight challenge, in collaboration with outside researchers. The challenges have provided a venue for close interaction and collaboration between government public health officials and academic and private-sector researchers. Among other government-sponsored infectious disease forecasting competitions in recent years, [<xref rid="pcbi.1007486.ref024" ref-type="bibr">24</xref>, <xref rid="pcbi.1007486.ref025" ref-type="bibr">25</xref>] this challenge has been unique in its prospective orientation over multiple outbreak seasons. Each week from early November through mid-May, participating teams submit probabalistic forecasts for various influenza-related targets of interest. During the 2015/2016 and 2016/2017 FluSight challenges, analysts at the CDC built a simple ensemble model by taking an unweighted average of all submitted models. This model has been one of the top performing models each season [<xref rid="pcbi.1007486.ref026" ref-type="bibr">26</xref>].</p><p>The FluSight challenge has been designed and retooled over the years with an eye towards maximizing the public health utility and integration of forecasts with real-time public health decision making. All forecast targets are derived from the weighted percentage of outpatient visits for influenza-like illness (wILI) collected through the U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet), weighted by state populations (<xref ref-type="fig" rid="pcbi.1007486.g001">Fig 1B and 1C</xref>). ILI is one of the most frequently used indicators of influenza activity in epidemiological surveillance. Weekly submissions to the FluSight challenge contain probabilistic and point forecasts for seven targets in each of 11 regions in the U.S. (national-level plus the 10 Health and Human Services (HHS) regions, <xref ref-type="fig" rid="pcbi.1007486.g001">Fig 1A</xref>). There are two classes of targets: &#x0201c;week-ahead&#x0201d; and &#x0201c;seasonal&#x0201d;. &#x0201c;Week ahead&#x0201d; targets refer to four short-term weekly targets (ILI percentages 1, 2, 3 and 4 weeks in the future) that are different for each week of the season. &#x0201c;Seasonal&#x0201d; targets refer to quantities (outbreak onset week, outbreak peak week, and outbreak peak intensity) that represent a single outcome observed for a region in a season (see <xref ref-type="sec" rid="sec008">Methods</xref>).</p><fig id="pcbi.1007486.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007486.g001</object-id><label>Fig 1</label><caption><title>Overview of region-level influenza surveillance data in the US.</title><p>(A) Map of the 10 U.S. Health and Human Services regions. Influenza forecasts are made at this geographic scale. (B) Publicly available wILI data from the CDC website for the national level. The y-axis shows the estimated percentage of doctor&#x02019;s office visits in which a patient presents with influenza-like illness for each week from September 2010 through July 2018. The dashed vertical line indicates the separation of the data used by the models presented here for the training (retrospective) and testing (prospective) phases of analysis. (C) Publicly available wILI data for National level and each of the 10 HHS regions. Darker colors indicate higher wILI.</p></caption><graphic xlink:href="pcbi.1007486.g001"/></fig><p>In March 2017, influenza forecasters who had worked with CDC in the past were invited to join in establishing the FluSight Network. This research consortium worked collaboratively throughout 2017 and 2018 to build and implement a real-time multi-model ensemble with performance-based model weights. A central goal of the FluSight Network was to demonstrate the benefit of performance-based weights in a real-time, multi-team ensemble setting by outperforming the &#x0201c;simple average&#x0201d; ensemble that CDC uses to inform decision making and situational awareness during the annual influenza season. The CDC used this project to evaluate in real-time the feasibility and accuracy of creating an ensemble forecast based on past performance. Based on the forecast accuracy shown in this experiment, the CDC decided to adopt the approach described here as its main forecasting approach for the 2018/2019 influenza season.</p><p>In this paper, we describe the development of this collaborative multi-model ensemble and present forecasting results from seven retrospective seasons and one prospective season. The FluSight Network assembled 21 component forecasting systems to build multi-model ensembles for seasonal influenza outbreaks (Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). These components encompassed a variety of different modeling philosophies, including Bayesian hierarchical models, mechanistic models of infectious disease transmission, statistical learning methodologies, and classical statistical models for time-series data. We show that using multi-model ensembles informed by past performance consistently improved forecast accuracy over using any single model and over multi-model ensembles that do not take past performance into account. Given the timing of this experiment, during a particularly severe influenza season, this work also provides the first evidence from a real-time forecasting study that performance-based weights can improve ensemble forecast accuracy during a high severity infectious disease outbreak. This research is an important example of collaboration between government and academic public health experts, setting a precedent and prototype for real-time collaboration in future outbreaks, such as a global influenza pandemic.</p></sec><sec sec-type="results" id="sec002"><title>Results</title><sec id="sec003"><title>Summary of ensemble components</title><p>Twenty-one individual component models were fit to historical data and used to make prospective forecasts during seven training seasons (2010/2011&#x02013;2016/2017, Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). Their forecast accuracy varied widely across region, season, and target. A detailed comparative analysis of component model forecast performance can be found elsewhere [<xref rid="pcbi.1007486.ref027" ref-type="bibr">27</xref>]; however, here we summarize a few key insights. A seasonal baseline model, whose forecasts for a particular target are based on data from previous seasons and do not update based on data from the current season, was used as a reference point for all component models. Over 50% of the individual component models out-performed the seasonal baseline model in forecasting 1-, 2-, and 3-week ahead incidence as well as season peak percentage and season peak week. However, season-to-season variability in relative forecast performance was large. For example, 10 component models had, in at least one season, better overall accuracy than the model with the best average performance across all seasons. To evaluate model accuracy, we followed CDC convention and used a metric that takes the geometric average of the probabilities assigned to a small range around the eventually observed values. This measure, which we refer to as &#x0201c;forecast score&#x0201d;, can be interpreted as the average probability a given forecast model assigned to the values deemed by the CDC to be accurate (see <xref ref-type="sec" rid="sec008">Methods</xref>). As such, higher values, on a scale of 0 to 1, indicate more accurate models.</p></sec><sec id="sec004"><title>Choice of ensemble model based on cross-validation</title><p>We pre-specified five candidate ensemble approaches prior to any systematic evaluation of ensemble component performance in previous seasons and prior to the 2017/2018 season (Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>) [<xref rid="pcbi.1007486.ref028" ref-type="bibr">28</xref>]. The pre-specified ensemble approaches all relied on taking weighted averages of the component models, including two seasonal baseline components, using a predictive density stacking approach (see <xref ref-type="sec" rid="sec008">Methods</xref>). They ranged in complexity from simple (every component is assigned a single weight) to more complex (components have different weights depending on the target and region being forecasted, see <xref ref-type="sec" rid="sec008">Methods</xref>). The FSNetwork Target-Type Weights (<monospace>FSNetwork-TTW</monospace>) ensemble model, a medium-complexity approach, outperformed all other multi-model ensembles and components in the training phase by a slim margin (<xref ref-type="fig" rid="pcbi.1007486.g002">Fig 2</xref>). The <monospace>FSNetwork-TTW</monospace> model built weighted model averages using 40 estimated weights, one for each model and target-type (week-ahead and seasonal) combination (<xref ref-type="fig" rid="pcbi.1007486.g003">Fig 3</xref>). In the training period, consisting of the seven influenza seasons prior to 2017/2018, this model achieved a leave-one-season-out cross-validated average forecast score of 0.406, compared with the FSNetwork Target Weights (<monospace>FSNetwork-TW</monospace>) model with a score of 0.404, the FSNetwork Constant Weights (<monospace>FSNetwork-CW</monospace>) model with a score of 0.400, and the FSNetwork Target-Region Weights (<monospace>FSNetwork-TRW</monospace>) model with a score of 0.400 (<xref ref-type="fig" rid="pcbi.1007486.g004">Fig 4</xref>). Prior to the start of the 2017-18 FluSight Challenge, we chose the target-type weights model as the model that would be submitted in real-time to the CDC during the 2017/2018 season. This choice was based on the pre-specified criteria of the chosen model having the highest score of any approach in the cross-validated training phase [<xref rid="pcbi.1007486.ref028" ref-type="bibr">28</xref>].</p><fig id="pcbi.1007486.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007486.g002</object-id><label>Fig 2</label><caption><title>Training phase performance of the five pre-specified multi-model ensembles.</title><p>The five ensembles tested were Equal Weights (EW), Constant Weights (CW), Target-Type Weights (TTW), Target Weights (TW), and Target-Region Weights (TRW). The models are sorted from simplest (left) to most complex (right), with the number of estimated weights (see <xref ref-type="sec" rid="sec008">Methods</xref>) for each model shown at the top. Each point represents the average forecast score for a particular season, with overall average across all seasons shown by the X.</p></caption><graphic xlink:href="pcbi.1007486.g002"/></fig><fig id="pcbi.1007486.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007486.g003</object-id><label>Fig 3</label><caption><title>Component model weights for the FluSight Network Target-Type Weights (<monospace>FSNetwork-TTW</monospace>) ensemble model in the 2017/2018 season.</title><p>Weights were estimated using cross-validated forecast performance in the 2010/2011 through the 2016/2017 seasons.</p></caption><graphic xlink:href="pcbi.1007486.g003"/></fig><fig id="pcbi.1007486.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007486.g004</object-id><label>Fig 4</label><caption><title>Overall test and training phase performance scores for selected models.</title><p>Displayed scores are averaged across targets, regions, and weeks, and plotted separately for selected models. Models shown include the <monospace>FSNetwork-TTW</monospace> model, the top performing model from each team during the training phase and, for the last two training seasons and the test season, the unweighted average of all FluSight models received by CDC. Model ranks within each row are indicated by color of each cell (darker colors indicates higher rank and more accurate forecasts) and the forecast score (rounded to two decimal places) is printed in each cell. Note that a component&#x02019;s standalone accuracy does not necessarily correlate to its contribution to the overall ensemble accuracy. See discussion in the Ensemble Components subsection of the Methods.</p></caption><graphic xlink:href="pcbi.1007486.g004"/></fig><p>Using the results from the training period, we estimated weights for the chosen <monospace>FSNetwork-TTW</monospace> ensemble model that would be used for the 2017/2018 real-time forecasting. The <monospace>FSNetwork-TTW</monospace> model assigned non-negligible weight (greater than 0.001) to 8 models for week-ahead targets and 6 models for seasonal targets (<xref ref-type="fig" rid="pcbi.1007486.g003">Fig 3</xref>). For week-ahead targets, the highest non-zero weight (0.42) was given to the <monospace>Delphi-DeltaDensity1</monospace> model. For seasonal targets, the highest weight (0.26) was given to the <monospace>LANL-DBM</monospace> model. In the weights for the seasonal targets, six models shared over 99.9% of the weight, with none of the six having less than 0.11 weight. All four research teams had at least one model with non-negligible weight in the chosen model.</p></sec><sec id="sec005"><title>Summary of ensemble real-time performance in 2017/2018 season</title><p>The 2017/2018 influenza season in the U.S. exhibited features that were unlike that of any season in the past 15 years (<xref ref-type="fig" rid="pcbi.1007486.g001">Fig 1B and 1C</xref>). As measured by wILI percentages at the national level, the 2017/2018 season was on par with the other two highest peaks on record since 1997: the 2003/2004 season and the 2009 H1N1 pandemic. In some regions, for example HHS Region 2 (New York and New Jersey) and HHS Region 4 (southeastern states), the peak wILI for the 2017/2018 season was more than 20% higher than previously observed peaks. Because all forecasting models rely, to some extent, on future trends mimicking observed patterns in the past, the anomalous dynamics in 2017/2018 posed a challenging &#x0201c;test season&#x0201d; for all models, including the new ensembles.</p><p>In spite of these unusual dynamics, the chosen <monospace>FSNetwork-TTW</monospace> ensemble showed the best performance among all component and ensemble models during the 2017/2018 season. In particular, we selected the single best component model from each team in the training phase stage and the <monospace>FluSight-unweighted_avg</monospace> model (the unweighted average of all models submitted to the CDC) to compare with the <monospace>FSNetwork-TTW</monospace> model (<xref ref-type="fig" rid="pcbi.1007486.g004">Fig 4</xref>). The results from 2017/2018 were consistent with and confirmed conclusions drawn from the training period, where the <monospace>FSNetwork-TTW</monospace> model outperformed all other ensemble models and components. The <monospace>FSNetwork-TTW</monospace> model had the highest average score in the training period (0.406) as well as the highest average score in the 2017/2018 test season (0.337). This strong and consistent performance by the chosen <monospace>FSNetwork-TTW</monospace> ensemble model is noteworthy given that our team identified this model prospectively, before the season began, essentially wagering that this ensemble model would have the best performance of any model in the 2017/2018 season, which it did. The <monospace>FSNetwork-TTW</monospace> model consistently outperformed a simpler ensemble model and the seasonal average model across all weeks of the 2017/2018 season (Fig F in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>).</p><p>The <monospace>FSNetwork-TTW</monospace> model showed a higher performance in both training and testing phase than the CDC baseline ensemble model, <monospace>FluSight-unweighted_avg</monospace>. This multi-model ensemble contained forecasts from 28 models submitted to the FluSight competition in 2017/2018. While some of these 28 models submitted to the CDC were or contained versions of the 21 models in our performance-based FluSight Network multi-model ensemble, over two-thirds of the models submitted to the CDC were not represented in the FluSight Network components. In 2017/2018, the <monospace>FSNetwork-TTW</monospace> model earned an average forecast score of 0.337 while the <monospace>FluSight-unweighted_avg</monospace> model earned an average forecast score of 0.321 (<xref ref-type="fig" rid="pcbi.1007486.g004">Fig 4</xref>).</p><p>In the 2017/2018 season, the top models from each contributing research team showed considerable variation in performance across the different prediction targets and regions (<xref ref-type="fig" rid="pcbi.1007486.g005">Fig 5</xref>). However, the <monospace>FSNetwork-TTW</monospace> model showed lower variability in performance than other methods. Across all 77 pairs of targets and regions, the <monospace>FSNetwork-TTW</monospace> model was the only one of the six selected models that never had the lowest forecast score. Additionally, it only had the second lowest score twice. While our ensemble model did not always have the best score in each target-region pair, its consistency and low variability across all combinations secured it the top average score.</p><fig id="pcbi.1007486.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007486.g005</object-id><label>Fig 5</label><caption><title>Average forecast scores and ranks by target and region for 2017/2018.</title><p>Models shown include the <monospace>FSNetwork-TTW</monospace> model, the top performing model from each team during the training phase and the unweighted average of all FluSight models received by CDC. Color indicates model rank in the 2017/2018 season (darker colors indicates higher rank and more accurate forecasts) and the forecast score (rounded to two decimal places) is printed in each cell. Regions are sorted with the most predictable region overall (i.e. highest forecast scores) at the top.</p></caption><graphic xlink:href="pcbi.1007486.g005"/></fig><p>Despite being optimized for high forecast score values, the <monospace>FSNetwork-TTW</monospace> showed robust performance in the 2017/2018 season across other performance metrics that measure forecast calibration and accuracy. Overall, the <monospace>FSNetwork-TTW</monospace> model ranked second among selected models in both RMSE and average bias, behind the <monospace>LANL-DBM</monospace> model (Fig A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). For example, during the scoring period of interest across all regions in the 2017/2018 season, the <monospace>FSNetwork-TTW</monospace> model&#x02019;s point estimates for season onset were on average less than half a week after the true value (average bias = 0.38 week) and for 1-week ahead ILI the estimates were underestimated by less than one-quarter of a percentage point (average bias = -0.23 ILI%).</p><p>According to the probability integral transform metric [<xref rid="pcbi.1007486.ref029" ref-type="bibr">29</xref>, <xref rid="pcbi.1007486.ref030" ref-type="bibr">30</xref>], the <monospace>FSNetwork-TTW</monospace> model was well-calibrated for all four week-ahead targets (Fig B in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). It was slightly less well-calibrated for peak performance, and showed indications of having too narrow predictive distributions over the 2017/2018 season. Over the entire training period prior to the 2017/2018 season, the <monospace>FSNetwork-TTW</monospace> model calibration results suggested that in general the model was a bit conservative, with often a too wide predictive distribution (Fig C in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>).</p><p>A new ensemble using the same components but taking into account their performance in the 2017/2018 season would have different weights. Components that received substantial weight in the original ensemble but did particularly poorly in the 2017/2018 season saw the largest drop in weight (Fig D in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). Overall, three components were added to the list of six existing components that received more than 0.001 weight for seasonal targets: <monospace>CU-EAKFC_SEIRS</monospace>, <monospace>CU-EKF_SEIRS</monospace>, and <monospace>ReichLab-SARIMA2</monospace>. One component (<monospace>ReichLab-SARIMA2</monospace>) was added to the list of eight existing components that received more than 0.001 weight for week-ahead targets.</p></sec><sec id="sec006"><title>Ensemble accuracy for peak forecasts</title><p>Forecast accuracy around the time of peak incidence is an important indicator of how useful a given model can be in real-time for public health decision-makers. To this end, we evaluated the scores of the <monospace>FSNetwork-TTW</monospace> ensemble model in each region during the 13 weeks centered around the peak week (<xref ref-type="fig" rid="pcbi.1007486.g006">Fig 6</xref>). Forecast scores of the peak percentage 6, 5, and 4 weeks before the peak week were lower than in past seasons, assigning on average 0.05, 0.06, and 0.05 probability to the eventually observed value, respectively. However, at and after the peak week this probability was over 0.70, quite a bit higher than average accuracy in past seasons.</p><fig id="pcbi.1007486.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1007486.g006</object-id><label>Fig 6</label><caption><title>Forecast score for the <monospace>FSNetwork-TTW</monospace> model in 2017/2018 by week relative to peak.</title><p>Scores for the two peak targets in each region were aligned to summarize performance relative to the peak week. On the x-axis, zero indicates the peak week and positive values represent weeks after the peak week. The black line indicates the overall geometric average across all regions. The grey band represents the geometric average across all regions and all seasons prior to 2017/2018.</p></caption><graphic xlink:href="pcbi.1007486.g006"/></fig><p>Similarly, for peak week the average forecast scores improved as the peak week approached. With the exception of a large dip in accuracy in HHS Region 7 just after the peak occured (due to revisions to observed wILI data in the weeks surrounding peak), the forecast scores for peak week tended to be high in the weeks following peak. The average score in more than half of the regions was greater than 0.75 for all weeks after peak.</p></sec></sec><sec sec-type="conclusions" id="sec007"><title>Discussion</title><p>Multi-model ensembles hold promise for giving decision makers the ability to use &#x0201c;one answer&#x0201d; that combines the strengths of many different modeling approaches while mitigating their weaknesses. This work presents the first attempt to systematically combine infectious disease forecasts from multiple research groups in real-time using an approach that factors in the past performance of each component method. Of the 29 models submitted to the CDC in 2017/2018 as part of their annual FluSight forecasting challenge, this ensemble was the second-highest scoring model overall. The top scoring model was an ensemble of human judgment forecasts [<xref rid="pcbi.1007486.ref031" ref-type="bibr">31</xref>].</p><p>By working across disciplines and research groups and by incorporating experts from government, academia and industry, this collaborative effort showed success in bringing measurable improvements in forecast accuracy and reductions in variability. We therefore are moving substantially closer to forecasts that can and should be used to complement routine, ongoing public health surveillance of infectious diseases. In the 2018/2019 influenza season, based on results from this study, the CDC used forecasts from the FluSight Network ensemble model in internal and external communication and planning reports.</p><p>Even in a very unusual influenza season, the multi-model ensemble approach presented here outperformed all components overall and did not see a large reduction in overall performance compared to performance during the training seasons. This bodes well for the long-term robustness of models such as this one, compared to single components that show higher variability in performance across specific years, regions, and targets. During the training and test phases, the weighted ensemble approaches outperformed two equal weight ensembles: one constructed based on FluSight Network models presented here (Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>) and one constructed by the CDC using a wider array of models [<xref rid="pcbi.1007486.ref026" ref-type="bibr">26</xref>]. This clearly illustrates the value of incorporating information on prior performance of component models when constructing a multi-model ensemble.</p><p>As shown by the FluSight Network Target-Type Weights component weighting structure presented above (<xref ref-type="fig" rid="pcbi.1007486.g003">Fig 3</xref>), no one model was ever used by the multi-model ensemble as the best answer and the ensemble instead relied on a combination of components to optimize performance. The ensemble assigned non-negligible weight to 11 of the 21 models available and the updated ensemble weights including the 2017/2018 performance would have added one model to that list. This work highlights the importance of incorporating different models into a single forecast based on past performance. Moving forward, it will be vital to develop and sustain a robust ecosystem of forecasting models for infectious disease that represent many different methods, data sources, and model structures. In particular, we see opportunities in developing incorporating pathogen-specific data into models [<xref rid="pcbi.1007486.ref032" ref-type="bibr">32</xref>] and using more spatially structured and multi-scale approaches [<xref rid="pcbi.1007486.ref011" ref-type="bibr">11</xref>, <xref rid="pcbi.1007486.ref033" ref-type="bibr">33</xref>]. The inclusion of mutiple forecasting approaches and use of past performance in determining an ensemble structure reduces the risk of relying on a single probabilistic forecast and therefore strengthens the case for incorporating forecasts into real-time decision-making.</p><p>While the multi-model ensemble approach described here works well for seasonal pathogens with multiple seasons of retrospective data available, it would be more limited in an emerging pandemic scenario. In these settings, there may not be any historical data on how models have performed nor reliable real-time data to train on. However, adaptive weighting approaches that dynamically update the weights over the course of a season or epidemic could remove the requirement that all models have a substantial track-record of performance. Preliminary work on adaptive weighting has shown some promise, though such approaches still rely on accurately reported real-time data [<xref rid="pcbi.1007486.ref034" ref-type="bibr">34</xref>]. Furthermore, a simple average of forecasts remains available in such situations and, as illustrated by the relatively strong performance of the FluSight Network Equal Weights model and the CDC&#x02019;s unweighted FluSight ensemble, can still offer advantages over individual models.</p><p>One risk of complex ensemble approaches is that they may be &#x0201c;overfit&#x0201d; to the data, resulting in models that place too much emphasis on one approach in a particular scenario or setting. This is a particular concern in applications such as this one, where the number of observations is fairly limited (hundreds to thousands of observations instead of hundreds of thousands). Against this backdrop, the relative simplicity of the FluSight Network Target-Type weights model is a strength, as there is less danger of these models being overfit to the data. Additionally, approaches that use regularization or penalization to reduce the number of effective parameters estimated by a particular model have been shown to have some practical utility in similar settings and may also have a role to play in future ensembles for infectious disease forecasting [<xref rid="pcbi.1007486.ref023" ref-type="bibr">23</xref>].</p><p>Formally measuring the quality of forecasts is challenging and the choice of metric can impact how models are constructed. Following the FluSight Challenge guidelines, we used a probabilistic measure of forecast accuracy, the modified log score, as our primary tool for evaluating forecast accuracy. We also assessed point estimate accuracy as a secondary measure (Fig A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). It has been shown that the modified log score (i.e. multiple bins considered accurate) used by the CDC is not strictly proper and could incentivize forecasting teams to modify forecast outputs if their goal was only to achieve the highest score possible [<xref rid="pcbi.1007486.ref035" ref-type="bibr">35</xref>, <xref rid="pcbi.1007486.ref036" ref-type="bibr">36</xref>]. Forecasts in the FluSight Network were not modified in such a way [<xref rid="pcbi.1007486.ref037" ref-type="bibr">37</xref>]. Most component forecasts were optimized for the proper log-score (i.e. single bins considered accurate) while the FluSight Network ensembles were optimized to the modified log score. By using single bin scoring rules to evaluate forecasts, practitioners could ensure that all forecasts were optimized with the same goal in mind. In the case of the FluSight Network forecasts, the CDC has prioritized accuracy in a probabilistic sense over point-estimate accuracy.</p><p>Even though we have shown the value in building collaborations between research teams to develop ensemble forecasts, these efforts largely rely on bespoke technological solutions. This challenge is not specific to infectious disease forecasting, but covers many areas of quantitative science. For this project, we built a highly customized solution that relied on GitHub, Travis Continuous Integration server, unix scripts, and model code in R, python, and MatLab. In all, the seven seasons of training data consisted of about 95MB and over 1.5m rows of data per model and about 2GB of forecast data for all models combined. The real-time forecasts for the 2017/2018 season added about 300MB of data. The framework developed by CDC directly facilitated this work by identifying targets, establishing common formats, and establishing a space for this collaboration. To build on this success and move ensemble infectious disease forecasting into a more generalizable, operational phase, technological advancements are necessary to both standardize data sources, model structures, and forecast formats as well as develop modeling tools that can facilitate the development and implementation of component and ensemble models.</p><p>With the promise of new, real-time data sources and continued methodological innovation for both component models and multi-model ensemble approaches, there is good reason to believe that infectious disease forecasting will continue to mature and improve in upcoming years. As modeling efforts become more commonplace in the support of public health decision-making worldwide, it will be critical to develop infrastructure so that multiple models can more easily be compared and combined. This will facilitate reproducibility and archiving of single-model forecasts, the creation of multi-model ensemble forecasts, and the communication of the forecasts and their uncertainty to decision-makers and the general public. Efforts such as this, that emphasize real-time testing and evaluation of forecasting models and facilitate close collaboration between public health officials and modeling researchers, are critical to improving our understanding of how best to use forecasts to improve public health response to seasonal and emerging epidemic threats.</p></sec><sec sec-type="materials|methods" id="sec008"><title>Materials and methods</title><sec id="sec009"><title>Influenza data</title><p>Forecasting targets for the CDC FluSight challenge are based on the U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet). ILINet is a syndromic surveillance system that publishes the weekly percentage of outpatient visits due to influenza-like illness, weighted based on state populations (wILI) from a network of more than 2,800 providers. Estimates of wILI are reported weekly by the CDC&#x02019;s Influenza Division for the United States as a whole as well as for each of the 10 Health and Human Services (HHS) regions. Reporting of &#x02018;current&#x02019; wILI is typically delayed by approximately one to two weeks from the calendar date of a doctor&#x02019;s office visit as data are collected and processed, and each weekly publication can also include revisions to prior reported values if new data become available. Larger revisions have been shown to be associated with decreased forecast accuracy [<xref rid="pcbi.1007486.ref027" ref-type="bibr">27</xref>]. For the US and each HHS Region, CDC publishes an annual baseline level of ILI activity based on off-season ILI levels [<xref rid="pcbi.1007486.ref002" ref-type="bibr">2</xref>].</p></sec><sec id="sec010"><title>Forecast targets and structure</title><p>As the goal was to submit our ensemble forecast in real-time to the CDC FluSight forecasting challenge, we adhered to guidelines and formats set forth by the challenge in determining forecast format. A season typically consists of forecast files generated weekly for 33 weeks, starting with epidemic week 43 (EW43) of one calendar year and ending with EW18 of the following year. Every week in a year is classified into an &#x0201c;MMWR week&#x0201d; (ranging from 1 to 52 or 53, depending on the year) using a standard definition established by the National Notifiable Diseases Surveillance System [<xref rid="pcbi.1007486.ref038" ref-type="bibr">38</xref>&#x02013;<xref rid="pcbi.1007486.ref040" ref-type="bibr">40</xref>]. Forecasts for the CDC FluSight challenge consist of seven targets: three seasonal targets and four short-term or &#x02018;week-ahead&#x02019; targets. The seasonal targets consist of season onset (defined as the first MMWR week where wILI is at or above baseline and remains above it for three consecutive weeks), season peak week (defined as the MMWR week of maximum wILI), and season peak percentage (defined as the maximum wILI value for the season). The short-term targets consist of forecasts for wILI values 1, 2, 3, and 4 weeks ahead of the most recently published data. With the two-week reporting delay in the publication of ILINet, these short-term forecasts are for the level of wILI occurring 1 week prior to the week the forecast is made, the current week, and the two weeks after the forecast is made. Forecasts are created for all targets for the US as a whole and for each of the 10 HHS Regions (<xref ref-type="fig" rid="pcbi.1007486.g001">Fig 1A&#x02013;1C</xref>).</p><p>For all targets, forecasts consist of probability distributions over bins of possible values for the target. For season onset and peak week, forecast bins consist of individual weeks within the influenza season, with an additional bin for onset week corresponding to a forecast of no onset. For short-term targets and peak intensity, forecast bins consist of levels of observed wILI rounded to the nearest 0.1% (the level of resolution for ILINet publicly reported by the CDC) up to 13%. Formally, the bins are defined as [0.00, 0.05), [0.05, 0.15), &#x02026;, [12.85, 12.95), [12.95, 100].</p><p>The CDC has developed a structured format for weekly influenza forecasts. All forecasts for this project used those data standards for all forecasts and this facilitated collaboration among the teams.</p></sec><sec id="sec011"><title>Forecast evaluation</title><p>Submitted forecasts were evaluated using the modified log score used by the CDC in their forecasting challenge, which provides a simultaneous measure of forecast accuracy and precision. The log score for a probabalistic forecast <italic>m</italic> is defined as log <italic>f</italic><sub><italic>m</italic></sub>(<italic>z</italic>*|<bold>x</bold>), where <italic>f</italic><sub><italic>m</italic></sub>(<italic>z</italic>|<bold>x</bold>) is the predicted density function from model <italic>m</italic> for some target <italic>Z</italic>, conditional on some data <bold>x</bold> and <italic>z</italic>* is the observed value of the target <italic>Z</italic>.</p><p>While a proper log score only evaluates the probability assigned to the exact observed value <italic>z</italic>*, the CDC uses a modified log score that classifies additional values as &#x0201c;accurate&#x0201d;. For predictions of season onset and peak week, probabilities assigned to the week before and after the observed week are included as correct, so the modified log score becomes <inline-formula id="pcbi.1007486.e001"><alternatives><graphic xlink:href="pcbi.1007486.e001.jpg" id="pcbi.1007486.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>f</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. For season peak percentage and the short-term forecasts, probabilities assigned to wILI values within 0.5 units of the observed values are included as correct, so the modified log score becomes <inline-formula id="pcbi.1007486.e002"><alternatives><graphic xlink:href="pcbi.1007486.e002.jpg" id="pcbi.1007486.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>-</mml:mo><mml:mo>.</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>f</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. In practice, and following CDC scoring convention, we truncate modified log scores to be no lower than -10. We refer to these modified log scores as simply log scores hereafter.</p><p>Individual log scores can be averaged across different combinations of forecast regions, target, weeks, or seasons. Each model <italic>m</italic> has an associated predictive density for each combination of region (<italic>r</italic>), target (<italic>t</italic>), season (<italic>s</italic>), and week (<italic>w</italic>). Each of these densities has an accompanying scalar log score, which could be represented as <inline-formula id="pcbi.1007486.e003"><alternatives><graphic xlink:href="pcbi.1007486.e003.jpg" id="pcbi.1007486.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. These individual log scores can be averaged across combinations of regions, targets, seasons, and weeks to compare model performance.</p><p>Following FluSight challenge convention, to focus model evaluation on periods of time that are more relevant for public health decision-making, only certain weeks were included when calculating the average log scores for each target. Forecasts of season onset were included for each region up to six weeks after the observed onset week within that region. Forecasts of peak week and peak intensity were included for all weeks in a region-season until the week in which the wILI measure dropped below the regional baseline level for the final time. Week-ahead forecasts for each region-season were included starting four weeks prior to the onset week through three weeks after the wILI goes below the regional baseline for the final time. All weeks were included for region-seasons that did not have high enough incidence to define a season onset week.</p><p>To enhance interpretability, we report exponentiated average log scores which are the geometric mean of probability a model assigned to the value(s) eventually deemed to be accurate. In this manuscript, we refer to these as &#x0201c;average forecast scores&#x0201d;. As an example, the average forecast score for model <italic>m</italic> in season <italic>s</italic> (as shown in <xref ref-type="fig" rid="pcbi.1007486.g002">Fig 2</xref>), is computed as
<disp-formula id="pcbi.1007486.e004"><alternatives><graphic xlink:href="pcbi.1007486.e004.jpg" id="pcbi.1007486.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x000b7;</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:munder><mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:munder><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula></p><p>As other forecasting efforts have used mean square error (MSE) or root mean square error (RMSE) of point predictions as an evaluation method, we additionally evaluated the prospective forecasts received during the 2017-2018 season using RMSE. The submitted point forecast was used to score each component, and a point forecast was generated for each FSNetwork model by taking the median of the predicted distribution. For each model <italic>m</italic>, we calculated <italic>RMSE</italic><sub><italic>m</italic>,<italic>t</italic></sub> for target <italic>t</italic>, averaging over all weeks <italic>w</italic> in the <italic>s</italic> = 2017/2018 season and all regions <italic>r</italic>, as <inline-formula id="pcbi.1007486.e005"><alternatives><graphic xlink:href="pcbi.1007486.e005.jpg" id="pcbi.1007486.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1007486.e006"><alternatives><graphic xlink:href="pcbi.1007486.e006.jpg" id="pcbi.1007486.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the point prediction of model <italic>m</italic> for observed value <inline-formula id="pcbi.1007486.e007"><alternatives><graphic xlink:href="pcbi.1007486.e007.jpg" id="pcbi.1007486.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>. Average bias is calculated as <inline-formula id="pcbi.1007486.e008"><alternatives><graphic xlink:href="pcbi.1007486.e008.jpg" id="pcbi.1007486.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mrow><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>.</p></sec><sec id="sec012"><title>Ensemble components</title><p>To provide training data for the ensemble, four teams submitted between 1 and 9 components each, for a total of 21 ensemble components. Teams submitted out-of-sample forecasts for the 2010/2011 through 2016/2017 influenza seasons. These models and their performance are evaluated in separate work [<xref rid="pcbi.1007486.ref027" ref-type="bibr">27</xref>]. Teams constructed their forecasts in a prospective fashion, using only data that were available at the time of the forecast. For some data sources (e.g., wILI prior to the 2014/2015 influenza season), data as they were published at the time were not available. In such cases, teams were still allowed to use those data sources while making best efforts to only use data available at the time forecasts would have been made.</p><p>For each influenza season, teams submitted weekly forecasts from epidemic week 40 (EW40) of the first year through EW20 of the following year, using standard CDC definitions for epidemic week [<xref rid="pcbi.1007486.ref038" ref-type="bibr">38</xref>&#x02013;<xref rid="pcbi.1007486.ref040" ref-type="bibr">40</xref>]. If a season contained EW53, forecasts were submitted for that week as well. In total, teams submitted 233 individual forecast files representing forecasts across the seven influenza seasons. Once submitted, the forecast files were not updated except in four instances where explicit programming bugs had resulted in numerical issues in the forecast. Teams were explicitly discouraged from re-tuning or adjusting their models for different prior seasons to avoid issues with over-fitting.</p><p>Teams utilized a variety of methods and modeling approaches in the construction of their component model submissions (Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). Seven of the components used a compartmental structure (i.e. Susceptible-Infectious-Recovered) to model the disease transmission process, while other components used more statistical approaches to directly model the observed wILI curve. Six of the components explicitly incorporated additional data sources beyond previous wILI data, including weather data and Google search data. Two components were constructed to represent a seasonal baseline based on historical data only.</p><p>Additionally, we obtained the predictive distributions from the CDC-created &#x0201c;unweighted average&#x0201d; model. This ensemble combined all forecast models received by the CDC in real-time in the 2015/2016 (14 models), 2016/2017 (28 models), and 2017/2018 (28 models) seasons [<xref rid="pcbi.1007486.ref026" ref-type="bibr">26</xref>]. These included models that are not part of the collaborative ensemble effort described in this manuscript, although some variations on the components presented here were also submitted to the CDC. Including this model allowed us to compare our ensemble accuracy to the model used by the CDC in real-time during these three seasons.</p><p>It is important to distinguish ensemble components from standalone forecasting models. Standalone models are optimized to be as accurate as possible on their own by, among other things, using proper smoothing. Ensemble components might be designed to be accurate on their own, or else they may be included merely to complement weak spots in other components, i.e. to reduce the ensemble&#x02019;s variance. Because we had sufficient cross-validation data to estimate ensemble weights for several dozen components, some groups contributed non-smoothed &#x0201c;complementing&#x0201d; components for that purpose (Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). Such components may perform poorly on their own, yet their contribution to overall ensemble accuracy may still be significant.</p><p>It should be noted that ensemble weights are not a measure of ensemble components&#x02019; standalone accuracy nor do they measure the overall contribution of a particular model to the ensemble accuracy. For example, consider a setting where a component that is identical (or highly similar) to an existing ensemble component with weight <italic>&#x003c0;</italic>* is added to a given ensemble. The accuracy of the original ensemble can be maintained in a number of ways, including (a) assigning each copy a weight of <italic>&#x003c0;</italic>*/2, or (b) assigning the first copy a weight of <italic>&#x003c0;</italic>* and the second copy a weight of 0. In both of these weightings, at least one high accuracy ensemble component would be assigned significantly lower weight due to the presence of another identical or similar component. In fact, we saw this in our results since the <monospace>Delphi-Stat</monospace> model was the top-performing component model but was a linear combination of other Delphi models. It received zero weight in all of our ensemble specifications. Additionally, inclusion of components with small weights can have a large impact on an ensemble&#x02019;s forecast accuracy.</p></sec><sec id="sec013"><title>Ensemble nomenclature</title><p>There are several different ways that the term ensemble has been used in practice. In this paper, we use the phrases &#x02018;multi-model ensemble&#x02019; or &#x02018;ensemble model&#x02019; interchangably to refer to models that represent mixtures of separate component models. However, a clear taxonomy of ensemble modeling might distinguish three distinct tiers of ensemble models. First, single-model ensemble methodologies can be used to fit models and make predictions. Examples of these approaches include the component models from Columbia University that use, e.g. Ensemble Average Kalman Filtering, to take weighted averages of model realizations to form predictive distributions (Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). Second, multi-model ensembles combine component models through techniques such as model stacking (see <xref ref-type="sec" rid="sec008">Methods</xref>). Among the models described in this work, one component model (<monospace>Delphi-Stat</monospace>) is a multi-model ensemble and all of the FluSight Network models are also multi-model ensembles (Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). Third, the term superensemble has been used for models that combine components that are themselves ensembles (either multi-model or single-model) [<xref rid="pcbi.1007486.ref016" ref-type="bibr">16</xref>, <xref rid="pcbi.1007486.ref041" ref-type="bibr">41</xref>]. Since not all of the components in our approach are ensembles themselves, we chose the term multi-model ensemble to refer to our approach.</p></sec><sec id="sec014"><title>Ensemble construction</title><p>All ensemble models were built using a method that combines component predictive distributions or densities using weighted averages. In the literature, this approach has been called stacking [<xref rid="pcbi.1007486.ref013" ref-type="bibr">13</xref>] or weighted density ensembles [<xref rid="pcbi.1007486.ref023" ref-type="bibr">23</xref>], and is similar to methods used in Bayesian model averaging [<xref rid="pcbi.1007486.ref018" ref-type="bibr">18</xref>]. Let <italic>f</italic><sub><italic>c</italic></sub>(<italic>z</italic><sub><italic>t</italic>,<italic>r</italic>,<italic>w</italic></sub>) represent the predictive density of ensemble component <italic>c</italic> for the value of the target <italic>Z</italic><sub><italic>t</italic>,<italic>r</italic>,<italic>w</italic></sub>, where <italic>t</italic> indexes the particular target, <italic>r</italic> indexes the region, and <italic>w</italic> indexes the week. We combine these components together into a multi-model ensemble with predictive density <italic>f</italic>(<italic>z</italic><sub><italic>t</italic>,<italic>r</italic>,<italic>w</italic></sub>) as follows:
<disp-formula id="pcbi.1007486.e009"><alternatives><graphic xlink:href="pcbi.1007486.e009.jpg" id="pcbi.1007486.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(2)</label></disp-formula>
where <italic>&#x003c0;</italic><sub><italic>c</italic>,<italic>t</italic>,<italic>r</italic></sub> is the weight assigned to component <italic>c</italic> for predictions of target <italic>t</italic> in region <italic>r</italic>. We require <inline-formula id="pcbi.1007486.e010"><alternatives><graphic xlink:href="pcbi.1007486.e010.jpg" id="pcbi.1007486.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:msubsup><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and thereby ensure that <italic>f</italic>(<italic>z</italic><sub><italic>t</italic>,<italic>r</italic>,<italic>w</italic></sub>) remains a valid probability distribution.</p><p>A total of five ensemble weighting schemes were considered, with varying complexity and number of estimated weights (Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>).</p><list list-type="bullet"><list-item><p>Equal Weight (<monospace>FSNetwork-EW</monospace>): This model consisted of assigning all components the same weight regardless of performance and is equivalent to an equally weighted probability density mixture of the components: <italic>&#x003c0;</italic><sub><italic>c</italic>,<italic>t</italic>,<italic>r</italic></sub> = 1/<italic>C</italic>.</p></list-item><list-item><p>Constant Weight model (<monospace>FSNetwork-CW</monospace>): The weights vary across components but have the same value for all targets and regions, for a total of 21 weights: <italic>&#x003c0;</italic><sub><italic>c</italic>,<italic>t</italic>,<italic>r</italic></sub> = <italic>&#x003c0;</italic><sub><italic>c</italic></sub>. For purposes of statistical estimation, we say that the degrees of freedom (df) is (21 &#x02212; 1) = 20. For each set of weights, once 20 weights are estimated the 21st is determined since they must add up to 1.</p></list-item><list-item><p>Target Type Weight model (<monospace>FSNetwork-TTW</monospace>): Weights are estimated separately for our two target-types (<italic>tt</italic>), short-term and seasonal targets, with no variation across regions. This results in a total of 42 weights (df = 40): <italic>&#x003c0;</italic><sub><italic>c</italic>,<italic>t</italic>,<italic>r</italic></sub> = <italic>&#x003c0;</italic><sub><italic>c</italic>,<italic>tt</italic></sub>.</p></list-item><list-item><p>Target Weight model (<monospace>FSNetwork-TW</monospace>): The weights are estimated separately for each of the seven targets for each component with no variation across regions, resulting in 147 weights (df = 140): <italic>&#x003c0;</italic><sub><italic>c</italic>,<italic>t</italic>,<italic>r</italic></sub> = <italic>&#x003c0;</italic><sub><italic>c</italic>,<italic>t</italic></sub>.</p></list-item><list-item><p>Target-Region Weight model (<monospace>FSNetwork-TRW</monospace>): The most complex model considered, this model estiamted weights separately for each component-target-region combination, resulting in 1617 unique weights (df = 1540): <italic>&#x003c0;</italic><sub><italic>c</italic>,<italic>t</italic>,<italic>r</italic></sub> = <italic>&#x003c0;</italic><sub><italic>c</italic>,<italic>t</italic>,<italic>r</italic></sub>.</p></list-item></list><p>Weights were estimated using the EM algorithm (Section 6 in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>) [<xref rid="pcbi.1007486.ref034" ref-type="bibr">34</xref>]. Weights for components were trained using a leave-one-season-out cross-validation approach on component forecasts from the 2010/2011 through 2016/2017 seasons. Given the limited number of seasons available for cross-validation, we used component model forecast scores from all other seasons as training data to estimate weights for a given test season, even if the training season occured chronologically after the test season of interest.</p></sec><sec id="sec015"><title>Ensemble evaluation</title><p>Based on the results of the cross-validation study, we selected one ensemble model as the official FluSight Network entry to the CDC&#x02019;s 2017/2018 influenza forecasting challenge. The criteria for this choice were pre-specified in September of 2017, prior to conducting the cross-validation experiments [<xref rid="pcbi.1007486.ref028" ref-type="bibr">28</xref>]. Component weights for the <monospace>FSNetwork-TTW</monospace> model were estimated using all seven seasons of component model forecasts. In real-time over the course of the 2017/2018 influenza season, participating teams submitted weekly forecasts from each component, which were combined using the estimated weights into the FluSight Network model and submitted to the CDC. The component weights for the submitted model remained unchanged throughout the course of the season.</p></sec><sec id="sec016"><title>Reproducibility and data availability</title><p>To maximize the reproducibility and data availability for this project, the data and code for the entire project are publicly available. The project is available on GitHub [<xref rid="pcbi.1007486.ref042" ref-type="bibr">42</xref>], with a permanent repository stored on Zenodo [<xref rid="pcbi.1007486.ref043" ref-type="bibr">43</xref>]. Code for specific models are either publicly available or available upon request from the modeling teams, with more model-specific details available at the related citations (Table A in <xref ref-type="supplementary-material" rid="pcbi.1007486.s001">S1 Text</xref>). Retrospective and real-time forecasts from the FluSight Network may be interactively browsed on the website <ext-link ext-link-type="uri" xlink:href="http://flusightnetwork.io">http://flusightnetwork.io</ext-link>. Additionally, this manuscript was dynamically generated using R version 3.6.0 (2019-04-26), Sweave, knitr, and make. These tools enable the intermingling of manuscript text with R code that run the central analyses, automatically regenerate parts of the analysis that have changed, and minimize the chance for errors in transcribing or translating results [<xref rid="pcbi.1007486.ref044" ref-type="bibr">44</xref>, <xref rid="pcbi.1007486.ref045" ref-type="bibr">45</xref>].</p></sec></sec><sec sec-type="supplementary-material" id="sec017"><title>Supporting information</title><supplementary-material content-type="local-data" id="pcbi.1007486.s001"><label>S1 Text</label><caption><title>Supplementary methods and results.</title><p>(PDF)</p></caption><media xlink:href="pcbi.1007486.s001.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>The findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention, Defense Advanced Research Projects Agency, Defense Threat Reduction Agency, the National Institutes of Health, National Institute for General Medical Sciences, National Science Foundation, or Uptake Technologies.</p></ack><ref-list><title>References</title><ref id="pcbi.1007486.ref001"><label>1</label><mixed-citation publication-type="other">Centers for Disease Control and Prevention. Estimated Influenza Illnesses, Medical visits, Hospitalizations, and Deaths in the United States&#x02014;2017-2018 influenza season | Seasonal Influenza (Flu) | CDC; 2018. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.cdc.gov/flu/about/burden/estimates.htm">https://www.cdc.gov/flu/about/burden/estimates.htm</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref002"><label>2</label><mixed-citation publication-type="other">Centers for Disease Control and Prevention. Overview of Influenza Surveillance in the United States; 2017. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.cdc.gov/flu/weekly/overview.htm">https://www.cdc.gov/flu/weekly/overview.htm</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref003"><label>3</label><mixed-citation publication-type="other">Centers for Disease Control and Prevention. Estimated Influenza Illnesses, Medical Visits, Hospitalizations, and Deaths Averted by Vaccination in the United States; 2018. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.cdc.gov/flu/about/disease/2016-17.htm">https://www.cdc.gov/flu/about/disease/2016-17.htm</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref004"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Shaman</surname><given-names>J</given-names></name>, <name><surname>Karspeck</surname><given-names>A</given-names></name>, <name><surname>Yang</surname><given-names>W</given-names></name>, <name><surname>Tamerius</surname><given-names>J</given-names></name>, <name><surname>Lipsitch</surname><given-names>M</given-names></name>. <article-title>Real-time influenza forecasts during the 2012-2013 season</article-title>. <source>Nature Communications</source>. <year>2013</year>;<volume>4</volume>
<pub-id pub-id-type="doi">10.1038/ncomms3837</pub-id>
<?supplied-pmid 24302074?><pub-id pub-id-type="pmid">24302074</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref005"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Yang</surname><given-names>W</given-names></name>, <name><surname>Karspeck</surname><given-names>A</given-names></name>, <name><surname>Shaman</surname><given-names>J</given-names></name>. <article-title>Comparison of Filtering Methods for the Modeling and Retrospective Forecasting of Influenza Epidemics</article-title>. <source>PLOS Computational Biology</source>. <year>2014</year>;<volume>10</volume>(<issue>4</issue>):<fpage>e1003583</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1003583</pub-id>
<?supplied-pmid 24762780?><pub-id pub-id-type="pmid">24762780</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref006"><label>6</label><mixed-citation publication-type="journal">
<name><surname>Yang</surname><given-names>S</given-names></name>, <name><surname>Santillana</surname><given-names>M</given-names></name>, <name><surname>Kou</surname><given-names>SC</given-names></name>. <article-title>Accurate estimation of influenza epidemics using Google search data via ARGO</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2015</year>;<volume>112</volume>(<issue>47</issue>):<fpage>14473</fpage>&#x02013;<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1515373112</pub-id>
<?supplied-pmid 26553980?><pub-id pub-id-type="pmid">26553980</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref007"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Chretien</surname><given-names>JP</given-names></name>, <name><surname>George</surname><given-names>D</given-names></name>, <name><surname>Shaman</surname><given-names>J</given-names></name>, <name><surname>Chitale</surname><given-names>RA</given-names></name>, <name><surname>McKenzie</surname><given-names>FE</given-names></name>. <article-title>Influenza Forecasting in Human Populations: A Scoping Review</article-title>. <source>PLOS ONE</source>. <year>2014</year>;<volume>9</volume>(<issue>4</issue>):<fpage>1</fpage>&#x02013;<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0094130</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref008"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Kandula</surname><given-names>S</given-names></name>, <name><surname>Hsu</surname><given-names>D</given-names></name>, <name><surname>Shaman</surname><given-names>J</given-names></name>. <article-title>Subregional Nowcasts of Seasonal Influenza Using Search Trends</article-title>. <source>Journal of Medical Internet Research</source>. <year>2017</year>;<volume>19</volume>(<issue>11</issue>):<fpage>e370</fpage>
<pub-id pub-id-type="doi">10.2196/jmir.7486</pub-id>
<?supplied-pmid 29109069?><pub-id pub-id-type="pmid">29109069</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref009"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Osthus</surname><given-names>D</given-names></name>, <name><surname>Gattiker</surname><given-names>J</given-names></name>, <name><surname>Priedhorsky</surname><given-names>R</given-names></name>, <name><surname>Del Valle</surname><given-names>SY</given-names></name>. <article-title>Dynamic Bayesian influenza forecasting in the United States with hierarchical discrepancy</article-title>. <source>Bayesian Analysis</source>. <year>2018</year>.</mixed-citation></ref><ref id="pcbi.1007486.ref010"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Brooks</surname><given-names>LC</given-names></name>, <name><surname>Farrow</surname><given-names>DC</given-names></name>, <name><surname>Hyun</surname><given-names>S</given-names></name>, <name><surname>Tibshirani</surname><given-names>RJ</given-names></name>, <name><surname>Rosenfeld</surname><given-names>R</given-names></name>. <article-title>Nonmechanistic forecasts of seasonal influenza with iterative one-week-ahead distributions</article-title>. <source>PLOS Computational Biology</source>. <year>2018</year>;<volume>14</volume>(<issue>6</issue>):<fpage>e1006134</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1006134</pub-id>
<?supplied-pmid 29906286?><pub-id pub-id-type="pmid">29906286</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref011"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Pei</surname><given-names>S</given-names></name>, <name><surname>Kandula</surname><given-names>S</given-names></name>, <name><surname>Yang</surname><given-names>W</given-names></name>, <name><surname>Shaman</surname><given-names>J</given-names></name>. <article-title>Forecasting the spatial transmission of influenza in the United States</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2018</year>;<volume>115</volume>(<issue>11</issue>):<fpage>2752</fpage>&#x02013;<lpage>2757</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1708856115</pub-id>
<?supplied-pmid 29483256?><pub-id pub-id-type="pmid">29483256</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref012"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Bates</surname><given-names>JM</given-names></name>, <name><surname>Granger</surname><given-names>CW</given-names></name>. <article-title>The combination of forecasts</article-title>. <source>Journal of the Operational Research Society</source>. <year>1969</year>;<volume>20</volume>(<issue>4</issue>):<fpage>451</fpage>&#x02013;<lpage>468</lpage>. <pub-id pub-id-type="doi">10.1057/jors.1969.103</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref013"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Wolpert</surname><given-names>DH</given-names></name>. <article-title>Stacked generalization</article-title>. <source>Neural Networks</source>. <year>1992</year>;<volume>5</volume>(<issue>2</issue>):<fpage>241</fpage>&#x02013;<lpage>259</lpage>. <pub-id pub-id-type="doi">10.1016/S0893-6080(05)80023-1</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref014"><label>14</label><mixed-citation publication-type="other">Polikar R. Ensemble based systems in decision making. IEEE Circuits and Systems Magazine. 2006.</mixed-citation></ref><ref id="pcbi.1007486.ref015"><label>15</label><mixed-citation publication-type="book">
<name><surname>Hastie</surname><given-names>T</given-names></name>, <name><surname>Tibshirani</surname><given-names>R</given-names></name>, <name><surname>Friedman</surname><given-names>J</given-names></name>. <source>The Elements of Statistical Learning</source>. <publisher-name>Springer</publisher-name>; <year>2009</year>.</mixed-citation></ref><ref id="pcbi.1007486.ref016"><label>16</label><mixed-citation publication-type="journal">
<name><surname>Krishnamurti</surname><given-names>T</given-names></name>, <name><surname>Kishtawal</surname><given-names>C</given-names></name>, <name><surname>LaRow</surname><given-names>TE</given-names></name>, <name><surname>Bachiochi</surname><given-names>DR</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Williford</surname><given-names>CE</given-names></name>, <etal>et al</etal>
<article-title>Improved weather and seasonal climate forecasts from multimodel superensemble</article-title>. <source>Science</source>. <year>1999</year>;<volume>285</volume>(<issue>5433</issue>):<fpage>1548</fpage>&#x02013;<lpage>1550</lpage>. <pub-id pub-id-type="doi">10.1126/science.285.5433.1548</pub-id>
<?supplied-pmid 10477515?><pub-id pub-id-type="pmid">10477515</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref017"><label>17</label><mixed-citation publication-type="journal">
<name><surname>Palmer</surname><given-names>TN</given-names></name>. <article-title>Predicting uncertainty in numerical weather forecasts</article-title>. <source>International Geophysics</source>. <year>2002</year>
<pub-id pub-id-type="doi">10.1016/S0074-6142(02)80152-8</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref018"><label>18</label><mixed-citation publication-type="journal">
<name><surname>Raftery</surname><given-names>AE</given-names></name>, <name><surname>Gneiting</surname><given-names>T</given-names></name>, <name><surname>Balabdaoui</surname><given-names>F</given-names></name>, <name><surname>Polakowski</surname><given-names>M</given-names></name>. <article-title>Using Bayesian Model Averaging to Calibrate Forecast Ensembles</article-title>. <source>Monthly Weather Review</source>. <year>2005</year>;<volume>133</volume>(<issue>5</issue>):<fpage>1155</fpage>&#x02013;<lpage>1174</lpage>. <pub-id pub-id-type="doi">10.1175/MWR2906.1</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref019"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Leutbecher</surname><given-names>M</given-names></name>, <name><surname>Palmer</surname><given-names>TN</given-names></name>. <article-title>Ensemble forecasting</article-title>. <source>Journal of Computational Physics</source>. <year>2008</year>
<pub-id pub-id-type="doi">10.1016/j.jcp.2007.02.014</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref020"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Yamana</surname><given-names>TK</given-names></name>, <name><surname>Kandula</surname><given-names>S</given-names></name>, <name><surname>Shaman</surname><given-names>J</given-names></name>. <article-title>Individual versus superensemble forecasts of seasonal influenza outbreaks in the United States</article-title>. <source>PLOS Computational Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>11</issue>):<fpage>e1005801</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1005801</pub-id>
<?supplied-pmid 29107987?><pub-id pub-id-type="pmid">29107987</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref021"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Smith</surname><given-names>ME</given-names></name>, <name><surname>Singh</surname><given-names>BK</given-names></name>, <name><surname>Irvine</surname><given-names>MA</given-names></name>, <name><surname>Stolk</surname><given-names>WA</given-names></name>, <name><surname>Subramanian</surname><given-names>S</given-names></name>, <name><surname>Hollingsworth</surname><given-names>TD</given-names></name>, <etal>et al</etal>
<article-title>Predicting lymphatic filariasis transmission and elimination dynamics using a multi-model ensemble framework</article-title>. <source>Epidemics</source>. <year>2017</year>;<volume>18</volume>:<fpage>16</fpage>&#x02013;<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1016/j.epidem.2017.02.006</pub-id>
<?supplied-pmid 28279452?><pub-id pub-id-type="pmid">28279452</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref022"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Viboud</surname><given-names>C</given-names></name>, <name><surname>Sun</surname><given-names>K</given-names></name>, <name><surname>Gaffey</surname><given-names>R</given-names></name>, <name><surname>Ajelli</surname><given-names>M</given-names></name>, <name><surname>Fumanelli</surname><given-names>L</given-names></name>, <name><surname>Merler</surname><given-names>S</given-names></name>, <etal>et al</etal>
<article-title>The RAPIDD Ebola forecasting challenge: Synthesis and lessons learnt</article-title>. <source>Epidemics</source>. <year>2018</year>;<volume>22</volume>:<fpage>13</fpage>&#x02013;<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.epidem.2017.08.002</pub-id>
<?supplied-pmid 28958414?><pub-id pub-id-type="pmid">28958414</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref023"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Ray</surname><given-names>EL</given-names></name>, <name><surname>Reich</surname><given-names>NG</given-names></name>. <article-title>Prediction of infectious disease epidemics via weighted density ensembles</article-title>. <source>PLOS Computational Biology</source>. <year>2018</year>;<volume>14</volume>(<issue>2</issue>):<fpage>e1005910</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1005910</pub-id>
<?supplied-pmid 29462167?><pub-id pub-id-type="pmid">29462167</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref024"><label>24</label><mixed-citation publication-type="other">DARPA. CHIKV Challenge Announces Winners, Progress toward Forecasting the Spread of Infectious Diseases; 2015. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.darpa.mil/news-events/2015-05-27">https://www.darpa.mil/news-events/2015-05-27</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref025"><label>25</label><mixed-citation publication-type="other">NOAA, CDC. Dengue Forecasting; 2018. Available from: <ext-link ext-link-type="uri" xlink:href="http://dengueforecasting.noaa.gov/about.php">http://dengueforecasting.noaa.gov/about.php</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref026"><label>26</label><mixed-citation publication-type="journal">
<name><surname>McGowan</surname><given-names>C</given-names></name>, <name><surname>Biggerstaff</surname><given-names>M</given-names></name>, <name><surname>Johansson</surname><given-names>MA</given-names></name>, <name><surname>Apfeldorf</surname><given-names>K</given-names></name>, <name><surname>Ben-Nun</surname><given-names>M</given-names></name>, <name><surname>Brooks</surname><given-names>L</given-names></name>, <etal>et al</etal>
<article-title>Collaborative efforts to forecast seasonal influenza in the United States, 2015-2016</article-title>. <source>Nature Scientific Reports</source>. <year>2019</year>;<volume>9</volume>(<issue>683</issue>).</mixed-citation></ref><ref id="pcbi.1007486.ref027"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Reich</surname><given-names>NG</given-names></name>, <name><surname>Brooks</surname><given-names>L</given-names></name>, <name><surname>Fox</surname><given-names>S</given-names></name>, <name><surname>Kandula</surname><given-names>S</given-names></name>, <name><surname>McGowan</surname><given-names>C</given-names></name>, <name><surname>Moore</surname><given-names>E</given-names></name>, <etal>et al</etal>
<article-title>A collaborative multiyear, multimodel assessment of seasonal influenza forecasting in the United States</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2019</year>; p. 201812594. <pub-id pub-id-type="doi">10.1073/pnas.1812594116</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref028"><label>28</label><mixed-citation publication-type="other">Reich N. Guidelines for a CDC FluSight ensemble (2017-2018); 2017. Available from: <ext-link ext-link-type="uri" xlink:href="https://github.com/FluSightNetwork/cdc-flusight-ensemble/blob/eadf553fcf85d89e16322ef1b44bc9990fc9e0a7/README.md">https://github.com/FluSightNetwork/cdc-flusight-ensemble/blob/eadf553fcf85d89e16322ef1b44bc9990fc9e0a7/README.md</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref029"><label>29</label><mixed-citation publication-type="journal">
<name><surname>Angus</surname><given-names>JE</given-names></name>. <article-title>The probability integral transform and related results</article-title>. <source>SIAM review</source>. <year>1994</year>;<volume>36</volume>(<issue>4</issue>):<fpage>652</fpage>&#x02013;<lpage>654</lpage>. <pub-id pub-id-type="doi">10.1137/1036146</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref030"><label>30</label><mixed-citation publication-type="journal">
<name><surname>Diebold</surname><given-names>FX</given-names></name>, <name><surname>Gunther</surname><given-names>TA</given-names></name>, <name><surname>Tay</surname><given-names>AS</given-names></name>. <article-title>Evaluating density forecasts with applications to financial risk management</article-title>. <source>International Economic Review</source>. <year>1998</year>;<volume>39</volume>(<issue>4</issue>):<fpage>863</fpage>&#x02013;<lpage>883</lpage>. <pub-id pub-id-type="doi">10.2307/2527342</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref031"><label>31</label><mixed-citation publication-type="journal">
<name><surname>Farrow</surname><given-names>DC</given-names></name>, <name><surname>Brooks</surname><given-names>LC</given-names></name>, <name><surname>Hyun</surname><given-names>S</given-names></name>, <name><surname>Tibshirani</surname><given-names>RJ</given-names></name>, <name><surname>Burke</surname><given-names>DS</given-names></name>, <name><surname>Rosenfeld</surname><given-names>R</given-names></name>. <article-title>A human judgment approach to epidemiological forecasting</article-title>. <source>PLOS Computational Biology</source>. <year>2017</year>;<volume>13</volume>(<issue>3</issue>):<fpage>e1005248</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1005248</pub-id>
<?supplied-pmid 28282375?><pub-id pub-id-type="pmid">28282375</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref032"><label>32</label><mixed-citation publication-type="journal">
<name><surname>Kandula</surname><given-names>S</given-names></name>, <name><surname>Yang</surname><given-names>W</given-names></name>, <name><surname>Shaman</surname><given-names>J</given-names></name>. <article-title>Type- and Subtype-Specific Influenza Forecast</article-title>. <source>American Journal of Epidemiology</source>. <year>2017</year>;<volume>185</volume>(<issue>5</issue>):<fpage>395</fpage>&#x02013;<lpage>402</lpage>. <pub-id pub-id-type="doi">10.1093/aje/kww211</pub-id>
<?supplied-pmid 28174833?><pub-id pub-id-type="pmid">28174833</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref033"><label>33</label><mixed-citation publication-type="journal">
<name><surname>Held</surname><given-names>L</given-names></name>, <name><surname>Meyer</surname><given-names>S</given-names></name>, <name><surname>Bracher</surname><given-names>J</given-names></name>. <article-title>Probabilistic forecasting in infectious disease epidemiology: the 13th Armitage lecture</article-title>. <source>Statistics in medicine</source>. <year>2017</year>;<volume>36</volume>(<issue>22</issue>):<fpage>3443</fpage>&#x02013;<lpage>3460</lpage>. <pub-id pub-id-type="doi">10.1002/sim.7363</pub-id>
<?supplied-pmid 28656694?><pub-id pub-id-type="pmid">28656694</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref034"><label>34</label><mixed-citation publication-type="other">McAndrew T, Reich NG. Adaptively stacking ensembles for influenza forecasting with incomplete data. arXiv. 2019.</mixed-citation></ref><ref id="pcbi.1007486.ref035"><label>35</label><mixed-citation publication-type="journal">
<name><surname>Gneiting</surname><given-names>T</given-names></name>, <name><surname>Raftery</surname><given-names>AE</given-names></name>. <article-title>Strictly proper scoring rules, prediction, and estimation</article-title>. <source>Journal of the American Statistical Association</source>. <year>2007</year>;<volume>102</volume>(<issue>477</issue>):<fpage>359</fpage>&#x02013;<lpage>378</lpage>. <pub-id pub-id-type="doi">10.1198/016214506000001437</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref036"><label>36</label><mixed-citation publication-type="journal">
<name><surname>Bracher</surname><given-names>J</given-names></name>. <article-title>On the multibin logarithmic score used in the FluSight competitions</article-title>. <source>PNAS</source>. <year>2019</year>
<pub-id pub-id-type="doi">10.1073/pnas.1912147116</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref037"><label>37</label><mixed-citation publication-type="journal">
<name><surname>Reich</surname><given-names>N</given-names></name>, <name><surname>Osthus</surname><given-names>D</given-names></name>, <name><surname>Ray</surname><given-names>E</given-names></name>, <name><surname>Yamana</surname><given-names>T</given-names></name>, <name><surname>Biggerstaff</surname><given-names>M</given-names></name>, <name><surname>Johansson</surname><given-names>M</given-names></name>, <etal>et al</etal>
<article-title>Reply to Bracher: Scoring probabilistic forecasts to maximize public health interpretability</article-title>. <source>PNAS</source>. <year>2019</year>
<pub-id pub-id-type="doi">10.1073/pnas.1912694116</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref038"><label>38</label><mixed-citation publication-type="other">New Mexico Department of Health. Indicator-Based Information System for Public Health Web; 2018. Available from: <ext-link ext-link-type="uri" xlink:href="https://ibis.health.state.nm.us/resource/MMWRWeekCalendar.html">https://ibis.health.state.nm.us/resource/MMWRWeekCalendar.html</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref039"><label>39</label><mixed-citation publication-type="other">Niemi J. MMWRweek: Convert Dates to MMWR Day, Week, and Year; 2015. Available from: <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=MMWRweek">https://CRAN.R-project.org/package=MMWRweek</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref040"><label>40</label><mixed-citation publication-type="other">Tushar A. pymmwr: MMWR weeks for Python; 2018. Available from: <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/pymmwr/">https://pypi.org/project/pymmwr/</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref041"><label>41</label><mixed-citation publication-type="journal">
<name><surname>Yamana</surname><given-names>TK</given-names></name>, <name><surname>Kandula</surname><given-names>S</given-names></name>, <name><surname>Shaman</surname><given-names>J</given-names></name>. <article-title>Superensemble forecasts of dengue outbreaks</article-title>. <source>Journal of The Royal Society Interface</source>. <year>2016</year>;<volume>13</volume>(<issue>123</issue>):<fpage>20160410</fpage>
<pub-id pub-id-type="doi">10.1098/rsif.2016.0410</pub-id></mixed-citation></ref><ref id="pcbi.1007486.ref042"><label>42</label><mixed-citation publication-type="other">Tushar A, Reich N, Yamana T, Osthus D, McGowan C, Ray E, et al. FluSightNetwork: cdc-flusight-ensemble repository; 2018. Available from: <ext-link ext-link-type="uri" xlink:href="https://github.com/FluSightNetwork/cdc-flusight-ensemble">https://github.com/FluSightNetwork/cdc-flusight-ensemble</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref043"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Tushar</surname><given-names>A</given-names></name>, <name><surname>Reich</surname><given-names>N</given-names></name>, <name><surname>Yamana</surname><given-names>T</given-names></name>, <name><surname>Osthus</surname><given-names>D</given-names></name>, <name><surname>McGowan</surname><given-names>C</given-names></name>, <name><surname>Ray</surname><given-names>E</given-names></name>, <etal>et al</etal>
<source>FluSightNetwork/cdc-flusight-ensemble v1.0</source>; <year>2018</year> Available from: <pub-id pub-id-type="doi">10.5281/zenodo.1255023</pub-id>.</mixed-citation></ref><ref id="pcbi.1007486.ref044"><label>44</label><mixed-citation publication-type="book">
<name><surname>Xie</surname><given-names>Y</given-names></name>. <source>Dynamic Documents with R and knitr</source>. <edition>2nd ed</edition>
<publisher-loc>Boca Raton, Florida</publisher-loc>: <publisher-name>Chapman and Hall/CRC</publisher-name>; <year>2015</year> Available from: <ext-link ext-link-type="uri" xlink:href="https://yihui.name/knitr/">https://yihui.name/knitr/</ext-link>.</mixed-citation></ref><ref id="pcbi.1007486.ref045"><label>45</label><mixed-citation publication-type="other">R Core Team. R: A Language and Environment for Statistical Computing; 2017. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link>.</mixed-citation></ref></ref-list></back><sub-article id="pcbi.1007486.r001" article-type="aggregated-review-documents"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007486.r001</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pitzer</surname><given-names>Virginia E.</given-names></name><role>Deputy Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2019 Virginia E. Pitzer</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Virginia E. Pitzer</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="rel-obj001" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007486" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">21 Aug 2019</named-content>
</p><p>Dear Dr Reich,</p><p>Thank you very much for submitting your manuscript, 'Accuracy of real-time multi-model ensemble forecasts for seasonal influenza in the U.S.', to PLOS Computational Biology. As with all papers submitted to the journal, yours was fully evaluated by the PLOS Computational Biology editorial team, and in this case, by independent peer reviewers. The reviewers appreciated the attention to an important topic but identified some aspects of the manuscript that should be improved.</p><p>We would therefore like to ask you to modify the manuscript according to the review recommendations before we can consider your manuscript for acceptance. Your revisions should address the specific points made by each reviewer and we encourage you to respond to particular issues Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out.raised.</p><p>In addition, when you are ready to resubmit, please be prepared to provide the following:</p><p>(1) A detailed list of your responses to the review comments and the changes you have made in the manuscript. We require a file of this nature before your manuscript is passed back to the editors.</p><p>(2) A copy of your manuscript with the changes highlighted (encouraged). We encourage authors, if possible to show clearly where changes have been made to their manuscript e.g. by highlighting text.</p><p>(3) A striking still image to accompany your article (optional). If the image is judged to be suitable by the editors, it may be featured on our website and might be chosen as the issue image for that month. These square, high-quality images should be accompanied by a short caption. Please note as well that there should be no copyright restrictions on the use of the image, so that it can be published under the Open-Access license and be subject only to appropriate attribution.</p><p>Before you resubmit your manuscript, please consult our Submission Checklist to ensure your manuscript is formatted correctly for PLOS Computational Biology: <ext-link ext-link-type="uri" xlink:href="http://www.ploscompbiol.org/static/checklist.action">http://www.ploscompbiol.org/static/checklist.action</ext-link>. Some key points to remember are:</p><p>- Figures uploaded separately as TIFF or EPS files (if you wish, your figures may remain in your main manuscript file in addition).</p><p>- Supporting Information uploaded as separate files, titled 'Dataset', 'Figure', 'Table', 'Text', 'Protocol', 'Audio', or 'Video'.</p><p>- Funding information in the 'Financial Disclosure' box in the online system.</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link ext-link-type="uri" xlink:href="https://pacev2.apexcovantage.com">https://pacev2.apexcovantage.com</ext-link> &#x000a0;PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <email>figures@plos.org</email>.</p><p>We hope to receive your revised manuscript within the next 30 days. If you anticipate any delay in its return, we ask that you let us know the expected resubmission date by email at <email>ploscompbiol@plos.org</email>.</p><p>If you have any questions or concerns while you make these revisions, please let us know.</p><p>Sincerely,</p><p>Virginia E. Pitzer, Sc.D.</p><p>Deputy Editor</p><p>PLOS Computational Biology</p><p>Rob De Boer</p><p>Deputy Editor</p><p>PLOS Computational Biology</p><p>A link appears below if there are any accompanying review attachments. If you believe any reviews to be missing, please contact <email>ploscompbiol@plos.org</email> immediately:</p><p>[LINK]</p><p>Reviewer's Responses to Questions</p><p><bold>Comments to the Authors:</bold></p><p><bold>Please note here if the review is uploaded as an attachment.</bold></p><p>Reviewer #1: Reich and colleagues describe the development and validation of a multi-model ensemble approach for flu forecasting. This work has arisen out of a collaborative network of groups participating in the CDC&#x02019;s flu forecasting challenge. It is hard to find much to fault with this study. The paper is clearly written, and the analysis plan and execution are rigorous and well-thought out. This study provides a gold-standard for how forecasting work should be performed, with clear, pre-specified outcomes, extensive development with cross validation, and an out-of-sample, real-time test of the method. I have only minor comments to help with the interpretation of some of the results.</p><p>I understand that the authors were constrained in the metrics that were reported based on the criteria for the CDC&#x02019;s contest. However, in some instances, it would be easier to interpret if the results were presented differently. For instance, the authors demonstrate that the TTW model performs better than the equal weight model. But it is difficult to evaluate from the forecast scores whether these improvements are meaningful. Expressing some of the results in terms of the units of measurement might be helpful. For instance, the estimate of peak week was off by an average of X weeks for TTW compared with Y weeks for equal-weighting.</p><p>Is there a way to visualize/summarize how the forecast accuracy for the different models changes throughout the season as data accrue for other forecasting targets (similar to what is done in figure 6 for peak week)?</p><p>Given that the ensembles performed worse than the prior-season average for predicting peak intensity early in the season (Fig 6), would there be any benefit to including the prior years average itself in the ensemble?</p><p>It seems that the ensemble weights here do not vary through the season and are based only on the cross-validation period. Would there be any benefit to using the cross-validation weights as a starting point and then allowing the ensemble weights to vary each week as the data accrue?</p><p>Could the authors comment on the criteria chosen by CDC for evaluating accuracy and precision as well as the forecasting targets and whether there are modifications that they would suggest based on their experience?</p><p>Reviewer #2: In this article, the authors demonstrate that a multi-model ensemble forecast was able to provide accurate forecasts of influenza-like illness activity over the course of the 2017/18 United States influenza season. They describe how a range of multi-model ensembles were created and trained on past years' data, how the best-performing ensemble was identified, and then entered into the CDC FluSight challenge. This ensemble not only out-performed each of the individual models in the ensemble, but also came second overall in the challenge, despite the 2017/18 season being highly unusual. These kinds of collaborative efforts and methodological developments are critical if such forecasts are to become a part of routine public health surveillance.</p><p>Comments:</p><p>1. The motivation for selecting only one ensemble model as the official FluSight Network entry in the 2017/18 challenge is perfectly reasonable and pragmatic.</p><p>But it would be great to see even a cursory comparison of the selected ensemble model (FSNetwork-TTW) and the four other ensemble models (FSNetwork-EW, FSNetwork-CW, FSNetwork-TW, FSNetwork-TRW) for 2017/18, even though those other models weren't officially entered into the competition. In particular, it would be really interesting to see whether the performance similarities between these models in the training phase were also evident in such an unusual influenza season, or if the differences between these models conveyed any (dis)advantages in this scenario.</p><p>2. In the result section (page 8, lines 179-182) the authors state:</p><p>"Overall, the FSNetwork-TTW model ranked second among selected models in both RMSE and average bias, behind the LANL-DBM model (see Appendix), suggesting that using separate weighting schemes for point estimates and predictive distribution may be valuable."</p><p>This gave me a moment's pause. The reasoning is explained in more detail in section 2.1 of the supplementary material, and some of this detail could be included in the main text. Perhaps just a reminder that ensemble weights were chosen to maximize log scores, rather than point-estimate errors, would be sufficient.</p><p>3. In figure 4, cells with dark blue background could have values shown in white text or a light color, rather than black text, to make it easier to read. This also applies to Figure 1 in the supplementary material.</p><p>4. In the methods section (page 15, lines 388-389) a reference is missing:</p><p>"Second, multi-model ensembles combine component models through techniques such as model stacking (see Section )."</p><p>**********</p><p><bold>Have all data underlying the figures and results presented in the manuscript been provided?</bold></p><p>Large-scale datasets should be made available via a public repository as described in the <italic>PLOS Computational Biology</italic>
<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/ploscompbiol/s/data-availability">data availability policy</ext-link>, and numerical data that underlies graphs or summary statistics should be provided in spreadsheet form as supporting information.</p><p>Reviewer #1: Yes</p><p>Reviewer #2: Yes</p><p>**********</p><p>PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p><p>Reviewer #1: No</p><p>Reviewer #2: No</p></body></sub-article><sub-article id="pcbi.1007486.r002" article-type="author-comment"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007486.r002</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><related-article id="rel-obj002" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007486" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">22 Sep 2019</named-content>
</p><supplementary-material content-type="local-data" id="pcbi.1007486.s002"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">201908-ploscompbio-response.docx (1).pdf</named-content></p></caption><media xlink:href="pcbi.1007486.s002.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></body></sub-article><sub-article id="pcbi.1007486.r003" article-type="aggregated-review-documents"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007486.r003</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pitzer</surname><given-names>Virginia E.</given-names></name><role>Deputy Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2019 Virginia E. Pitzer</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Virginia E. Pitzer</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="rel-obj003" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007486" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">14 Oct 2019</named-content>
</p><p>Dear Dr Reich,</p><p>We are pleased to inform you that your manuscript 'Accuracy of real-time multi-model ensemble forecasts for seasonal influenza in the U.S.' has been provisionally accepted for publication in PLOS Computational Biology.</p><p>Before your manuscript can be formally accepted you will need to complete some formatting changes, which you will receive in a follow up email. Please be aware that it may take several days for you to receive this email; during this time no action is required by you. Once you have received these formatting requests, please note that your manuscript will not be scheduled for publication until you have made the required changes.</p><p>In the meantime, please log into Editorial Manager at <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pcompbiol/">https://www.editorialmanager.com/pcompbiol/</ext-link>, click the "Update My Information" link at the top of the page, and update your user information to ensure an efficient production and billing process.</p><p>One of the goals of PLOS is to make science accessible to educators and the public. PLOS staff issue occasional press releases and make early versions of PLOS Computational Biology articles available to science writers and journalists. PLOS staff also collaborate with Communication and Public Information Offices and would be happy to work with the relevant people at your institution or funding agency. If your institution or funding agency is interested in promoting your findings, please ask them to coordinate their releases with PLOS (contact <email>ploscompbiol@plos.org</email>).</p><p>Thank you again for supporting Open Access publishing. We look forward to publishing your paper in PLOS Computational Biology.</p><p>Sincerely,</p><p>Virginia E. Pitzer, Sc.D.</p><p>Deputy Editor</p><p>PLOS Computational Biology</p><p>Rob De Boer</p><p>Deputy Editor</p><p>PLOS Computational Biology</p><p>Reviewer's Responses to Questions</p><p><bold>Comments to the Authors:</bold></p><p><bold>Please note here if the review is uploaded as an attachment.</bold></p><p>Reviewer #1: The authors have thoroughly responded to my comments.</p><p>Reviewer #2: The authors have thoroughly addressed all of my original comments, and I have no further comments for them to address. This is a very well designed study, reported in a very clear manuscript.</p><p>**********</p><p><bold>Have all data underlying the figures and results presented in the manuscript been provided?</bold></p><p>Large-scale datasets should be made available via a public repository as described in the <italic>PLOS Computational Biology</italic>
<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/ploscompbiol/s/data-availability">data availability policy</ext-link>, and numerical data that underlies graphs or summary statistics should be provided in spreadsheet form as supporting information.</p><p>Reviewer #1: Yes</p><p>Reviewer #2: Yes</p><p>**********</p><p>PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p><p>Reviewer #1: No</p><p>Reviewer #2: No</p></body></sub-article><sub-article id="pcbi.1007486.r004" article-type="editor-report"><front-stub><article-id pub-id-type="doi">10.1371/journal.pcbi.1007486.r004</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pitzer</surname><given-names>Virginia E.</given-names></name><role>Deputy Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2019 Virginia E. Pitzer</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Virginia E. Pitzer</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="rel-obj004" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007486" related-article-type="reviewed-article"/></front-stub><body><p>
<named-content content-type="letter-date">12 Nov 2019</named-content>
</p><p>PCOMPBIOL-D-19-01044R1 </p><p>Accuracy of real-time multi-model ensemble forecasts for seasonal influenza in the U.S.</p><p>Dear Dr Reich,</p><p>I am pleased to inform you that your manuscript has been formally accepted for publication in PLOS Computational Biology. Your manuscript is now with our production department and you will be notified of the publication date in due course.</p><p>The corresponding author will soon be receiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript carefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the work, will likely cause delays to the publication date of your manuscript. </p><p>Soon after your final files are uploaded, unless you have opted out, the early version of your manuscript will be published online. The date of the early version will be your article's publication date. The final article will be published to the same URL, and all versions of the paper will be accessible to readers.</p><p>Thank you again for supporting PLOS Computational Biology and open-access publishing. We are looking forward to publishing your work! </p><p>With kind regards,</p><p>Matt Lyles</p><p>PLOS Computational Biology | Carlyle House, Carlyle Road, Cambridge CB4 3DN | United Kingdom <email>ploscompbiol@plos.org</email> | Phone +44 (0) 1223-442824 | <ext-link ext-link-type="uri" xlink:href="http://ploscompbiol.org">ploscompbiol.org</ext-link> | @PLOSCompBiol</p></body></sub-article></article>