<?xml version="1.0" encoding="UTF-8"?>
<p>Similar to metadata documentation, detailed documentation of sample processing and ultimately the choice of data analysis tools and parameter settings are becoming more widely advocated (
 <xref rid="ref-105" ref-type="bibr">Nature Editorial, 2017</xref>). Hence, appropriate workflow documentation is essential and will become an important component of comparative biology in the genomic era in general and in particular in host-pathogen interaction studies. This begins with explicit wet lab protocol documentation that can be easily referenced in publications, such as protocols.io (
 <xref rid="ref-158" ref-type="bibr">Teytelman et al., 2016</xref>), a protocol repository. This complements other peer-reviewed options from journals specifically dedicated to methods publication, such as 
 <italic>Nature Protocols</italic>, 
 <italic>JOVE</italic>, or 
 <italic>MethodsX</italic>. For data analysis, the use of scripted pipelines and version-controlled analyses has been advocated to address challenges of analysis reproducibility (e.g., 
 <xref rid="ref-108" ref-type="bibr">Nunez-Iglesias, 2015</xref>). At the most basic level this includes a scripted analysis that does not require manual command input and thus is completely repeatable given the same raw data and sufficient computational time (
 <xref rid="ref-10" ref-type="bibr">Beaulieu-Jones &amp; Greene, 2016</xref>). Today, various toolkits, repositories and work platforms exist that advocate these principles and facilitate their implementation (see 
 <ext-link ext-link-type="uri" xlink:href="https://github.com/pditommaso/awesome-pipeline" xmlns:xlink="http://www.w3.org/1999/xlink">https://github.com/pditommaso/awesome-pipeline</ext-link> for a non-exhaustive but curated list). Among others, these include literate programming options such as provided by R Markdown (
 <ext-link ext-link-type="uri" xlink:href="https://rmarkdown.rstudio.com" xmlns:xlink="http://www.w3.org/1999/xlink">https://rmarkdown.rstudio.com</ext-link>) or Jupyter (
 <ext-link ext-link-type="uri" xlink:href="https://jupyter.org" xmlns:xlink="http://www.w3.org/1999/xlink">https://jupyter.org</ext-link>), code development repository such as GitHub (
 <ext-link ext-link-type="uri" xlink:href="https://github.com" xmlns:xlink="http://www.w3.org/1999/xlink">https://github.com</ext-link>) or Dryad (
 <ext-link ext-link-type="uri" xlink:href="https://datadryad.org" xmlns:xlink="http://www.w3.org/1999/xlink">https://datadryad.org</ext-link>), as an example of a more general digital repository. For genomics specifically, the graphical user interface guided data integration, analysis, and publishing platform Galaxy has been a long-time advocate of communicating standardized best practices of analysis workflows and thus ensuring reproducibility and development of common analyses pipelines. Overall the adoption of best practices and detailed workflow documentation will improve reproducibility and integration of results across studies, however, it does not preclude the careful selection and validation of appropriate methods (
 <xref rid="ref-96" ref-type="bibr">Lotterhos, Moore &amp; Stapleton, 2018</xref>). In principle, this could ultimately lead to automated analysis of organisms with more limited genomic resources, which might permit linking of metadata (such as whether a study is experimental or naturally observed) with sequence data across studies. Such examples are currently still mainly restricted to curated data sets with a narrow purpose, for example Bgee (
 <xref rid="ref-9" ref-type="bibr">Bastian et al., 2008</xref>) which facilitates automated cross-species comparison of “healthy” control individuals. Extensions of such projects would open up exciting frontiers in comparative studies of host-pathogen interactions across different systems and beyond. At this time, however, comparative studies such as the recent investigation into MHC copy number variation across 
 <italic>Aves</italic> (
 <xref rid="ref-102" ref-type="bibr">Minias et al., 2018</xref>) illustrate the norm: researchers evaluate large amounts of data from repositories, which they curate by consulting the primary publication for a specific question, and statistically account for inconsistencies and uncertainties of the assembled data in their analysis.
</p>
