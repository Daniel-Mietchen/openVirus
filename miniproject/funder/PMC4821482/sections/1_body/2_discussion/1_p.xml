<?xml version="1.0" encoding="UTF-8"?>
<p>Predicting whether or not a major epidemic is likely, from the limited data typically available during the first few days of an outbreak, has received surprisingly little attention. A notable exception is the paper by Drake [
 <xref rid="pcbi.1004836.ref051" ref-type="bibr">51</xref>], which shows that the exact final size varies significantly between simulated outbreaks under identical conditions. He investigates how this variability scales with the contact rate between individuals and the efficacy and speed of control responses. However an incubation period is not explicitly included in the model used. Craft 
 <italic>et al</italic>. [
 <xref rid="pcbi.1004836.ref007" ref-type="bibr">7</xref>] use a model of rabies in canids to show that the first four death times cannot be used to forecast major outbreaks. However, by assuming that the data consist of death times alone, the factors potentially responsible for this imprecision are confounded. Neither Drake [
 <xref rid="pcbi.1004836.ref051" ref-type="bibr">51</xref>] nor Craft 
 <italic>et al</italic>. [
 <xref rid="pcbi.1004836.ref007" ref-type="bibr">7</xref>] quantify the error caused by presymptomatic infection. In addition to quantifying this error, our main message is that presymptomatic infection by itself is sufficient to cause error in predictions of whether or not an outbreak will be major, let alone in predicting the final size exactly. This error is particularly notable when there are no infected individuals in the population at all (i.e. the outbreak has already faded out), since the distribution that estimates the number of presymptomatic infected individuals will include values other than zero.
</p>
