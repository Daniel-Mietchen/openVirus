<?xml version="1.0" encoding="UTF-8"?>
<p>XGBoost is a meta-algorithm used to construct an ensemble of strong learners from weak learners, typically decision trees, on a modified dataset [
 <xref rid="B103-ijms-20-05743" ref-type="bibr">103</xref>]. XGBoost, proposed by Chen and Guestrin [
 <xref rid="B104-ijms-20-05743" ref-type="bibr">104</xref>] is a boosted tree algorithm, which follows the principle of gradient boosting. In recent years, XGBoost has been used extensively by data scientists and achieves satisfactory results on various biological problems [
 <xref rid="B105-ijms-20-05743" ref-type="bibr">105</xref>]. In this study, the prediction of AVPs can be considered as a binary classification problem. Given a peptide sequence, we used XGBoost to predict its class label (−1 or 1), where +1 and −1 represent AVPs and Non-AVPs, respectively. For achieving the best XGBoost model, five parameters namely eta (i.e., the number of the learning rate), max_depth (i.e., the number of the depth of the tree), colsample_bytree (i.e., the number of features or variables to construct a learner), subsample (i.e., the number of samples or observations to construct a learner), and nrounds (i.e., the maximum number of iterations) were determined using the caret R package [
 <xref rid="B100-ijms-20-05743" ref-type="bibr">100</xref>] with a five-fold CV approach. The search space of eta, max_depth, colsample_bytree, subsample and nrounds are (0.3, 0.4), (1,5), (0.6,0.8), (0.500, 1.000), and (50,250) with the steps of 0.1, 1, 0.2, 0.125, and 50, respectively.
</p>
