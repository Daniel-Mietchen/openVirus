<?xml version="1.0" encoding="UTF-8"?>
<p>The testing and evaluation of our proposed system is an important challenge for its operational use. In order to evaluate the system against actual situational evidence from FF100 and forecasting, rather than the hypothetical evidence used in 
 <xref ref-type="fig" rid="pmed.1003018.g002">Fig 2</xref>, we would require the relevant data to be collected concurrently during an outbreak. An initial evaluation step could involve conducting an FF100 trial during a seasonal influenza epidemic in a jurisdiction where seasonal forecasting tools are already routinely used. In addition to providing data against which to evaluate the performance of algorithms and models within the system, this would enable the identification of operational challenges associated with the FF100 study design and its implementation. Tabletop exercises would also be important for testing and improving the system, particularly to obtain feedback on the clarity of presentation of alternative response strategies and uncertainties. Tabletop exercises/response drills are already a matter of routine in many jurisdictions; we are calling for analytics to be an integral part of these exercises.
</p>
