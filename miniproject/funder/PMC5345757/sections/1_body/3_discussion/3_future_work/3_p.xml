<?xml version="1.0" encoding="UTF-8"?>
<p>Another improvement to consider is a weighted combination of predictions whereby participants who have historically had more accurate predictions are given more weight in the aggregation process. This is similar in spirit to weighting user recommendations and rankings, which has been shown to increase accuracy in those settings [
 <xref rid="pcbi.1005248.ref039" ref-type="bibr">39</xref>, 
 <xref rid="pcbi.1005248.ref040" ref-type="bibr">40</xref>]. In the case of Epicast, there is limited evidence suggesting that some participants are overall more (or less) accurate than other participants. One example of this is in 
 <xref ref-type="fig" rid="pcbi.1005248.g005">Fig 5B</xref> where one participant has significantly higher Win Rate than Epicast and several other users. On the other hand, it is not clear whether the variance of prediction error is sufficiently small to learn which users are the most accurate in a reasonable amount of time—before the epidemic peak, for example. In other words, differences in 
 <italic>accuracy</italic> may be exploitable, but only if 
 <italic>precision</italic> is sufficiently high. If this is the case, then an adaptive weighting scheme could benefit the overall forecast. However, there is a critical obstacle that hinders the practical implementation of such a scheme: because of backfill, the final measure of accuracy is not known for many months. Despite this, we propose, implement, and analyze one such scheme in 
 <xref ref-type="supplementary-material" rid="pcbi.1005248.s001">S1 Text</xref>—the result is a small and statistically insignificant increase in accuracy.
</p>
