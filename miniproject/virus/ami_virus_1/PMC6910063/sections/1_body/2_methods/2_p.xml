<?xml version="1.0" encoding="UTF-8"?>
<p>The appropriate statistical tools (EP Evaluator software) were used to calculate pre-test probabilities: sensitivity (diagnostic test's ability to detect true positive) and specificity (diagnostic test's ability to detect true negative). We also calculated post-test probabilities: positive predictive value (rate of patients with positive tests who effectively have the disease according to the gold standard test), negative predictive value (rate of patients with negative tests who effectively do not have the disease according to the gold standard test). Finally, we calculated accuracy, which is the probability of the test providing correct results, and the Kappa coefficient, a measure of the level of agreement between two methods, adjusted by the odds, 
 <italic>i.e.,</italic> it informs the non-random chance, ranging from âˆ’1 to 1, where 0.00 is no agreement, 0.00-0.20 is poor agreement, 0.21-0.40 fair agreement, 0.41-0.60 moderate agreement, 0.61-0.80 good agreement, 0.81-0.99 very good agreement, and 1 is perfect agreement.
 <sup>(</sup>
 <xref rid="B15" ref-type="bibr">
  <sup>15</sup>
 </xref>
 <sup>,</sup>
 <xref rid="B16" ref-type="bibr">
  <sup>16</sup>
 </xref>
 <sup>)</sup>
</p>
