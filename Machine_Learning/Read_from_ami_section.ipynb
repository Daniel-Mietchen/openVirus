{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import string\n",
    "import os  \n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    '''\n",
    "    Function to clean the text so that it can easily be used for the classification\n",
    "    '''\n",
    "    letters_nums_only = re.sub(\"[^a-zA-Z0-9_]\", \" \", str(raw_text)) ## Remove everything except alphabets and numbers\n",
    "    lower_case = letters_nums_only.lower() # converting the entire text to lower case\n",
    "    no_blanks_alpha_num_lower = lower_case.strip() #Remove spaces at the beginning and at the end of the string\n",
    "    return no_blanks_alpha_num_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getcsv(output_file, parent_dir):\n",
    "    '''\n",
    "    function to walk through the directories, run clean_text and get the output in a file\n",
    "    '''\n",
    "\n",
    "    for root1, dirs, files in os.walk(parent_dir): # To search through the directory in aloop\n",
    "        for name in files:\n",
    "            new_path = (os.path.join(root1, name)) # get the path to the directory \n",
    "            #print(new_path)          \n",
    "\n",
    "            abstract_title = (re.findall(r'_(\\w+)', name) or None,)[0] #get the abstract name only in the directory and not the full name which is othewise serialized \n",
    "            #print( abstract_title)\n",
    "            if  abstract_title  is not None:\n",
    "                if abstract_title[0] =='abstract': # finding fulltext.xml file within the directory \n",
    "                    #new_path = (os.path.join(root, name)) \n",
    "                    \n",
    "                    tree = ET.parse(new_path) # parse the path \n",
    "                    file_full_path = os.path.dirname(new_path) # getting the directory path as a variable\n",
    "                    file_name = file_full_path.split(\"\\\\\")[5:6] # getting filename from the full path\n",
    "                    #print(file_name[0])\n",
    "                    #print(tree)\n",
    "                    \n",
    "                    root = tree.getroot() # navigation to the root of the file\n",
    "                    #print(root)\n",
    "\n",
    "                    abstracts = root.findall(\"./\") # searching the fulltext file to get abstract\n",
    "                    #print(abstracts)\n",
    "\n",
    "                    for text_ in abstracts: #navigate through every line in the abstract\n",
    "                        no_blanks_alpha_num_lower  = clean_text(text_.text) # applying the cleantext function\n",
    "                        #print(output_line)\n",
    "                        output_line = file_name[0] +  \",\" + no_blanks_alpha_num_lower  +  \"\\n\" # put output in a string\n",
    "                        #print(output_line)\n",
    "                        output_file.write(output_line) # write to a  file\n",
    "\n",
    "                    \n",
    "    output_file.close() #close the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the functions created \n",
    "outfile1 = open('files_with_titles_like_abstract.csv','w')\n",
    "parent_dir_true  = \"C:\\\\Users\\\\eless\\\\ami_12_08_2020\\\\corpus_950\"\n",
    "\n",
    "\n",
    "getcsv(outfile1, parent_dir_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_sentences(out_put, in_put):\n",
    "    '''\n",
    "    Remove all words like \"abstract\", \"author summary\" etc \n",
    "    '''\n",
    "    with open(in_put, \"r\") as a_file:\n",
    "        for line in a_file:\n",
    "            if len(line.split()) >2: # searches for sentences which are longer than 2 words \n",
    "                out_put.write(line)\n",
    "\n",
    "    out_put.close()\n",
    "    \n",
    "in_put1 = 'files_with_titles_like_abstract.csv'\n",
    "#in_put2 = 'not_virus.csv'\n",
    "\n",
    "out_put1 = open('files_with_abstract_content_only.csv','w')\n",
    "#out_put2 = open('not_virus_no_junk.csv','w')\n",
    "\n",
    "remove_small_sentences(out_put1,in_put1)\n",
    "#remove_small_sentences(out_put2, in_put2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add titles like articleID and text to the csv file\n",
    "import csv \n",
    "with open('files_with_abstract_content_only.csv',newline='') as f:\n",
    "    read_line = csv.reader(f)\n",
    "    data = [line for line in read_line]\n",
    "with open('data_with_title.csv','w',newline='') as f:\n",
    "    write_line = csv.writer(f)\n",
    "    write_line.writerow(['ArticleId','Text'])\n",
    "    write_line.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data file as a pandas dataframe\n",
    "train_data_1 = pd.read_csv(\"data_with_title.csv\")\n",
    "train_data_1 = train_data_1.iloc[1:] #remove the first row because it appears as data but is acually the title only \n",
    "train_data = train_data_1.sample(frac=1).reset_index(drop=True)  #shufffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC7188425</td>\n",
       "      <td>this review will briefly examine the clinical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC7294903</td>\n",
       "      <td>all ct scans detected ggos  symptomatic patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC7080275</td>\n",
       "      <td>understanding the transition of epidemic to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMC7194918</td>\n",
       "      <td>the recent global pandemic created by the coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC7258708</td>\n",
       "      <td>discriminating between patients with microbial...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ArticleId                                               Text\n",
       "0  PMC7188425  this review will briefly examine the clinical ...\n",
       "1  PMC7294903  all ct scans detected ggos  symptomatic patien...\n",
       "2  PMC7080275  understanding the transition of epidemic to en...\n",
       "3  PMC7194918  the recent global pandemic created by the coro...\n",
       "4  PMC7258708  discriminating between patients with microbial..."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge rows that have the same paper ID \n",
    "Merged_similar_rows = train_data.groupby('ArticleId', sort=False).Text.unique().agg('  '.join).reset_index()\n",
    "Merged_similar_rows.head() #visualizing the data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_similar_rows.to_csv('data_file_final.csv')  ## writing to the CSV file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
